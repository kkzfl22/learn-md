#### 配制3个节点2副本的分片

![image-20241105000230996](D:/java/myself/learn/learn-md/clickhouse/images/image-20241105000230996.png)

os11

```xml
<macros>
	<shard>01</shard>
	<replica>rep_1_1</replica>
</macros>
```

os12

```xml
<macros>
	<shard>02</shard>
	<replica>rep_2_2</replica>
</macros>
```

os13

```xml
<macros>
	<shard>03</shard>
	<replica>rep_3_3</replica>
</macros>
```







##### 11.2.3.1 集群分片的配制文件

使用外部文件的方式，内置文件同副本方式中的一样。

os11上的配制

在/etc/clickhouse-server/config.d/ 下创建metrika-cluster.xml

vi /etc/clickhouse-server/config.d/metrika-cluster.xml

```xml
<?xml version="1.0"?>
<yandex>
    <remote_servers>
        <nullnull_cluster>
            <!-- 集群名称-->
            <shard>
                <!--集群的第一个分片-->
                <internal_replication>true</internal_replication>
                <replica>
                    <!--该分片的第一个副本-->
                    <default_database>nullnull01</default_database>
                    <host>os11</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <!--该分片的第二个副本-->
                    <default_database>nullnull01</default_database>
                    <host>os12</host>
                    <port>9000</port>
                </replica>
            </shard>
            <shard>
                <!--集群的第二个分片-->
                <internal_replication>true</internal_replication>
                <replica>
                    <!--该分片的第一个副本-->
                    <default_database>nullnull02</default_database>
                    <host>os12</host>
                    <port>9000</port>
                </replica>
                 <replica>
                    <!--该分片的第二个副本-->
                    <default_database>nullnull02</default_database>
                    <host>os13</host>
                    <port>9000</port>
                </replica>
            </shard>
            <shard>
                <!--集群的第三个分片-->
                <internal_replication>true</internal_replication>
                <replica>
                    <!--该分片的第一个副本-->
                    <default_database>nullnull03</default_database>
                    <host>os13</host>
                    <port>9000</port>
                </replica>
                 <replica>
                    <!--该分片的第二个副本-->
                    <default_database>nullnull03</default_database>
                    <host>os11</host>
                    <port>9000</port>
                </replica>
            </shard>
        </nullnull_cluster>
    </remote_servers>
    <zookeeper-servers>
        <node index="1">
            <host>os11</host>
            <port>2181</port>
        </node>
        <node index="2">
            <host>os12</host>
            <port>2181</port>
        </node>
        <node index="3">
            <host>os13</host>
            <port>2181</port>
        </node>
    </zookeeper-servers>
    <macros>
        <!--宏标签，建表时需要引入的参数。名称可以随便定义-->
        <shard01>01</shard01>
        <replica01>rep_1_1</replica01>
	    <shard02>03</shard02>
	    <replica02>rep_3_2</replica02>
    </macros>
</yandex>
```

os12配制

在/etc/clickhouse-server/config.d/ 下创建metrika-cluster.xml

vi /etc/clickhouse-server/config.d/metrika-cluster.xml

```xml
<?xml version="1.0"?>
<yandex>
    <remote_servers>
        <nullnull_cluster>
            <!-- 集群名称-->
            <shard>
                <!--集群的第一个分片-->
                <internal_replication>true</internal_replication>
                <replica>
                    <!--该分片的第一个副本-->
                    <default_database>nullnull01</default_database>
                    <host>os11</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <!--该分片的第二个副本-->
                    <default_database>nullnull01</default_database>
                    <host>os12</host>
                    <port>9000</port>
                </replica>
            </shard>
            <shard>
                <!--集群的第二个分片-->
                <internal_replication>true</internal_replication>
                <replica>
                    <!--该分片的第一个副本-->
                    <default_database>nullnull02</default_database>
                    <host>os12</host>
                    <port>9000</port>
                </replica>
                 <replica>
                    <!--该分片的第二个副本-->
                    <default_database>nullnull02</default_database>
                    <host>os13</host>
                    <port>9000</port>
                </replica>
            </shard>
            <shard>
                <!--集群的第三个分片-->
                <internal_replication>true</internal_replication>
                <replica>
                    <!--该分片的第一个副本-->
                    <default_database>nullnull03</default_database>
                    <host>os13</host>
                    <port>9000</port>
                </replica>
                 <replica>
                    <!--该分片的第二个副本-->
                    <default_database>nullnull03</default_database>
                    <host>os11</host>
                    <port>9000</port>
                </replica>
            </shard>
        </nullnull_cluster>
    </remote_servers>
    <zookeeper-servers>
        <node index="1">
            <host>os11</host>
            <port>2181</port>
        </node>
        <node index="2">
            <host>os12</host>
            <port>2181</port>
        </node>
        <node index="3">
            <host>os13</host>
            <port>2181</port>
        </node>
    </zookeeper-servers>
    <macros>
        <!--宏标签，建表时需要引入的参数。名称可以随便定义-->
        <shard01>01</shard01>
        <replica01>rep_1_2</replica01>
	    <shard02>03</shard02>
	    <replica02>rep_2_1</replica02>
	</macros>
</yandex>
```

os13配制

在/etc/clickhouse-server/config.d/ 下创建metrika-cluster.xml

vi /etc/clickhouse-server/config.d/metrika-cluster.xml

```xml
<?xml version="1.0"?>
<yandex>
    <remote_servers>
        <nullnull_cluster>
            <!-- 集群名称-->
            <shard>
                <!--集群的第一个分片-->
                <internal_replication>true</internal_replication>
                <replica>
                    <!--该分片的第一个副本-->
                    <default_database>nullnull01</default_database>
                    <host>os11</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <!--该分片的第二个副本-->
                    <default_database>nullnull01</default_database>
                    <host>os12</host>
                    <port>9000</port>
                </replica>
            </shard>
            <shard>
                <!--集群的第二个分片-->
                <internal_replication>true</internal_replication>
                <replica>
                    <!--该分片的第一个副本-->
                    <default_database>nullnull02</default_database>
                    <host>os12</host>
                    <port>9000</port>
                </replica>
                 <replica>
                    <!--该分片的第二个副本-->
                    <default_database>nullnull02</default_database>
                    <host>os13</host>
                    <port>9000</port>
                </replica>
            </shard>
            <shard>
                <!--集群的第三个分片-->
                <internal_replication>true</internal_replication>
                <replica>
                    <!--该分片的第一个副本-->
                    <default_database>nullnull03</default_database>
                    <host>os13</host>
                    <port>9000</port>
                </replica>
                 <replica>
                    <!--该分片的第二个副本-->
                    <default_database>nullnull03</default_database>
                    <host>os11</host>
                    <port>9000</port>
                </replica>
            </shard>
        </nullnull_cluster>
    </remote_servers>
    <zookeeper-servers>
        <node index="1">
            <host>os11</host>
            <port>2181</port>
        </node>
        <node index="2">
            <host>os12</host>
            <port>2181</port>
        </node>
        <node index="3">
            <host>os13</host>
            <port>2181</port>
        </node>
    </zookeeper-servers>
    <macros>
        <!--宏标签，建表时需要引入的参数。名称可以随便定义-->
        <shard01>01</shard01>
        <replica01>rep_2_2</replica01>
		<shard02>03</shard02>
		<replica02>rep_3_1</replica02>
	</macros>
</yandex>
```

修改/etc/clickhouse-server/config.xml（每台机器都需要修改）

vi /etc/clickhouse-server/config.xml

```xml
<zookeeper incl="zookeeper-servers" optional="true" />
<include_from>/etc/clickhouse-server/config.d/metrika-cluster.xml</include_from>
```

注意： /etc/clickhouse-server/config.d/metrika-cluster.xml 文件所属的用户和组，需要为clickhouse

```sh
chown clickhouse:clickhouse /etc/clickhouse-server/config.d/metrika-cluster.xml
```

重启clickhouse节点

```sh
clickhouse restart
```



开放端口

```sh
firewall-cmd --permanent --zone=public --add-port=8123/tcp
firewall-cmd --permanent --zone=public --add-port=9000/tcp
firewall-cmd --permanent --zone=public --add-port=9009/tcp
firewall-cmd --reload
```





##### 11.2.3.2 集群分片的表创建

参考：

https://zhuanlan.zhihu.com/p/461792873?utm_id=0

https://www.cnblogs.com/wan-ming-zhu/p/18095576

```sql
# os11上的建表语句
# 1. 集群会自动在os12和os13上创建表。
# 2. 集群名称要和配制文件中的一致。
# 3. 分片和副本名称从配制文件中的宏定义中获取

# 查看宏信息
SELECT * FROM system.macros;
# 查看集群集群
select * from system.clusters;



CREATE DATABASE IF NOT EXISTS nullnull01;
CREATE DATABASE IF NOT EXISTS nullnull03;


# drop table nullnull01.cluster_user;
# os1 上创建分片1和副本1
create table nullnull01.cluster_user  (
    id UInt32,
    order_id String, 
    name String,
    money decimal(16,2),
    create_time Datetime    
)engine=ReplicatedMergeTree('/clickhouse/cluster/table/{shard01}/r1/cluster_user','{replica01}')
partition by toYYYYMMDD(create_time)
order by (id,order_id,intHash32(id))
SAMPLE BY intHash32(id);

# os1上创建的节点3的分片2
create table nullnull03.cluster_user (
	id UInt32,
    order_id String, 
    name String,
    money decimal(16,2),
    create_time Datetime    
)engine=ReplicatedMergeTree('/clickhouse/cluster/table/{shard02}/r1/cluster_user','{replica02}')
partition by toYYYYMMDD(create_time)
order by (id,order_id,intHash32(id))
SAMPLE BY intHash32(id);

# 查询数据
select * from nullnull01.cluster_user;
select * from nullnull03.cluster_user;




# os12节点上执行表
# 创建库操作
CREATE DATABASE IF NOT EXISTS nullnull01;
CREATE DATABASE IF NOT EXISTS nullnull02;


# drop table  nullnull02.cluster_user;
# drop table nullnull01.cluster_user;
# 创建分片2和副本1
create table nullnull02.cluster_user  (
    id UInt32,
    order_id String, 
    name String,
    money decimal(16,2),
    create_time Datetime    
)engine=ReplicatedMergeTree('/clickhouse/cluster/table/{shard01}/r1/cluster_user','{replica01}')
partition by toYYYYMMDD(create_time)
order by (id,order_id,intHash32(id))
SAMPLE BY intHash32(id);

# 创建分片1和副本2
create table nullnull01.cluster_user (
	id UInt32,
    order_id String, 
    name String,
    money decimal(16,2),
    create_time Datetime    
)engine=ReplicatedMergeTree('/clickhouse/cluster/table/{shard02}/r1/cluster_user','{replica02}')
partition by toYYYYMMDD(create_time)
order by (id,order_id,intHash32(id))
SAMPLE BY intHash32(id);



# os12上检查表是否创建成功。
select * from nullnull02.cluster_user;
select * from nullnull01.cluster_user;



# os13上执行
CREATE DATABASE IF NOT EXISTS nullnull03;
CREATE DATABASE IF NOT EXISTS nullnull02;

# 创建分片3和副本1
create table nullnull03.cluster_user  (
    id UInt32,
    order_id String, 
    name String,
    money decimal(16,2),
    create_time Datetime    
)engine=ReplicatedMergeTree('/clickhouse/cluster/table/{shard01}/r1/cluster_user','{replica01}')
partition by toYYYYMMDD(create_time)
order by (id,order_id,intHash32(id))
SAMPLE BY intHash32(id);

# 创建分片2和副本2
create table nullnull02.cluster_user (
	id UInt32,
    order_id String, 
    name String,
    money decimal(16,2),
    create_time Datetime    
)engine=ReplicatedMergeTree('/clickhouse/cluster/table/{shard02}/r1/cluster_user','{replica02}')
partition by toYYYYMMDD(create_time)
order by (id,order_id,intHash32(id))
SAMPLE BY intHash32(id);



# os12上检查表是否创建成功。
select * from nullnull03.cluster_user;
select * from nullnull02.cluster_user;


CREATE DATABASE IF NOT EXISTS nullnull;


# 在os11上创建分页式表，用于数据查询分发的管理。
create table nullnull.cluster_user_distribute (
	id UInt32,
    order_id String, 
    name String,
    money decimal(16,2),
    create_time Datetime    
)engine==Distributed(nullnull_cluster,nullnull,cluster_user,intHash32(id));



# 查看数据
select * from nullnull.cluster_user_distribute;


# 查看zookeeper中的信息
zkCli.sh
ls -R  /clickhouse/cluster/table/


# 插入数据测试
insert into nullnull.cluster_user_distribute values
(1,'001','空空1',20000,'2024-10-27 19:50:00'),
(2,'002','空空2',20000,'2024-10-27 19:50:00'),
(3,'003','空空3',20000,'2024-10-27 19:50:00'),
(4,'004','空空4',20000,'2024-10-28 19:50:00'),
(5,'005','空空5',20000,'2024-10-28 19:50:00'),
(6,'006','空空6',20000,'2024-10-28 19:50:00');


# 查询分布式表
select * from nullnull.cluster_user_distribute;


# 至每台机器查询本地表。查看分片信息
#os11
select * from nullnull.cluster_user;
#os12
select * from nullnull.cluster_user;
#os13
select * from nullnull.cluster_user;

```

输出：

```sh
# 查看集群
os11 :) select * from system.clusters;

SELECT *
FROM system.clusters

Query id: 49ede318-b9e4-4cc9-904e-7f7093b4549d

┌─cluster─────────────────────────────────────────┬─shard_num─┬─shard_weight─┬─replica_num─┬─host_name─┬─host_address─┬─port─┬─is_local─┬─user────┬─default_database─┬─errors_count─┬─slowdowns_count─┬─estimated_recovery_time─┐
│ nullnull_cluster                                │         1 │            1 │           1 │ os11      │ 192.168.5.11 │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ nullnull_cluster                                │         1 │            1 │           2 │ os12      │ 192.168.5.12 │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ nullnull_cluster                                │         2 │            1 │           1 │ os12      │ 192.168.5.12 │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ nullnull_cluster                                │         2 │            1 │           2 │ os13      │ 192.168.5.13 │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ nullnull_cluster                                │         3 │            1 │           1 │ os13      │ 192.168.5.13 │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ nullnull_cluster                                │         3 │            1 │           2 │ os11      │ 192.168.5.11 │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_one_shard_three_replicas_localhost │         1 │            1 │           1 │ 127.0.0.1 │ 127.0.0.1    │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_one_shard_three_replicas_localhost │         1 │            1 │           2 │ 127.0.0.2 │ 127.0.0.2    │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_one_shard_three_replicas_localhost │         1 │            1 │           3 │ 127.0.0.3 │ 127.0.0.3    │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_two_shards                         │         1 │            1 │           1 │ 127.0.0.1 │ 127.0.0.1    │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_two_shards                         │         2 │            1 │           1 │ 127.0.0.2 │ 127.0.0.2    │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_two_shards_internal_replication    │         1 │            1 │           1 │ 127.0.0.1 │ 127.0.0.1    │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_two_shards_internal_replication    │         2 │            1 │           1 │ 127.0.0.2 │ 127.0.0.2    │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_two_shards_localhost               │         1 │            1 │           1 │ localhost │ ::1          │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ test_cluster_two_shards_localhost               │         2 │            1 │           1 │ localhost │ ::1          │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ test_shard_localhost                            │         1 │            1 │           1 │ localhost │ ::1          │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ test_shard_localhost_secure                     │         1 │            1 │           1 │ localhost │ ::1          │ 9440 │        0 │ default │                  │            0 │               0 │                       0 │
│ test_unavailable_shard                          │         1 │            1 │           1 │ localhost │ ::1          │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ test_unavailable_shard                          │         2 │            1 │           1 │ localhost │ ::1          │    1 │        0 │ default │                  │            0 │               0 │                       0 │
└─────────────────────────────────────────────────┴───────────┴──────────────┴─────────────┴───────────┴──────────────┴──────┴──────────┴─────────┴──────────────────┴──────────────┴─────────────────┴─────────────────────────┘

19 rows in set. Elapsed: 0.001 sec. 

# os11创建分片1和副本1
os11 :) create table nullnull01.cluster_user  (
            id UInt32,
            order_id String, 
            name String,
            money decimal(16,2),
            create_time Datetime    
        )engine=ReplicatedMergeTree('/clickhouse/cluster/table/{shard01}/r1/cluster_user','{replica01}')
        partition by toYYYYMMDD(create_time)
        order by (id,order_id,intHash32(id))
        SAMPLE BY intHash32(id);

CREATE TABLE nullnull01.cluster_user
(
    `id` UInt32,
    `order_id` String,
    `name` String,
    `money` decimal(16, 2),
    `create_time` Datetime
)
ENGINE = ReplicatedMergeTree('/clickhouse/cluster/table/{shard01}/r1/cluster_user', '{replica01}')
PARTITION BY toYYYYMMDD(create_time)
ORDER BY (id, order_id, intHash32(id))
SAMPLE BY intHash32(id)

Query id: 9b7af0cd-4477-493e-9640-72b15097fa53

Ok.

0 rows in set. Elapsed: 0.041 sec.

# os11创建分片3副本2
os11 :) create table nullnull03.cluster_user (
        ^Iid UInt32,
            order_id String, 
            name String,
            money decimal(16,2),
            create_time Datetime    
        )engine=ReplicatedMergeTree('/clickhouse/cluster/table/{shard02}/r1/cluster_user','{replica02}')
        partition by toYYYYMMDD(create_time)
        order by (id,order_id,intHash32(id))
        SAMPLE BY intHash32(id);

CREATE TABLE nullnull03.cluster_user
(
    `id` UInt32,
    `order_id` String,
    `name` String,
    `money` decimal(16, 2),
    `create_time` Datetime
)
ENGINE = ReplicatedMergeTree('/clickhouse/cluster/table/{shard02}/r1/cluster_user', '{replica02}')
PARTITION BY toYYYYMMDD(create_time)
ORDER BY (id, order_id, intHash32(id))
SAMPLE BY intHash32(id)

Query id: 78d99e97-8852-48db-873c-65ebf0e1c183

Ok.

0 rows in set. Elapsed: 0.215 sec. 


# 查询数据
os11 :) select * from nullnull01.cluster_user;

SELECT *
FROM nullnull01.cluster_user

Query id: 32358c3a-d598-45cd-b985-601420daa218

Ok.

0 rows in set. Elapsed: 0.001 sec. 

os11 :) select * from nullnull03.cluster_user;

SELECT *
FROM nullnull03.cluster_user

Query id: 6e4c0949-bbd2-4b9c-8681-da444c321f94

Ok.

0 rows in set. Elapsed: 0.001 sec.
```

os12节点上执行创建

```sh
# 创建分片2的副本1
os12 :) create table nullnull02.cluster_user  (
            id UInt32,
            order_id String, 
            name String,
            money decimal(16,2),
            create_time Datetime    
        )engine=ReplicatedMergeTree('/clickhouse/cluster/table/{shard01}/r1/cluster_user','{replica01}')
        partition by toYYYYMMDD(create_time)
        order by (id,order_id,intHash32(id))
        SAMPLE BY intHash32(id);

CREATE TABLE nullnull02.cluster_user
(
    `id` UInt32,
    `order_id` String,
    `name` String,
    `money` decimal(16, 2),
    `create_time` Datetime
)
ENGINE = ReplicatedMergeTree('/clickhouse/cluster/table/{shard01}/r1/cluster_user', '{replica01}')
PARTITION BY toYYYYMMDD(create_time)
ORDER BY (id, order_id, intHash32(id))
SAMPLE BY intHash32(id)

Query id: 9ffbe819-6f6d-407c-a3d0-ab6498c8a558

Ok.

0 rows in set. Elapsed: 0.044 sec. 

# 创建分片1和副本2
os12 :) create table nullnull01.cluster_user (
        ^Iid UInt32,
            order_id String, 
            name String,
            money decimal(16,2),
            create_time Datetime    
        )engine=ReplicatedMergeTree('/clickhouse/cluster/table/{shard02}/r1/cluster_user','{replica02}')
        partition by toYYYYMMDD(create_time)
        order by (id,order_id,intHash32(id))
        SAMPLE BY intHash32(id);

CREATE TABLE nullnull01.cluster_user
(
    `id` UInt32,
    `order_id` String,
    `name` String,
    `money` decimal(16, 2),
    `create_time` Datetime
)
ENGINE = ReplicatedMergeTree('/clickhouse/cluster/table/{shard02}/r1/cluster_user', '{replica02}')
PARTITION BY toYYYYMMDD(create_time)
ORDER BY (id, order_id, intHash32(id))
SAMPLE BY intHash32(id)

Query id: 5c9c841a-1c02-4275-bf8a-0b93b84e2877

Ok.

0 rows in set. Elapsed: 0.036 sec. 

os12 :) select * from nullnull02.cluster_user;

SELECT *
FROM nullnull02.cluster_user

Query id: 11948a81-ba91-4abd-8c59-171b9bc82e1c

Ok.

0 rows in set. Elapsed: 0.001 sec. 

os12 :) select * from nullnull01.cluster_user;

SELECT *
FROM nullnull01.cluster_user

Query id: ddc7401f-cc24-484c-bf40-cdb498e71f69

Ok.

0 rows in set. Elapsed: 0.001 sec. 

```

os13上执行

```sh
# 创建库
os13 :) CREATE DATABASE IF NOT EXISTS nullnull03;

CREATE DATABASE IF NOT EXISTS nullnull03

Query id: 594ac86f-7286-44c1-863c-4768e5906728

Ok.

0 rows in set. Elapsed: 0.004 sec. 

os13 :) CREATE DATABASE IF NOT EXISTS nullnull02;

CREATE DATABASE IF NOT EXISTS nullnull02

Query id: 3b2c4739-824a-42a9-b519-3c51069146c8

Ok.

0 rows in set. Elapsed: 0.271 sec. 

os13 :)

# 创建分片3和副本1
os13 :) create table nullnull03.cluster_user  (
            id UInt32,
            order_id String, 
            name String,
            money decimal(16,2),
            create_time Datetime    
        )engine=ReplicatedMergeTree('/clickhouse/cluster/table/{shard01}/r1/cluster_user','{replica01}')
        partition by toYYYYMMDD(create_time)
        order by (id,order_id,intHash32(id))
        SAMPLE BY intHash32(id);

CREATE TABLE nullnull03.cluster_user
(
    `id` UInt32,
    `order_id` String,
    `name` String,
    `money` decimal(16, 2),
    `create_time` Datetime
)
ENGINE = ReplicatedMergeTree('/clickhouse/cluster/table/{shard01}/r1/cluster_user', '{replica01}')
PARTITION BY toYYYYMMDD(create_time)
ORDER BY (id, order_id, intHash32(id))
SAMPLE BY intHash32(id)

Query id: 264d7d9a-9fe9-4751-912a-2ab40de26a01

Ok.

0 rows in set. Elapsed: 0.157 sec. 
# 创建分片2和副本2
os13 :) create table nullnull02.cluster_user (
        ^Iid UInt32,
            order_id String, 
            name String,
            money decimal(16,2),
            create_time Datetime    
        )engine=ReplicatedMergeTree('/clickhouse/cluster/table/{shard02}/r1/cluster_user','{replica02}')
        partition by toYYYYMMDD(create_time)
        order by (id,order_id,intHash32(id))
        SAMPLE BY intHash32(id);

CREATE TABLE nullnull02.cluster_user
(
    `id` UInt32,
    `order_id` String,
    `name` String,
    `money` decimal(16, 2),
    `create_time` Datetime
)
ENGINE = ReplicatedMergeTree('/clickhouse/cluster/table/{shard02}/r1/cluster_user', '{replica02}')
PARTITION BY toYYYYMMDD(create_time)
ORDER BY (id, order_id, intHash32(id))
SAMPLE BY intHash32(id)

Query id: 90d8e31e-5846-414d-a05f-8105214d62e7

Ok.

0 rows in set. Elapsed: 0.029 sec.

# 查询数据
os13 :) select * from nullnull03.cluster_user;

SELECT *
FROM nullnull03.cluster_user

Query id: 3887e699-ef7d-4d01-b846-5910fc8c0f24

Ok.

0 rows in set. Elapsed: 0.001 sec. 

os13 :) select * from nullnull02.cluster_user;

SELECT *
FROM nullnull02.cluster_user

Query id: 3185d187-0a82-4dee-a19c-4d0e717510d0

Ok.

0 rows in set. Elapsed: 0.001 sec. 

```



