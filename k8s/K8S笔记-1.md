# K8S笔记

## Kubernetes安装

**硬件要求**

| 硬件 | 要求    |
| ---- | ------- |
| CPU  | 至少2核 |
| 内存 | 至少3G  |
| 硬件 | 至少50G |

**节点信息**

| 主机名        | IP           |
| ------------- | ------------ |
| k8s-master-50 | 192.168.5.50 |
| k8s-node-51   | 192.168.5.51 |
| k8s-node-52   | 192.168.5.52 |
| k8s-node-53   | 192.168.5.53 |

系统要求：

推荐使用centos7.7及以上版本

国内建议使用阿里云下载

```http
http://mirrors.aliyun.com/centos/7/isos/x86_64/
```

### 配制阿里云yum源

```sh
# 1. 下载安装wget
yum install -y wget

#2.备份默认的yum
mv /etc/yum.repos.d /etc/yum.repos.d.backup

#3.设置新的yum目录
mkdir -p /etc/yum.repos.d

#4.下载阿里yum配置到该目录中，选择对应版本
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo

#5.更新epel源为阿里云epel源
mv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backup
mv /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backup

wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo

#6.重建缓存
yum clean all
yum makecache

#7.看一下yum仓库有多少包
yum repolist
yum update
```

可以做一个系统快照版本，防止后面的翻车，以做好回滚，不用从头开始。做系统快照建议关机做，关机后的快照特别小。

### 升级系统内核

默认此处安装的7.9版本，内核版本为3.10，需要做下升级,此安装完成后，系统内核版本需要升级到了`4.4`，但别太高，不然操作不一致。

```sh
# 导入仓库源
rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm

# 查看可安装的软件包
yum --enablerepo="elrepo-kernel" list --showduplicates | sort -r | grep kernel-lt.x86_64

# 升级到最新内核版本的方法
yum --enablerepo=elrepo-kernel install -y kernel-lt


grep initrd16 /boot/grub2/grub.cfg
grub2-set-default 0

reboot
```

### 查看相关的系统信息

```sh
# 查看centos系统内核命令
uname -r
uname -a

# 查看CPU
lscpu

# 查看内存
free 
free -g

# 查看硬盘信息
fdisk -l
```

### centos7系统配制

```sh
# 1. 开发环境可以直接关闭防火墙，但生产环境，切记，万不能关
systemctl stop firewalld
systemctl disable firewalld


# 2. 关闭selinux
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
setenforce 0


# 3. 网桥过滤
vi /etc/sysctl.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-arptables = 1
net.ipv4.ip_forward=1
net.ipv4.ip_forward_use_pmtu = 0
#生效命令
sysctl --system
#查看效果
sysctl -a|grep "ip_forward"



# 4.  开启IPVS
#安装IPVS
yum -y install ipset ipvsadm -y
#编译ipvs.modules文件
vi /etc/sysconfig/modules/ipvs.modules
#文件内容如下
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
# 在4.4内核版本中使用nf_conntrack_ipv4
modprobe -- nf_conntrack_ipv4
#赋予权限并执行
chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack
#重启电脑，检查是否生效
reboot
lsmod | grep ip_vs_rr



# 5. 同步时间
#安装软件
yum -y install ntpdate
#向阿里云服务器同步时间
ntpdate time1.aliyun.com
#删除本地时间并设置时区为上海
rm -rf /etc/localtime
ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
#查看时间
date -R || date



# 6.命令补全
#安装bash-completion
yum -y install bash-completion bash-completion-extras
#使用bash-completion
source /etc/profile.d/bash_completion.sh



# 7. 关闭swap分区
#临时关闭：
swapoff -a
#永久关闭：
vi /etc/fstab
将文件中的/dev/mapper/centos-swap这行代码注释掉
#/dev/mapper/centos-swap swap swap defaults 0 0
#确认swap已经关闭：若swap行都显示 0 则表示关闭成功
free -m


# 8. hosts配置
#文件内容如下:
cat <<EOF >> /etc/hosts
192.168.5.50 k8s-master-50
192.168.5.51 k8s-node-51
192.168.5.52 k8s-node-52
192.168.5.53 k8s-node-53
EOF

cat /etc/hosts
```

### 安装docker

```sh
# 1. 安装docker的前置条件
yum install -y yum-utils device-mapper-persistent-data lvm2

# 2. 添加源
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum makecache fast

# 3. 查看docker更新版本
yum list docker-ce --showduplicates | sort -r


# 4. 安装docker版本,这里需要指定版本为18.09.8
yum -y install docker-ce-18.09.8
# 如果需要安装最新版本，使用
# yum -y install docker-ce
# 开启docker服务开机启动
systemctl start docker
systemctl status docker
# 查看docker版本
docker version
# docker-client版本：当前最新版本
# docker-server需要为版本为：18.09.8

# 5. 阿里云镜像加速器地址
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://ys2mfbsh.mirror.aliyuncs.com"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker

# 6. 设置为开机自启动
systemctl enable docker

# 7. 修改Cgroup Driver
vi /etc/docker/daemon.json
# 修改daemon.json，新增：
"exec-opts": ["native.cgroupdriver=systemd"]
# 修改cgroupdriver是为了消除安装k8s集群时的告警：
# [WARNING IsDockerSystemdCheck]: 
# detected “cgroupfs” as the Docker cgroup driver. The recommended driver is “systemd”. 
# Please follow the guide at https://kubernetes.io/docs/setup/cri/......
#重启docker服务：
systemctl daemon-reload 
systemctl restart docker
#查看修改后状态：
docker info | grep Cgroup


# 查看docker的信息
docker -v
docker info
```



### 使用kubeadm安装k8s

| 软件 | kubeadm                        | kubelet                                                    | kubectl                             | docker-ce |
| ---- | ------------------------------ | ---------------------------------------------------------- | ----------------------------------- | --------- |
| 版本 | 初始化集群管理.<br/>版本1.17.5 | 用于接收api-server指令对pod生命周期进行管理<br/>版本1.17.5 | 集群命令行管理工具<br/>版本：1.17.5 | 18.09.8   |

```sh
# 1. 安装yum源
vi /etc/yum.repos.d/kubernates.repo
# 加入内容
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
       
# 2. 更新缓存
yum clean all
yum -y makecache

# 3. 验证是否可用
yum list | grep kubeadm
#如果提示要验证yum-key.gpg是否可用，输入y。
#查找到kubeadm。显示版本

# 4. 查看k8s的版本
yum list kubelet --showduplicates | sort -r

# 5. 安装k8s-1.17.5
yum install -y kubelet-1.17.5 kubeadm-1.17.5 kubectl-1.17.5


# 6. 设置kubelet
# 增加配置信息
# 如果不配置kubelet，可能会导致K8S集群无法启动。为实现docker使用的cgroupdriver与kubelet使用的cgroup的一致性。
vi /etc/sysconfig/kubelet
KUBELET_EXTRA_ARGS="--cgroup-driver=systemd"


# 7. 设置开机启动
systemctl enable kubelet
```



### 初始化镜像

如果是首次安装k8s，手里没有备份好的镜像，需要执行此操作，如果已经已经有了镜像备份，请跳过本章节

```sh
# 查看安装集群需要镜像
kubeadm config images list
# 可得到如下这样一个列表
k8s.gcr.io/kube-apiserver:v1.17.5
k8s.gcr.io/kube-controller-manager:v1.17.5
k8s.gcr.io/kube-scheduler:v1.17.5
k8s.gcr.io/kube-proxy:v1.17.5
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.4.3-0
k8s.gcr.io/coredns:1.6.5
```

使用脚本下载镜像

```sh
mkdir -p /data/k8s
cd /data/k8s
# 编辑内容
vi images.sh
```

脚本内容:

从阿里云下载镜像，再下载的镜像应该去除"k8s.gcr.io"的前缀，版本换成kubeadm config images list命令获取到的版本

```shell
#!/bin/bash
images=(
   kube-apiserver:v1.17.5
   kube-controller-manager:v1.17.5
   kube-scheduler:v1.17.5
   kube-proxy:v1.17.5
   pause:3.1
   etcd:3.4.3-0
   coredns:1.6.5
)
for imageName in ${images[@]} ; 
do
   docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
   docker tag  registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName
   docker rmi  registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
done
```



执行脚本保存镜像

```sh
cd /data/k8s
# 给脚本授权
chmod +x images.sh 
# 执行脚本
./images.sh 

# 查看镜像
docker images

# 保存镜像
docker save -o k8s-1.17.5-all.image           \
k8s.gcr.io/kube-proxy:v1.17.5                 \
k8s.gcr.io/kube-controller-manager:v1.17.5    \
k8s.gcr.io/kube-apiserver:v1.17.5             \
k8s.gcr.io/kube-scheduler:v1.17.5             \
k8s.gcr.io/coredns:1.6.5                      \
k8s.gcr.io/etcd:3.4.3-0                       \
k8s.gcr.io/pause:3.1                          
```

此时基础的环境已经安装完成，可以做一个系统快照。以便安装翻车的时候恢复快照。



### 初始化集群网络

准备镜像

```sh
# 官网下载地址：
https://docs.projectcalico.org/v3.14/manifests/calico.yaml
# github地址：
https://github.com/projectcalico/calico


# 1. 镜像下载：
docker pull calico/cni:v3.14.2
docker pull calico/pod2daemon-flexvol:v3.14.2
docker pull calico/node:v3.14.2
docker pull calico/kube-controllers:v3.14.2

# 2. 初始化集群信息: calico网络,此在master节点上执行
kubeadm init --apiserver-advertise-address=192.168.5.50 --kubernetes-version v1.17.5 --service-cidr=10.1.0.0/16 --pod-network-cidr=10.81.0.0/16


# 3. 执行配制命令
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# 4. node节点加入集群信息 此命令，需要在3个node节点上执行
kubeadm join 192.168.5.50:6443 --token h1bc1a.q9i19q7smxl2pzom \
    --discovery-token-ca-cert-hash sha256:91d21fad4c1f4cdd3a7a01cae4d69b03805486ee90a4c2e77ac6b63f3ae66295 

# 5. kubectl命令自动补全,每个节点都执行下
echo "source <(kubectl completion bash)" >> ~/.bash_profile
source ~/.bash_profile
#在 bash 中设置当前 shell 的自动补全，要先安装 bash-completion 包。
echo "unset MAILCHECK">> /etc/profile
source /etc/profile
#在你的 bash shell 中永久的添加自动补全


# 6.yum-key.gpg验证未通过的相关执行
wget https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
wget https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
rpm --import yum-key.gpg
rpm --import rpm-package-key.gpg


# 查看集群的状态,集群的状态，同样是在master节点才能查看
kubectl get nodes

# 7. 下载网格的初始化文件
wget https://docs.projectcalico.org/v3.14/manifests/calico.yaml

# 执行网络初始化,此初始化，仅需要在master节点执行。
kubectl apply -f calico.yaml

# 查看集群的状态,同样是在master节点才能查看
kubectl get nodes
```

集群初始化:

```sh
[root@k8s-master-50 ~]# kubeadm init --apiserver-advertise-address=192.168.5.50 --kubernetes-version v1.17.5 --service-cidr=10.1.0.0/16 --pod-network-cidr=10.81.0.0/16
W1227 11:01:55.994044    3131 validation.go:28] Cannot validate kube-proxy config - no validator is available
W1227 11:01:55.994077    3131 validation.go:28] Cannot validate kubelet config - no validator is available
[init] Using Kubernetes version: v1.17.5
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master-50 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.1.0.1 192.168.5.50]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master-50 localhost] and IPs [192.168.5.50 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master-50 localhost] and IPs [192.168.5.50 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
W1227 11:01:57.949710    3131 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[control-plane] Creating static Pod manifest for "kube-scheduler"
W1227 11:01:57.950137    3131 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 15.012704 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.17" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master-50 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node k8s-master-50 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: h1bc1a.q9i19q7smxl2pzom
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.5.50:6443 --token h1bc1a.q9i19q7smxl2pzom \
    --discovery-token-ca-cert-hash sha256:91d21fad4c1f4cdd3a7a01cae4d69b03805486ee90a4c2e77ac6b63f3ae66295 
```

节点加入集群的日志：

```sh
[root@k8s-node51 ~]# kubeadm join 192.168.5.50:6443 --token h1bc1a.q9i19q7smxl2pzom \
>     --discovery-token-ca-cert-hash sha256:91d21fad4c1f4cdd3a7a01cae4d69b03805486ee90a4c2e77ac6b63f3ae66295 
W1227 11:04:05.199752    3301 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
[preflight] Running pre-flight checks
        [WARNING Hostname]: hostname "k8s-node51" could not be reached
        [WARNING Hostname]: hostname "k8s-node51": lookup k8s-node51 on 192.168.3.1:53: no such host
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

经过以上的步骤，集群相关的安装已经完成：

查看集群的一个状态:

```sh
[root@k8s-master-50 ~]# kubectl get nodes
NAME            STATUS     ROLES    AGE     VERSION
k8s-master-50   NotReady   master   10m     v1.17.5
k8s-node51      NotReady   <none>   8m53s   v1.17.5
k8s-node52      NotReady   <none>   8m49s   v1.17.5
k8s-node53      NotReady   <none>   8m35s   v1.17.5
```

集群网络初始化：

```sh
[root@k8s-master-50 k8s]# kubectl apply -f calico.yaml
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
```

集群网络初始化成功后的一个状态:

```sh
[root@k8s-master-50 k8s]# kubectl get nodes
NAME            STATUS   ROLES    AGE   VERSION
k8s-master-50   Ready    master   16m   v1.17.5
k8s-node51      Ready    <none>   14m   v1.17.5
k8s-node52      Ready    <none>   14m   v1.17.5
k8s-node53      Ready    <none>   14m   v1.17.5
```





## k8s之NameSpace

中文名称：命名空间，可以认为namespace是kubernets集群中的虚拟化集群。在一个kubernetes集群中可以拥有多个命名空间，它们逻辑上彼此隔离。可以为你提供组织、安全、甚至性能方面的帮助。

Namespace是一对资源和对象的抽象集合。比如可以用系来将系统内部的对象划分为不同的项目组或用户组。常见的是pods,servers,replication controllers和deployment和deployments等都属于某一个namespace(默认是default)，而node，persistentVolumnets等则不属性任何namespace。

大多数的kubernetes中集群默认会有一个default的namespace。实际上应该是4个

- default:  资源默认被创建于default命名空间。
- kube-system： kubernetes系统组件使用。
- kube-node-lease: kubernetes集群节点租约状态。v1.13加入
- kube-public: 公共资源使用。但实际上现在并不常用。

这个默认（default）的namespace并没有什么特别，但你不能删除它。这很适合刚刚开始使用kubernetes和一些小的产品系统。但不建议应用于大型生产系统。因为，这种复杂系统中。团队会非常容易意外地或者无意识的重写或者中断其他service。相反，请创建多个命名空间来把你的service（服务）分割成一时半会儿 容易管理的块。

作用：多租户情况下，实现资源的隔离。

属于逻辑隔离。

属于管理边界。

不属性网络边界。

可以针对每个namespace做资源配额。

**查看命名空间**

```sh
kubectl get namespace
# 简写命令
kubectl get ns

# 查看所有命名空间的pod资源
kubectl get pod --all-namepsace
kubectl get pod -A
```

输出 ：

```sh
[root@k8s-master-50 ~]# kubectl get namespace
NAME              STATUS   AGE
default           Active   22h
kube-node-lease   Active   22h
kube-public       Active   22h
kube-system       Active   22h
[root@k8s-master-50 ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   22h
kube-node-lease   Active   22h
kube-public       Active   22h
kube-system       Active   22h
[root@k8s-master-50 ~]# kubectl get pod --all-namespaces
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6b94766748-hfbjs   1/1     Running   3          22h
kube-system   calico-node-27kqm                          1/1     Running   3          22h
kube-system   calico-node-8w2pp                          1/1     Running   3          22h
kube-system   calico-node-brwl7                          1/1     Running   3          22h
kube-system   calico-node-s6d49                          1/1     Running   4          22h
kube-system   coredns-6955765f44-rq92j                   1/1     Running   3          22h
kube-system   coredns-6955765f44-wmr68                   1/1     Running   3          22h
kube-system   etcd-k8s-master-50                         1/1     Running   3          22h
kube-system   kube-apiserver-k8s-master-50               1/1     Running   4          22h
kube-system   kube-controller-manager-k8s-master-50      1/1     Running   3          22h
kube-system   kube-proxy-4cbsx                           1/1     Running   3          22h
kube-system   kube-proxy-l6k6x                           1/1     Running   3          22h
kube-system   kube-proxy-pq57w                           1/1     Running   4          22h
kube-system   kube-proxy-tlb5j                           1/1     Running   3          22h
kube-system   kube-scheduler-k8s-master-50               1/1     Running   3          22h
[root@k8s-master-50 ~]# kubectl get pod -A
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6b94766748-hfbjs   1/1     Running   3          22h
kube-system   calico-node-27kqm                          1/1     Running   3          22h
kube-system   calico-node-8w2pp                          1/1     Running   3          22h
kube-system   calico-node-brwl7                          1/1     Running   3          22h
kube-system   calico-node-s6d49                          1/1     Running   4          22h
kube-system   coredns-6955765f44-rq92j                   1/1     Running   3          22h
kube-system   coredns-6955765f44-wmr68                   1/1     Running   3          22h
kube-system   etcd-k8s-master-50                         1/1     Running   3          22h
kube-system   kube-apiserver-k8s-master-50               1/1     Running   4          22h
kube-system   kube-controller-manager-k8s-master-50      1/1     Running   3          22h
kube-system   kube-proxy-4cbsx                           1/1     Running   3          22h
kube-system   kube-proxy-l6k6x                           1/1     Running   3          22h
kube-system   kube-proxy-pq57w                           1/1     Running   4          22h
kube-system   kube-proxy-tlb5j                           1/1     Running   3          22h
kube-system   kube-scheduler-k8s-master-50               1/1     Running   3          22h
```

命名空间的说明：

```sh
default: 用户创建的pod默认都在此命名空间。
kube-public 所有用户均可以访问，包括未认证用户。
kube-node-lease kubernetes集群节点租约状态。
kube-system kubernetes集群在使用。
```

**创建和删除Namespace**

```sh
kubectl create namespace nullnull
kubectl delete namespace nullnull

# 简写命令
kubectl create ns nullnull
kubectl delete ns nullnull

```

输出:

```sh
[root@k8s-master-50 ~]# kubectl create namespace nullnull
namespace/nullnull created
[root@k8s-master-50 ~]# kubectl get namespace
NAME              STATUS   AGE
default           Active   23h
kube-node-lease   Active   23h
kube-public       Active   23h
kube-system       Active   23h
nullnull          Active   20s
[root@k8s-master-50 ~]# kubectl delete namespace nullnull
namespace "nullnull" deleted
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get namespace
NAME              STATUS   AGE
default           Active   23h
kube-node-lease   Active   23h
kube-public       Active   23h
kube-system       Active   23h
[root@k8s-master-50 ~]# kubectl create ns nullnull
namespace/nullnull created
[root@k8s-master-50 ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   23h
kube-node-lease   Active   23h
kube-public       Active   23h
kube-system       Active   23h
nullnull          Active   5s
[root@k8s-master-50 ~]# kubectl delete ns nullnull
namespace "nullnull" deleted
[root@k8s-master-50 ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   23h
kube-node-lease   Active   23h
kube-public       Active   23h
kube-system       Active   23h
```



## K8S之POD



![image-20231228102017454](.\images\image-20231228102017454.png)

pod是kubernetes集群能够调度的最小单元。POD是容器的封装。

在kubernetes集群中，Pod是所有业务类型的基础，也是K8S管理的最小单位级，它是一个或多个容器的组合。这些容器共享存储，网络和命名空间，以及如何运行的规范。在Pod中，所有容器都被同一安排和调度，并运行在共享的上下文中。对于具体应用而言，Pod是它们的逻辑主机，Pod包含业务相关的多个应用容器。

**pod的两个必知特点**

`网络`：每一个Pod都会被指派唯一的一个IP地址，在Pod中，每一个容器共享网络命名空间，包括IP地址和网络端口。在同一个Pod中的容器可以和localhost进行互相通信。当Pod中的容器需要与Pod外的实体进行通信时，则需要通过端口等共享的网络资源。

`存储`：Pod能够被指定共享存储卷的集合，在Pod中所有的容器能够访问共享存储卷，允许这些容器共享数据。存储卷也允许在一个Pod持久化数据，以防止其中的容器需要被重启。

**Pod的工作方式**

k8s一般不直接创建Pod，而是通过控制器和模板配置来管理和调度。

- Pod模板。

后续章节会介绍Pod模板。

- Pod重启

在Pod中的容器可能会由于异常等原因导致其终止退出，Kubernetes提供了重启策略以重启容器。重启策略对同一个Pod的所有容器起作用，容器的重启由Node上的kubelet执行。Pod支持三种重启策略：

1. Always: 只要退出就会重启。
2. OnFailure: 只有在失败退出时（exit code不等于0）时，才会重启。
3. Never: 只要退出，就不再重启。

注意：重启是指在Pod宿主Node上进行本地重启，而不是调度到其它的Node上。

- 资源限制

kubernetes通过cgroups限制容器的CPU和内存等计算资源，包括requests（请求，调度器保证调度到资源充足的Node上）和limits(上限)等。

**查看Pod**

```sh
# 查看default命名空间下的Pods
kubectl get pods
# 查看kube-system命名空间下的pods
kubectl get pods -n kube-system
# 查看所有命名空间下的pods
kubectl get pod --all-namespace
kubectl get pod -A
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl get pods
No resources found in default namespace.
[root@k8s-master-50 ~]# kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6b94766748-hfbjs   1/1     Running   3          23h
calico-node-27kqm                          1/1     Running   3          23h
calico-node-8w2pp                          1/1     Running   3          23h
calico-node-brwl7                          1/1     Running   3          23h
calico-node-s6d49                          1/1     Running   4          23h
coredns-6955765f44-rq92j                   1/1     Running   3          23h
coredns-6955765f44-wmr68                   1/1     Running   3          23h
etcd-k8s-master-50                         1/1     Running   3          23h
kube-apiserver-k8s-master-50               1/1     Running   4          23h
kube-controller-manager-k8s-master-50      1/1     Running   3          23h
kube-proxy-4cbsx                           1/1     Running   3          23h
kube-proxy-l6k6x                           1/1     Running   3          23h
kube-proxy-pq57w                           1/1     Running   4          23h
kube-proxy-tlb5j                           1/1     Running   3          23h
kube-scheduler-k8s-master-50               1/1     Running   3          23h
[root@k8s-master-50 ~]# kubectl get pod --all-namespaces
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6b94766748-hfbjs   1/1     Running   3          23h
kube-system   calico-node-27kqm                          1/1     Running   3          23h
kube-system   calico-node-8w2pp                          1/1     Running   3          23h
kube-system   calico-node-brwl7                          1/1     Running   3          23h
kube-system   calico-node-s6d49                          1/1     Running   4          23h
kube-system   coredns-6955765f44-rq92j                   1/1     Running   3          23h
kube-system   coredns-6955765f44-wmr68                   1/1     Running   3          23h
kube-system   etcd-k8s-master-50                         1/1     Running   3          23h
kube-system   kube-apiserver-k8s-master-50               1/1     Running   4          23h
kube-system   kube-controller-manager-k8s-master-50      1/1     Running   3          23h
kube-system   kube-proxy-4cbsx                           1/1     Running   3          23h
kube-system   kube-proxy-l6k6x                           1/1     Running   3          23h
kube-system   kube-proxy-pq57w                           1/1     Running   4          23h
kube-system   kube-proxy-tlb5j                           1/1     Running   3          23h
kube-system   kube-scheduler-k8s-master-50               1/1     Running   3          23h
[root@k8s-master-50 ~]# kubectl get pod -A
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6b94766748-hfbjs   1/1     Running   3          23h
kube-system   calico-node-27kqm                          1/1     Running   3          23h
kube-system   calico-node-8w2pp                          1/1     Running   3          23h
kube-system   calico-node-brwl7                          1/1     Running   3          23h
kube-system   calico-node-s6d49                          1/1     Running   4          23h
kube-system   coredns-6955765f44-rq92j                   1/1     Running   3          23h
kube-system   coredns-6955765f44-wmr68                   1/1     Running   3          23h
kube-system   etcd-k8s-master-50                         1/1     Running   3          23h
kube-system   kube-apiserver-k8s-master-50               1/1     Running   4          23h
kube-system   kube-controller-manager-k8s-master-50      1/1     Running   3          23h
kube-system   kube-proxy-4cbsx                           1/1     Running   3          23h
kube-system   kube-proxy-l6k6x                           1/1     Running   3          23h
kube-system   kube-proxy-pq57w                           1/1     Running   4          23h
kube-system   kube-proxy-tlb5j                           1/1     Running   3          23h
kube-system   kube-scheduler-k8s-master-50               1/1     Running   3          23h
[root@k8s-master-50 ~]# 
```

**创建Pod**

```sh
# 下载镜像
# K8S集群的每一个节点都需要下载镜像，选择不同的镜像，下载镜像的大小也不同。
docker pull tomcat:9.0.20-jre8-alpine
docker pull tomcat:9.0.37-jdk8-openjdk-slim
docker pull tomcat:9.0.37-jdk8


# 在defalt命名空间中创建一个pod副本的deployment
kubectl run tomcat9-run --image=tomcat:9.0.20-jre8-alpine --port=8080

kubectl get pod
kubectl get pod -o wide

# 使用pod的ip访问容器
curl ****:8080
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl run tomcat9-run --image=tomcat:9.0.20-jre8-alpine --port=8080
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
deployment.apps/tomcat9-run created
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get pod
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9-run-6595fb5f85-fls4k   1/1     Running   0          72s
[root@k8s-master-50 ~]# kubectl get pod -w
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9-run-6595fb5f85-fls4k   1/1     Running   0          75s
^[[A^[[A^C[root@k8s-master-50 ~]# kubectl get pod -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP            NODE         NOMINATED NODE   READINESS GATES
tomcat9-run-6595fb5f85-fls4k   1/1     Running   0          85s   10.81.92.65   k8s-node51   <none>           <none>
[root@k8s-master-50 ~]# curl 10.81.92.65:8080
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        ......
    </body>

</html>
```

**扩容**

```sh
# 查看当前部署信息
kubectl get deployment
kubectl get deployment -o wide

# 将一个容器扩容至3个
kubectl scale --replicas=3 deployment/tomcat9-run

# 查看当前部署信息
kubectl get deployment
kubectl get deployment -o wide

# 查看pod信息
kubectl get pod
kubectl get pod -o wide
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl get deployment
NAME          READY   UP-TO-DATE   AVAILABLE   AGE
tomcat9-run   1/1     1            1           7m1s
[root@k8s-master-50 ~]# kubectl get deployment -o wide
NAME          READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS    IMAGES                      SELECTOR
tomcat9-run   1/1     1            1           7m1s   tomcat9-run   tomcat:9.0.20-jre8-alpine   run=tomcat9-run
[root@k8s-master-50 ~]# kubectl scale --replicas=3 deployment/tomcat9-run
deployment.apps/tomcat9-run scaled
[root@k8s-master-50 ~]# kubectl get deployment
NAME          READY   UP-TO-DATE   AVAILABLE   AGE
tomcat9-run   3/3     3            3           7m13s
[root@k8s-master-50 ~]# kubectl get deployment -o wide
NAME          READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS    IMAGES                      SELECTOR
tomcat9-run   3/3     3            3           7m13s   tomcat9-run   tomcat:9.0.20-jre8-alpine   run=tomcat9-run
[root@k8s-master-50 ~]# kubectl get  pod
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9-run-6595fb5f85-fls4k   1/1     Running   0          21m
tomcat9-run-6595fb5f85-gtxdp   1/1     Running   0          14m
tomcat9-run-6595fb5f85-m4drb   1/1     Running   0          14m
[root@k8s-master-50 ~]# kubectl get  pod -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
tomcat9-run-6595fb5f85-fls4k   1/1     Running   0          22m   10.81.92.65     k8s-node51   <none>           <none>
tomcat9-run-6595fb5f85-gtxdp   1/1     Running   0          15m   10.81.237.193   k8s-node52   <none>           <none>
tomcat9-run-6595fb5f85-m4drb   1/1     Running   0          15m   10.81.149.193   k8s-node53   <none>           <none>
[root@k8s-master-50 ~]# 
```

通过观察pod的一个输出可以发现，它们家口的网段是一致的，并且被分配了不同的节点上，10.81.92.65这个IP就是在node51这个节点上，其他的IP也对应了不同的节点。

而且使用此IP，加上8080端口，就可以直接访问

```sh
[root@k8s-master-50 ~]# curl 10.81.237.193:8080



<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        <div id="wrapper">
            <div id="navigation" class="curved container">
                <span id="nav-home"><a href="https://tomcat.apache.org/">Home</a></span>
                <span id="nav-hosts"><a href="/docs/">Documentation</a></span>
                <span id="nav-config"><a href="/docs/config/">Configuration</a></span>
                <span id="nav-examples"><a href="/examples/">Examples</a></span>
                <span id="nav-wiki"><a href="https://wiki.apache.org/tomcat/FrontPage">Wiki</a></span>
                <span id="nav-lists"><a href="https://tomcat.apache.org/lists.html">Mailing Lists</a></span>
                <span id="nav-help"><a href="https://tomcat.apache.org/findhelp.html">Find Help</a></span>
                <br class="separator" />
                ......
            </div>
       </div>
    </body>

</html>
```



**创建服务**

只有创建了服务才能被外网所访问

```sh
# 将之前部署的deployment（tomcat9-run）与service产生关系，外部便可以访问了。
kubectl expose deployment tomcat9-run --name=tomcat9-svc --port=8888 --target-port=8080 --protocol=TCP --type=NodePort
# 这个端口号是对内，对K8S集群内其他应该暴露的一个端口号
--port=8888
# deployment容器的端口号，
--target-port=8080

kubectl get service

# 简写
kubectl get svc
kubectl get svc -o wide

# 使用服务端口访问
curl 10.1.210.157:8888

# 外部使用浏览器访问
http://192.168.5.50:32395/
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl expose deployment tomcat9-run --name=tomcat9-svc --port=8888 --target-port=8080 --protocol=TCP --type=NodePort
service/tomcat9-svc exposed
[root@k8s-master-50 ~]# kubectl get service
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes    ClusterIP   10.1.0.1       <none>        443/TCP          29h
tomcat9-svc   NodePort    10.1.210.157   <none>        8888:32395/TCP   24m
[root@k8s-master-50 ~]# kubectl get svc
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes    ClusterIP   10.1.0.1       <none>        443/TCP          29h
tomcat9-svc   NodePort    10.1.210.157   <none>        8888:32395/TCP   16s
[root@k8s-master-50 ~]# kubectl get svc -o wide
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE   SELECTOR
kubernetes    ClusterIP   10.1.0.1       <none>        443/TCP          29h   <none>
tomcat9-svc   NodePort    10.1.210.157   <none>        8888:32395/TCP   48s   run=tomcat9-run
[root@k8s-master-50 ~]# curl 10.1.210.157:8888



<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        <div id="wrapper">
		......
		</div>
	</body>
</html>
```

浏览器打开测试：

![image-20231228163416398](.\images\image-20231228163416398.png)





## kubectl常用命令

语法规则：

```sh
kubectl [command] [TYPE] [NAME] [flags] [options]
```

- command: 指定一个或者多个资源执行的操作，例：`create`、`get `、`delete`。
- TYPE： 指定资源类型。 资源类型不区分大小写，可以指定单数、复数或缩写形式

```sh
kubectl get pod tomcat9-run-6595fb5f85-fls4k
kubectl get pods tomcat9-run-6595fb5f85-fls4k
kubectl get po tomcat9-run-6595fb5f85-fls4k
```

- NAME: 指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息`kubectl get pods`,对多个资源执行操作时，可以按类型和名称指定每个资源，或者一个或多个文件。
  - 要按类型和名称指定资源：
    - 要对所有类型相同的资源进行分组，可以执行： `TYPE1 name1 name2 name<#>`。
    - 分别指定多个资源类型：`TYPE1/name1 TYPE2/name2 TYPE3/name3 TYPE<#>/name<#>`。例如`kubectl get pod/example-pod1 replicationcontroller/example-rc1`
  - 用一个或多个文件指定资源：`-f file -f file2 -f file<#>`
    - 使用YAML而不是JSON，是因为YAML更容易使用，特别用于文件配制时。例：`kubectl get pod -f ./pod`
- flags: 指定可选参数，例如，可以使用-S或者-server参数指定Kubernetes API服务器的地址和端口。

注意：从命令行指定参数会覆盖默认值和任何相应的环境变量。

相关的帮助信息可以使用`kubectl help`查看

### **get命令**

`kubectl get` - 列出一个或者多个资源。

```sh
# 查看集群的状态信息
kubectl cluster-info

# 查看集群的状态
kubectl get cs

# 查看集群的节点信息
kubectl get nodes

# 查看集群命名空间
kubectl get ns

# 查看指定命名空间的服务
kubectl get svc -n kube-system

# 以纯文本列出所有Pod
kubectl get pods

# 以纯文本列出所有的Pod,并包含附加信息（如节点名）
kubectl get pods -o wide

# 以纯文本格式列出具有指定名称的副本控制器。提示：可以使用别名‘rc’缩短和替换‘replicationcontroller’资源类型。
kubectl get replicationcontroller <rc-name>
kubectl get rc <rc-name>

# 以纯文本格式列出所有副本控制器和服务
kubectl get rc,servers

# 以纯文本格式列出所有守护程序集，包括未初始化的守护程序集
# 在 Kubernetes 中，--include-uninitialized 标志在 Kubernetes 版本 1.19 之前是有效的，但在 Kubernetes 1.19 版本之后被废弃并删除。因此，如果你的 Kubernetes 版本为 1.19 或更新版本，则无法使用该标志。
kubectl get pods --include-uninitialized

# 列出在节点server01上运行的所有pod
kubectl get pods --field-selector=spec.nodeName=k8s-node51
```

输出：

```sh
[root@k8s-master-50 ~]# kubectl cluster-info
Kubernetes master is running at https://192.168.5.50:6443
KubeDNS is running at https://192.168.5.50:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get nodes
NAME            STATUS   ROLES    AGE   VERSION
k8s-master-50   Ready    master   46h   v1.17.5
k8s-node51      Ready    <none>   46h   v1.17.5
k8s-node52      Ready    <none>   46h   v1.17.5
k8s-node53      Ready    <none>   46h   v1.17.5
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   47h
kube-node-lease   Active   47h
kube-public       Active   47h
kube-system       Active   47h
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get svc -n kube-system
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.1.0.10    <none>        53/UDP,53/TCP,9153/TCP   47h
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9-run-6595fb5f85-fls4k   1/1     Running   1          17h
tomcat9-run-6595fb5f85-gtxdp   1/1     Running   1          17h
tomcat9-run-6595fb5f85-m4drb   1/1     Running   1          17h
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get pods -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
tomcat9-run-6595fb5f85-fls4k   1/1     Running   1          17h   10.81.92.66     k8s-node51   <none>           <none>
tomcat9-run-6595fb5f85-gtxdp   1/1     Running   1          17h   10.81.237.194   k8s-node52   <none>           <none>
tomcat9-run-6595fb5f85-m4drb   1/1     Running   1          17h   10.81.149.194   k8s-node53   <none>           <none>
[root@k8s-master-50 ~]#
[root@k8s-master-50 ~]# kubectl get replicationcontroller 
No resources found in default namespace.
[root@k8s-master-50 ~]# kubectl get rc
No resources found in default namespace.
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get rc,services
NAME                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/kubernetes    ClusterIP   10.1.0.1       <none>        443/TCP          47h
service/tomcat9-svc   NodePort    10.1.210.157   <none>        8888:32395/TCP   17h
[root@k8s-master-50 ~]# 

[root@k8s-master-50 ~]# kubectl get pods --field-selector=spec.nodeName=k8s-node51
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9-run-6595fb5f85-fls4k   1/1     Running   1          18h

```



### **describe命令**

kubectl describe 命令显示一个或多个资源的详细状态，默认情况下包括未初始化的资源。

```sh
# 显示名称为<node-name>的节点的详细信息
kubectl describe nodes <node-name>

# 显示名为 <pod-name>的pod的详细信息
kubectl describe pods/<pod-name>

# 显示名为<rc-name>的副本控制器管理的所有pod的详细信息.
# 副本控制器创建的任何pod都以复制控制器的名称为前缀
kubectl describe pods <rc-name>

# 描述所有的Pod，不包括未初始化的pod
kubectl describe pods --include-uninitialized=false
# 在当前1.17.5版本中，此参数已经不生产效了
#Error: unknown flag: --include-uninitialized
#See 'kubectl describe --help' for usage.
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl describe nodes k8s-master-50
Name:               k8s-master-50
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=k8s-master-50
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/master=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 192.168.5.50/24
                    projectcalico.org/IPv4IPIPTunnelAddr: 10.81.230.0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 27 Dec 2023 11:02:11 +0800
Taints:             node-role.kubernetes.io/master:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  k8s-master-50
  AcquireTime:     <unset>
  RenewTime:       Fri, 29 Dec 2023 10:40:43 +0800
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Fri, 29 Dec 2023 09:54:24 +0800   Fri, 29 Dec 2023 09:54:24 +0800   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Fri, 29 Dec 2023 10:38:53 +0800   Wed, 27 Dec 2023 11:02:09 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Fri, 29 Dec 2023 10:38:53 +0800   Wed, 27 Dec 2023 11:02:09 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Fri, 29 Dec 2023 10:38:53 +0800   Wed, 27 Dec 2023 11:02:09 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Fri, 29 Dec 2023 10:38:53 +0800   Wed, 27 Dec 2023 11:17:04 +0800   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.5.50
  Hostname:    k8s-master-50
Capacity:
  cpu:                2
  ephemeral-storage:  59313872Ki
  hugepages-2Mi:      0
  memory:             16388392Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  54663664345
  hugepages-2Mi:      0
  memory:             16285992Ki
  pods:               110
System Info:
  Machine ID:                 c849151047d18342826600c28ebd79c1
  System UUID:                c8491510-47d1-8342-8266-00c28ebd79c1
  Boot ID:                    cf221679-f52f-4ab2-b804-91308f2f665f
  Kernel Version:             5.4.265-1.el7.elrepo.x86_64
  OS Image:                   CentOS Linux 7 (Core)
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://18.9.8
  Kubelet Version:            v1.17.5
  Kube-Proxy Version:         v1.17.5
PodCIDR:                      10.81.0.0/24
PodCIDRs:                     10.81.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  kube-system                 calico-kube-controllers-6b94766748-hfbjs    0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 calico-node-brwl7                           250m (12%)    0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 coredns-6955765f44-rq92j                    100m (5%)     0 (0%)      70Mi (0%)        170Mi (1%)     47h
  kube-system                 coredns-6955765f44-wmr68                    100m (5%)     0 (0%)      70Mi (0%)        170Mi (1%)     47h
  kube-system                 etcd-k8s-master-50                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 kube-apiserver-k8s-master-50                250m (12%)    0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 kube-controller-manager-k8s-master-50       200m (10%)    0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 kube-proxy-l6k6x                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 kube-scheduler-k8s-master-50                100m (5%)     0 (0%)      0 (0%)           0 (0%)         47h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                1 (50%)     0 (0%)
  memory             140Mi (0%)  340Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From                       Message
  ----    ------                   ----               ----                       -------
  Normal  Starting                 46m                kubelet, k8s-master-50     Starting kubelet.
  Normal  NodeHasSufficientMemory  46m (x8 over 46m)  kubelet, k8s-master-50     Node k8s-master-50 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    46m (x8 over 46m)  kubelet, k8s-master-50     Node k8s-master-50 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     46m (x7 over 46m)  kubelet, k8s-master-50     Node k8s-master-50 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  46m                kubelet, k8s-master-50     Updated Node Allocatable limit across pods
  Normal  Starting                 46m                kube-proxy, k8s-master-50  Starting kube-proxy.
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl describe pods/tomcat9-run-6595fb5f85-gtxdp
Name:         tomcat9-run-6595fb5f85-gtxdp
Namespace:    default
Priority:     0
Node:         k8s-node52/10.0.2.15
Start Time:   Thu, 28 Dec 2023 16:25:57 +0800
Labels:       pod-template-hash=6595fb5f85
              run=tomcat9-run
Annotations:  cni.projectcalico.org/podIP: 10.81.237.194/32
              cni.projectcalico.org/podIPs: 10.81.237.194/32
Status:       Running
IP:           10.81.237.194
IPs:
  IP:           10.81.237.194
Controlled By:  ReplicaSet/tomcat9-run-6595fb5f85
Containers:
  tomcat9-run:
    Container ID:   docker://7a6a24e758c62d5a5e9b8075256bb1d63f252c3b438e1484b62d160b7f4dfa7f
    Image:          tomcat:9.0.20-jre8-alpine
    Image ID:       docker-pullable://tomcat@sha256:17accf0afeeecce0310d363490cd60a788aa4630ab9c9c802231d6fbd4bb2375
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 29 Dec 2023 09:54:33 +0800
    Last State:     Terminated
      Reason:       Error
      Exit Code:    143
      Started:      Thu, 28 Dec 2023 16:25:59 +0800
      Finished:     Thu, 28 Dec 2023 17:36:13 +0800
    Ready:          True
    Restart Count:  1
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason          Age                From                 Message
  ----    ------          ----               ----                 -------
  Normal  SandboxChanged  48m (x2 over 49m)  kubelet, k8s-node52  Pod sandbox changed, it will be killed and re-created.
  Normal  Pulled          48m                kubelet, k8s-node52  Container image "tomcat:9.0.20-jre8-alpine" already present on machine
  Normal  Created         48m                kubelet, k8s-node52  Created container tomcat9-run
  Normal  Started         48m                kubelet, k8s-node52  Started container tomcat9-run
[root@k8s-master-50 ~]# 

[root@k8s-master-50 ~]# kubectl describe pods  tomcat9-run-6595fb5f85-fls4k
Name:         tomcat9-run-6595fb5f85-fls4k
Namespace:    default
Priority:     0
Node:         k8s-node51/10.0.2.15
Start Time:   Thu, 28 Dec 2023 16:18:50 +0800
Labels:       pod-template-hash=6595fb5f85
              run=tomcat9-run
Annotations:  cni.projectcalico.org/podIP: 10.81.92.66/32
              cni.projectcalico.org/podIPs: 10.81.92.66/32
Status:       Running
IP:           10.81.92.66
IPs:
  IP:           10.81.92.66
Controlled By:  ReplicaSet/tomcat9-run-6595fb5f85
Containers:
  tomcat9-run:
    Container ID:   docker://180f29f74c2ff39f40c7618b08bf735c684109beec52e60a0aad4d0188ec4279
    Image:          tomcat:9.0.20-jre8-alpine
    Image ID:       docker-pullable://tomcat@sha256:17accf0afeeecce0310d363490cd60a788aa4630ab9c9c802231d6fbd4bb2375
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 29 Dec 2023 09:53:52 +0800
    Last State:     Terminated
      Reason:       Error
      Exit Code:    143
      Started:      Thu, 28 Dec 2023 16:18:51 +0800
      Finished:     Thu, 28 Dec 2023 17:36:13 +0800
    Ready:          True
    Restart Count:  1
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>
[root@k8s-master-50 ~]# 
```

说明：

`kubectl get`命令通常用于检索同一资源类型的一个或者多个资源，它具有丰富的参数，允许使用-o或者--output参数自定义输出格式，也可以指定-w或者--watch开始观察特定对象的一时半会儿。

`kubectl describe`命令更侧重于描述指定资源的许多方面，它可以调用对API服务的多个API调用来为用户构建视图。例如: `kubectl describe node`命令不仅检索有关节点信息，还检索在其上运行的pod的摘要，为节点生成事件等。

### 进入容器命令

kubectl exec - 对pod中的容器执行命令。 与docker的exec命令非常类似

```sh
# 从pod <pod-name> 中获取运行‘date’的输出。默认情况下，输出来自第一个容器
kubectl exec <pod-name> date


# 在指定的pod(<tpod-name>)和指定的容器(<container-name>)进行输出
kubectl exec <pod-name> -c <container-name> date

# 使用一个交互的TTY并运行/bin/bash.默认情况下，输出来自第一个容器
kubectl exec -it <pod-name> /bin/bash
```



输出:

```sh
[root@k8s-master-50 ~]# kubectl exec tomcat9-run-6595fb5f85-xn8qs date
Sat Dec 30 11:10:22 UTC 2023
[root@k8s-master-50 ~]# kubectl exec tomcat9-run-6595fb5f85-nb2lq -c tomcat9-run date
Sat Dec 30 11:33:12 UTC 2023
[root@k8s-master-50 ~]# kubectl exec -it tomcat9-run-6595fb5f85-xn8qs bash
bash-4.4# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
4: eth0@if3248: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1440 qdisc noqueue state UP 
    link/ether 0a:49:be:8d:69:30 brd ff:ff:ff:ff:ff:ff
    inet 10.81.149.196/32 scope global eth0
       valid_lft forever preferred_lft forever
bash-4.4# exit
exit
```



### logs命令

kubectl logs - 打印Pod中容器的日志

```sh
# 从pod返回日志快照。
kubectl logs <pod-name>

# 从<pod-name>开始流程式传输日志，这类似于linux的'tail -f'命令
kubectl logs -f <pod-name>
```

输出

```sh
[root@k8s-master-50 ~]# kubectl logs tomcat9-run-6595fb5f85-nb2lq
30-Dec-2023 11:24:33.714 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version name:   Apache Tomcat/9.0.20
30-Dec-2023 11:24:33.721 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server built:          May 3 2019 22:26:00 UTC
30-Dec-2023 11:24:33.721 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version number: 9.0.20.0
......
30-Dec-2023 11:24:34.486 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["http-nio-8080"]
30-Dec-2023 11:24:34.491 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["ajp-nio-8009"]
30-Dec-2023 11:24:34.492 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in [482] milliseconds
[root@k8s-master-50 ~]# kubectl logs -f tomcat9-run-6595fb5f85-nb2lq
30-Dec-2023 11:24:33.714 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version name:   Apache Tomcat/9.0.20
30-Dec-2023 11:24:33.721 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server built:          May 3 2019 22:26:00 UTC
30-Dec-2023 11:24:33.721 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version number: 9.0.20.0
......
30-Dec-2023 11:24:34.484 INFO [main] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory [/usr/local/tomcat/webapps/manager] has finished in [50] ms
30-Dec-2023 11:24:34.486 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["http-nio-8080"]
30-Dec-2023 11:24:34.491 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["ajp-nio-8009"]
30-Dec-2023 11:24:34.492 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in [482] milliseconds
^C
```

### 格式化输出:

```sh
#将pod信息格式化输出到一个yaml文件中
kubectl get pod <pod-name> -o yaml
```

输出：

```yaml
[root@k8s-master-50 ~]# kubectl get pod tomcat9-run-6595fb5f85-nb2lq -o yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    cni.projectcalico.org/podIP: 10.81.92.68/32
    cni.projectcalico.org/podIPs: 10.81.92.68/32
  creationTimestamp: "2023-12-30T11:24:31Z"
  generateName: tomcat9-run-6595fb5f85-
  labels:
    pod-template-hash: 6595fb5f85
    run: tomcat9-run
  name: tomcat9-run-6595fb5f85-nb2lq
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: tomcat9-run-6595fb5f85
    uid: a681fe0d-f5d7-4974-9a6c-1539a55facd8
  resourceVersion: "220905"
  selfLink: /api/v1/namespaces/default/pods/tomcat9-run-6595fb5f85-nb2lq
  uid: c495c84b-4f75-4a84-a7cb-56db195d44a8
spec:
  containers:
  - image: tomcat:9.0.20-jre8-alpine
    imagePullPolicy: IfNotPresent
    name: tomcat9-run
    ports:
    - containerPort: 8080
      protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-bgthv
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: k8s-node51
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: default-token-bgthv
    secret:
      defaultMode: 420
      secretName: default-token-bgthv
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2023-12-30T11:24:31Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2023-12-30T11:24:33Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2023-12-30T11:24:33Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2023-12-30T11:24:31Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://6a4c1088bd02c51c8a54d37e221b9211ee0f84d340b414a9fdc471421abf1666
    image: tomcat:9.0.20-jre8-alpine
    imageID: docker-pullable://tomcat@sha256:17accf0afeeecce0310d363490cd60a788aa4630ab9c9c802231d6fbd4bb2375
    lastState: {}
    name: tomcat9-run
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2023-12-30T11:24:33Z"
  hostIP: 10.0.3.5
  phase: Running
  podIP: 10.81.92.68
  podIPs:
  - ip: 10.81.92.68
  qosClass: BestEffort
  startTime: "2023-12-30T11:24:31Z"
[root@k8s-master-50 ~]#
```



### **delete命令**

kubectl delete 从文件、stdin或者指定标签选择器、名称、资源选择器或资源中删除资源

```sh
# 使用pod.yaml文件中类型和名称删除pod
kubectl delete -f pod.yaml

# 删除标签名=<lab-name>的所有pod和服务
kubectl delete pods,services -l name=<label-name>

# 删除所有具有标签名称=<label-name>的pod和服务，包括未初始化的那些
kubectl delete pods,services -l name=<label-name> --include-uninitialized

# 删除所有pod 包括未初始化的pod
kubectl delete pods --all

# 强制删除一个pod，需要添加参数
--force --grace-period=0
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl delete pod tomcat9-run-6595fb5f85-wdmxg
pod "tomcat9-run-6595fb5f85-wdmxg" deleted

[root@k8s-master-50 ~]# kubectl delete pod  tomcat9-run-6595fb5f85-nb2lq  --force --grace-period=0
warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
pod "tomcat9-run-6595fb5f85-nb2lq" force deleted
```



### 资源缩写

| 资源名                   | 缩写名 | API分组                   | 按命名空间 | 资源类型                |
| ------------------------ | ------ | ------------------------- | ---------- | ----------------------- |
| configmaps               | cm     |                           | true       | ConfigMap               |
| namespaces               | ns     |                           | false      | Namespace               |
| nodes                    | no     |                           | false      | Node                    |
| persistentvolumeclaims   | pvc    |                           | true       | PersistentVolumeClaim   |
| persistentvolumes        | pv     |                           | false      | PersistentVolume        |
| pods                     | po     |                           | true       | Pod                     |
| secrets                  |        |                           | true       | Secret                  |
| serviceaccounts          | sa     |                           | true       | ServiceAccount          |
| services                 | svc    |                           | true       | Service                 |
| daemonsets               | ds     | apps                      | true       | DaemonSet               |
| horizontalpodautoscalers | hpa    | autoscaling               | true       | HorizontalPodAutoscaler |
| cronjobs                 | cj     | batch                     | true       | CronJob                 |
| jobs                     |        | batch                     | true       | Job                     |
| ingresses                | ing    | extensions                | true       | ingress                 |
| poddisruptionbudgets     | pod    | policy                    | true       | PodDisruptionBudget     |
| clusterrolebindings      |        | rbac.authorization.k8s.io | false      | ClusterRoleBinding      |
| Clusterroles             |        | rbac.authorization.k8s.io | false      | clusterRole             |
| rolebindings             |        | rbac.authorization.k8s.io | true       | RoleBinding             |
| roles                    |        | rbac.authorization.k8s.io | true       | Role                    |
| storageclasses           | sc     | storage.k8s.io            | false      | StorageClass            |







## 使用yml文件配制k8s

### 使用yaml文件配制namespace

nullnullnamespace.yml

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: nullnull
```

远程运行

```sh
mkdir -p /data/k8s/namespace
cd /data/k8s/namespace
# 将文件上传到此目录

# 运行创建namespace的yml文件
kubectl apply -f nullnullnamespace.yml


# 可以运行删除Namespace
kubectl delete -f nullnullnamespace.yml
```

输出：

```sh
[root@k8s-master-50 namespace]# kubectl apply -f nullnullnamespace.yml
namespace/nullnull created
[root@k8s-master-50 namespace]# kubectl get namespace
NAME              STATUS   AGE
default           Active   3d13h
kube-node-lease   Active   3d13h
kube-public       Active   3d13h
kube-system       Active   3d13h
nullnull          Active   56s
[root@k8s-master-50 namespace]# kubectl delete -f nullnullnamespace.yml 
namespace "nullnull" deleted
[root@k8s-master-50 namespace]# kubectl get namespace
NAME              STATUS   AGE
default           Active   3d13h
kube-node-lease   Active   3d13h
kube-public       Active   3d13h
kube-system       Active   3d13h
[root@k8s-master-50 namespace]# 
```

### 使用yaml文件创建pod

nullnullpod.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tomcat9
  labels:
    app: tomcat9
 spec:
    containers:
      - name: tomcat9
        image: tomcat:9.0.20-jre8-alpine
        imagePullPolicy: ifNotPresent
    restartPolicy: Always
```

imagePullPolicy:

```sh
# 镜像下载的策略：
imagePullPolicy:
	Always: 总是拉取
	IfNotPresent: 如果本地有镜像，使用本地，如果本地没有镜像，下载镜像。
	Never: 只使用本地镜像，从不拉取
```

restartPolicy:

```sh
restartPolicy:
	Always: 只要退出就重启
	OnFailure: 失败退出时（exit code 不为0）才重启。
	Never: 永不重启
```

远程运行

```sh
mkdir -p /data/k8s/pod
cd /data/k8s/pod
# 将文件上传到此目录

# 运行pod
kubectl apply -f nullnullpod.yml

# 删除pod
kubectl delete -f nullnullpod.yml
```

输出:

```sh
[root@k8s-master-50 pod]# kubectl apply -f nullnullpod.yml
pod/tomcat9 created
[root@k8s-master-50 pod]# kubectl get pod
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9                        1/1     Running   0          12s
tomcat9-run-6595fb5f85-7d2xw   1/1     Running   0          4h45m
tomcat9-run-6595fb5f85-w96pz   1/1     Running   0          4h44m
tomcat9-run-6595fb5f85-xn8qs   1/1     Running   0          5h23m
[root@k8s-master-50 pod]# kubectl get pod -o wide
NAME                           READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES
tomcat9                        1/1     Running   0          28s     10.81.149.197   k8s-node53   <none>           <none>
tomcat9-run-6595fb5f85-7d2xw   1/1     Running   0          4h45m   10.81.92.69     k8s-node51   <none>           <none>
tomcat9-run-6595fb5f85-w96pz   1/1     Running   0          4h44m   10.81.237.197   k8s-node52   <none>           <none>
tomcat9-run-6595fb5f85-xn8qs   1/1     Running   0          5h23m   10.81.149.196   k8s-node53   <none>           <none>
[root@k8s-master-50 pod]# curl 10.81.149.197:8080



<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        <div id="wrapper">
           	......
            <p class="copyright">Copyright &copy;1999-2023 Apache Software Foundation.  All Rights Reserved</p>
        </div>
    </body>

</html>
[root@k8s-master-50 pod]# kubectl delete -f nullnullpod.yml 
pod "tomcat9" deleted
[root@k8s-master-50 pod]# kubectl get pod -o wide
NAME                           READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES
tomcat9-run-6595fb5f85-7d2xw   1/1     Running   0          4h48m   10.81.92.69     k8s-node51   <none>           <none>
tomcat9-run-6595fb5f85-w96pz   1/1     Running   0          4h46m   10.81.237.197   k8s-node52   <none>           <none>
tomcat9-run-6595fb5f85-xn8qs   1/1     Running   0          5h25m   10.81.149.196   k8s-node53   <none>           <none>
```



### 使用yml文件Deployment

nullnulldeployment.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tomcat-deployment
  labels:
    apps: tomcat-deployment
spec:
  replicas: 3
  template:
    metadata:
      name: tomcat-deployment
      labels:
        app: tomcat
    spec:
      containers:
        - name: tomcat-deployment
          image: tomcat:9.0.20-jre8-alpine
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
  selector:
    matchLabels:
      app: tomcat
```

matchLabels

```sh
# 总结：
# 在Deployment中必须写matchLabels
# 在定义模板的时候必须定义labels，因为Deployment.spec.selector是必须的字段，而他又必须和template.labels对应。
```

运行deployment

```sh
mkdir -p /data/k8s/deployment
cd /data/k8s/deployment


kubectl apply -f nullnulldeployment.yml

# 查看deployment
kubectl get deployment -o wide

# 删除deployment
kubectl delete -f  nullnulldeployment.yml

# 再次查看deployment
kubectl get deployment -o wide
```

输出

```sh
[root@k8s-master-50 deployment]# kubectl apply -f nullnulldeployment.yml 
deployment.apps/tomcat-deployment created
[root@k8s-master-50 deployment]# kubectl get deployment -o wide
NAME                READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS          IMAGES                      SELECTOR
tomcat-deployment   3/3     3            3           58s   tomcat-deployment   tomcat:9.0.20-jre8-alpine   app=tomcat
[root@k8s-master-50 deployment]# kubectl delete -f  nullnulldeployment.yml
deployment.apps "tomcat-deployment" deleted
[root@k8s-master-50 deployment]# kubectl get deployment -o wide
No resources found in default namespace.
[root@k8s-master-50 deployment]# 
```

**控制器类型**

| 控制器名称  | 作用                                                         |
| ----------- | ------------------------------------------------------------ |
| Deployment  | 声明式更新控制器，用于发布无状态应用                         |
| ReplicaSet  | 副本集控制器，用于对Pod进行副本规模扩大和剪裁                |
| StatefulSet | 有状态的副本集，用于发布有状态的应用                         |
| DaemonSet   | 在k8s集群每个上node运行一个副本，用于发布监控或者日志收集类的应用。 |
| Job         | 运行一次性作业任务                                           |
| CronJob     | 运行周期性作业任务                                           |

Deployment控制器介绍：

具有上线部署、滚动升级、创建副本、回滚到以前某一版本（成功/稳定）等功能

Deployment包含ReplicaSet，除非需要自定义升级功能或者根本不需要升级Pod，否则还是建议使用Deployment而不是直接使用ReplicaSet



### 使用yml文件创建service

nullnullservice.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tomcat-deploy
  labels:
    app: tomcat-deploy
spec:
  replicas: 1
  template:
    metadata:
      name: tomcat-deploy
      labels:
        app: tomcat-pod
    spec:
      containers:
        - name: tomcat-deploy
          image: tomcat:9.0.20-jre8-alpine
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
      restartPolicy: Always
  selector:
    matchLabels:
      app: tomcat-pod

---
apiVersion: v1
kind: Service
metadata:
  name: tomcat-svc
spec:
  selector:
    app: tomcat-pod
  ports:
    - port: 8888
      targetPort: 8080
      nodePort: 30088
      protocol: TCP
  type: NodePort
```

servicer的selector

```sh
service.spec.selector.app 选择的内容仍是template.label.app的内容，不是deployment控制器的label内容。
```

service类型

```sh
ClusterIP: 默认，分配一个集群内部可以访问的虚拟IP
NodePort： 在每个Node上分配一个端口作业外部访问入口 
LoadBalancer: 工作在特定的Cloud Provider上，例如：Google Cloud,AWS, openStack
ExternalName: 表示把集群外部的服务引入到集群内部中来，即实现了集群内部pod和集群外部的服务进行通信。
```

service参数：

```sh
port: 访问service使用的端口
targetPort: pod中容器端口
nodePort: 通过Node实现外网用户访问k8s集群内service(30000-32767)
```

运行

```sh
mkdir -p /data/k8s/service
cd /data/k8s/service


kubectl apply -f nullnullservice.yml

# 查看service
kubectl get service -o wide

# 访问测试
curl *.*.*.*: xxxx

# 删除servioce
kubectl delete -f  nullnullservice.yml

# 再次查看service
kubectl get service -o wide
```

输出：

```sh
[root@k8s-master-50 service]# kubectl apply -f nullnullservice.yml
deployment.apps/tomcat-deploy created
service/tomcat-svc created
[root@k8s-master-50 service]# kubectl get service -o wide
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE     SELECTOR
kubernetes    ClusterIP   10.1.0.1       <none>        443/TCP          4d13h   <none>
tomcat-svc    NodePort    10.1.77.65     <none>        8888:30088/TCP   25s     app=tomcat-pod
[root@k8s-master-50 service]# curl http://192.168.5.50:30088



<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        <div id="wrapper">
            ......
        </div>
    </body>

</html>
[root@k8s-master-50 service]# kubectl delete -f  nullnullservice.yml
deployment.apps "tomcat-deploy" deleted
service "tomcat-svc" deleted
[root@k8s-master-50 service]# kubectl get service -o wide
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
kubernetes   ClusterIP   10.1.0.1     <none>        443/TCP   4d13h   <none>

```



## 资源清单

### pod

资源清单有5 个顶组的字段组成： apiVersion、kind、metadata、spec、status。

```yaml
# 如果没有给定group名称，那么默认为core,可以使用kubectl apiversions 获取当前k8s版本上所有的apiVersion的版本信息（每个版本可能不同）
apiVersion: group/apiversion 
# 资源类型
kind: 

# 资源元数据信息
metadata:
  name:
  namespace:
  labes:
  # 主要目的是方便用户阅读查找
  annotations:

# 期望的状态(disired state)
spec:

# 当前的状态，本字段有kubernetes自身维护，用户不用去定义
status
```

使用kubectl命令可以查看apiVersion的各个版本信息

```sh
[root@k8s-master-50 ~]# kubectl api-versions
admissionregistration.k8s.io/v1
admissionregistration.k8s.io/v1beta1
apiextensions.k8s.io/v1
apiextensions.k8s.io/v1beta1
apiregistration.k8s.io/v1
apiregistration.k8s.io/v1beta1
apps/v1
authentication.k8s.io/v1
authentication.k8s.io/v1beta1
authorization.k8s.io/v1
authorization.k8s.io/v1beta1
autoscaling/v1
autoscaling/v2beta1
autoscaling/v2beta2
batch/v1
batch/v1beta1
certificates.k8s.io/v1beta1
coordination.k8s.io/v1
coordination.k8s.io/v1beta1
crd.projectcalico.org/v1
discovery.k8s.io/v1beta1
events.k8s.io/v1beta1
extensions/v1beta1
networking.k8s.io/v1
networking.k8s.io/v1beta1
node.k8s.io/v1beta1
policy/v1beta1
rbac.authorization.k8s.io/v1
rbac.authorization.k8s.io/v1beta1
scheduling.k8s.io/v1
scheduling.k8s.io/v1beta1
storage.k8s.io/v1
storage.k8s.io/v1beta1
v1
[root@k8s-master-50 ~]#
```

获取字段设置的帮助文档

```sh
[root@k8s-master-50 ~]# kubectl explain pod
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status       <Object>
     Most recently observed status of the pod. This data may not be up to date.
     Populated by the system. Read-only. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

[root@k8s-master-50 ~]# kubectl explain namespace
KIND:     Namespace
VERSION:  v1

DESCRIPTION:
     Namespace provides a scope for Names. Use of multiple namespaces is
     optional.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Spec defines the behavior of the Namespace. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status       <Object>
     Status describes the current status of a Namespace. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

[root@k8s-master-50 ~]# kubectl explain deployment
KIND:     Deployment
VERSION:  apps/v1

DESCRIPTION:
     Deployment enables declarative updates for Pods and ReplicaSets.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object metadata.

   spec <Object>
     Specification of the desired behavior of the Deployment.

   status       <Object>
     Most recently observed status of the Deployment.

[root@k8s-master-50 ~]# kubectl explain service
KIND:     Service
VERSION:  v1

DESCRIPTION:
     Service is a named abstraction of software service (for example, mysql)
     consisting of local port (for example 3306) that the proxy listens on, and
     the selector that determines which pods will answer requests sent through
     the proxy.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Spec defines the behavior of a service.
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status       <Object>
     Most recently observed status of the service. Populated by the system.
     Read-only. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

[root@k8s-master-50 ~]# 
```

字段配制的格式

```sh
<map[String]string><[]string><[]Object>

apiVersion <string> # 表示字符串类型
metadata <Object> # 表示需要嵌套多层字段
labels <map[String]string> #表示由k:v组成的映射
finalizers <[]string>      # 表示字符串列表
ownerReferences <[]Object> # 表示对象列表
hostPid <boolean>    # 布尔类型
priority <Integer>   # 整型
name <string>  -required- # 如果类型后面接-required- 表示为必填字段
```

![image-20240103123031953](.\images\image-20240103123031953.png)

### init C案例

准备镜像

```sh
docker pull busybox:1.32.0
docker pull nginx:1.17.10-alpine
```

initC的特点：

1. initC总是运行到成功为止。
2. 每个init C容器都必须在下一下init启动之前成功完成。
3. 如果initC容器运行失败，K8S会不断的重启该POD，直到initC容器成功为止
4. 如果pod对应的restartPolicy为never,它就不会重新启动。



initcpod.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
    - name: myapp-container
      image: busybox:1.32.0
      imagePullPolicy: IfNotPresent
      # 整个命令的意思是：打印“the app is running”，然后等待1小时。
      # 如果“echo the app is running”命令成功执行（返回值为0），那么sleep 3600命令才会被执行。
      command: ['sh','-c','echo the app is running && sleep 3600']
  initContainers:
    - name: init-myservice
      image: busybox:1.32.0
      imagePullPolicy: IfNotPresent
      # 等待某个服务变得可用，等待一个 DNS 记录被创建或更新，以便其他服务可以解析并使用它。
      command: ['sh','-c','until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
    - name: init-mydb
      image: busybox:1.32.0
      command: ['sh','-c','until nslookup mydb; do echo waiting for mydb; sleep 2; done;']
```

init-myservice.yml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
```

init-mydb.yml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9377
```



执行命令:

```sh
# 将文件上传k8s服务器，准备执行


# 启动initcpod的服务
kubectl apply -f initcpod.yml

# 查看pod的启动情况
kubectl get pods

# 查看详细的pod的启动情况
kubectl describe pod myapp-pod

# 查看myapp-pod中的第一个initcontainer日志
kubectl logs myapp-pod -c init-myservice


# 运行init-myservice服务
kubectl apply -f init-myservice.yml


# 查看init-myservice服务的运行情况
kubectl get svc

# 查看myapp-pod的运行情况，需要等一会，会发现pod的第一个init已经就绪
kubectl get pods

# 运行mydb服务
kubectl apply -f init-mydb.yml

# 查看mydb服务的运行情况
kubectl get svc

# 检查mydb的运行信息
kubectl get pods -w
```



输出：

```sh
[root@k8s-master-50 initc]# kubectl apply -f initcpod.yml
pod/myapp-pod created
[root@k8s-master-50 initc]# kubectl get pods
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          6s
[root@k8s-master-50 initc]# kubectl describe pod myapp-pod
Name:         myapp-pod
Namespace:    default
Priority:     0
Node:         k8s-node52/10.0.3.6
Start Time:   Wed, 03 Jan 2024 18:50:07 +0800
Labels:       app=myapp
Annotations:  cni.projectcalico.org/podIP: 10.81.237.200/32
              cni.projectcalico.org/podIPs: 10.81.237.200/32
              kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"app":"myapp"},"name":"myapp-pod","namespace":"default"},"spec":{"c...
Status:       Pending
IP:           10.81.237.200
IPs:
  IP:  10.81.237.200
Init Containers:
  init-myservice:
    Container ID:  docker://5293d13fac38790005149034f193d53924e23e99431d0bb21057c67e3b5ebd42
    Image:         busybox:1.32.0
    Image ID:      docker-pullable://busybox@sha256:bde48e1751173b709090c2539fdf12d6ba64e88ec7a4301591227ce925f3c678
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      until nslookup myservice; do echo waiting for myservice; sleep 2; done;
    State:          Running
      Started:      Wed, 03 Jan 2024 18:50:26 +0800
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
  init-mydb:
    Container ID:  
    Image:         busybox:1.32.0
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      until nslookup mydb; do echo waiting for mydb; sleep 2; done;
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Containers:
  myapp-container:
    Container ID:  
    Image:         busybox:1.32.0
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      echo the app is running && sleep 3600
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       False 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From                 Message
  ----    ------     ----  ----                 -------
  Normal  Scheduled  65s   default-scheduler    Successfully assigned default/myapp-pod to k8s-node52
  Normal  Pulling    63s   kubelet, k8s-node52  Pulling image "busybox:1.32.0"
  Normal  Pulled     46s   kubelet, k8s-node52  Successfully pulled image "busybox:1.32.0"
  Normal  Created    46s   kubelet, k8s-node52  Created container init-myservice
  Normal  Started    46s   kubelet, k8s-node52  Started container init-myservice
  
  # 可以发现，正在初始化init-myservice,但由于现在init-myservice还没有启动，肯定是启动不成功的
  
  [root@k8s-master-50 initc]# kubectl logs myapp-pod -c init-myservice
Server:         10.1.0.10
Address:        10.1.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer

waiting for myservice
Server:         10.1.0.10
Address:        10.1.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer

waiting for myservice
Server:         10.1.0.10
Address:        10.1.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer

waiting for myservice
Server:         10.1.0.10
Address:        10.1.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer

waiting for myservice
Server:         10.1.0.10
Address:        10.1.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer

# 可以发现，由于服务没有启动，在不断的打印日志，输出

[root@k8s-master-50 initc]# kubectl apply -f init-myservice.yml
service/init-myservice created
[root@k8s-master-50 initc]# kubectl get svc
NAME             TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
init-myservice   ClusterIP   10.1.11.253   <none>        80/TCP    37s
kubernetes       ClusterIP   10.1.0.1      <none>        443/TCP   7d7h
# 第一个初始化服务已经成功

[root@k8s-master-50 ~]# kubectl get pods 
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          6m58s

# 此时服务还未检测到，第一阶段还是失败状态。

[root@k8s-master-50 ~]# kubectl get pods -w
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          28m
myapp-pod   0/1     Init:1/2   0          28m
myapp-pod   0/1     Init:1/2   0          28m

# 启动了初始化的服务1，init已经成功了一半了

[root@k8s-master-50 initc]# kubectl apply -f init-mydb.yml
service/mydb created
[root@k8s-master-50 initc]# kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.1.0.1       <none>        443/TCP   7d8h
mydb         ClusterIP   10.1.251.136   <none>        80/TCP    6s
myservice    ClusterIP   10.1.191.192   <none>        80/TCP    5m2s
[root@k8s-master-50 initc]# kubectl get pods -w
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:1/2   0          32m
myapp-pod   0/1     PodInitializing   0          33m
myapp-pod   1/1     Running           0          33m
  
```

至此initC的案例已经成功的实现了。



## 结束



