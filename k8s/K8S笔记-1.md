# K8S笔记

## Kubernetes安装

**硬件要求**

| 硬件 | 要求    |
| ---- | ------- |
| CPU  | 至少2核 |
| 内存 | 至少3G  |
| 硬件 | 至少50G |

**节点信息**

| 主机名        | IP           |
| ------------- | ------------ |
| k8s-master-50 | 192.168.5.50 |
| k8s-node-51   | 192.168.5.51 |
| k8s-node-52   | 192.168.5.52 |
| k8s-node-53   | 192.168.5.53 |

系统要求：

推荐使用centos7.7及以上版本

国内建议使用阿里云下载

```http
http://mirrors.aliyun.com/centos/7/isos/x86_64/
```

### 配制阿里云yum源

```sh
# 1. 下载安装wget
yum install -y wget

#2.备份默认的yum
mv /etc/yum.repos.d /etc/yum.repos.d.backup

#3.设置新的yum目录
mkdir -p /etc/yum.repos.d

#4.下载阿里yum配置到该目录中，选择对应版本
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo

#5.更新epel源为阿里云epel源
mv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backup
mv /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backup

wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo

#6.重建缓存
yum clean all
yum makecache

#7.看一下yum仓库有多少包
yum repolist
yum update
```

可以做一个系统快照版本，防止后面的翻车，以做好回滚，不用从头开始。做系统快照建议关机做，关机后的快照特别小。

### 升级系统内核

默认此处安装的7.9版本，内核版本为3.10，需要做下升级,此安装完成后，系统内核版本需要升级到了`4.4`，但别太高，不然操作不一致。

```sh
# 导入仓库源
rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm

# 查看可安装的软件包
yum --enablerepo="elrepo-kernel" list --showduplicates | sort -r | grep kernel-lt.x86_64

# 升级到最新内核版本的方法
yum --enablerepo=elrepo-kernel install -y kernel-lt


grep initrd16 /boot/grub2/grub.cfg
grub2-set-default 0

reboot
```

### 查看相关的系统信息

```sh
# 查看centos系统内核命令
uname -r
uname -a

# 查看CPU
lscpu

# 查看内存
free 
free -g

# 查看硬盘信息
fdisk -l
```

### centos7系统配制

```sh
# 1. 开发环境可以直接关闭防火墙，但生产环境，切记，万不能关
systemctl stop firewalld
systemctl disable firewalld


# 2. 关闭selinux
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
setenforce 0


# 3. 网桥过滤
vi /etc/sysctl.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-arptables = 1
net.ipv4.ip_forward=1
net.ipv4.ip_forward_use_pmtu = 0
#生效命令
sysctl --system
#查看效果
sysctl -a|grep "ip_forward"



# 4.  开启IPVS
#安装IPVS
yum -y install ipset ipvsadm -y
#编译ipvs.modules文件
vi /etc/sysconfig/modules/ipvs.modules
#文件内容如下
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
# 在4.4内核版本中使用nf_conntrack_ipv4
modprobe -- nf_conntrack_ipv4
#赋予权限并执行
chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack
#重启电脑，检查是否生效
reboot
lsmod | grep ip_vs_rr



# 5. 同步时间
#安装软件
yum -y install ntpdate
#向阿里云服务器同步时间
ntpdate time1.aliyun.com
#删除本地时间并设置时区为上海
rm -rf /etc/localtime
ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
#查看时间
date -R || date



# 6.命令补全
#安装bash-completion
yum -y install bash-completion bash-completion-extras
#使用bash-completion
source /etc/profile.d/bash_completion.sh



# 7. 关闭swap分区
#临时关闭：
swapoff -a
#永久关闭：
vi /etc/fstab
将文件中的/dev/mapper/centos-swap这行代码注释掉
#/dev/mapper/centos-swap swap swap defaults 0 0
#确认swap已经关闭：若swap行都显示 0 则表示关闭成功
free -m


# 8. hosts配置
#文件内容如下:
cat <<EOF >> /etc/hosts
192.168.5.50 k8s-master-50
192.168.5.51 k8s-node-51
192.168.5.52 k8s-node-52
192.168.5.53 k8s-node-53
EOF

cat /etc/hosts
```

### 安装docker

```sh
# 1. 安装docker的前置条件
yum install -y yum-utils device-mapper-persistent-data lvm2

# 2. 添加源
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum makecache fast

# 3. 查看docker更新版本
yum list docker-ce --showduplicates | sort -r


# 4. 安装docker版本,这里需要指定版本为18.09.8
yum -y install docker-ce-18.09.8
# 如果需要安装最新版本，使用
# yum -y install docker-ce
# 开启docker服务开机启动
systemctl start docker
systemctl status docker
# 查看docker版本
docker version
# docker-client版本：当前最新版本
# docker-server需要为版本为：18.09.8

# 5. 阿里云镜像加速器地址
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://ys2mfbsh.mirror.aliyuncs.com"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker

# 6. 设置为开机自启动
systemctl enable docker

# 7. 修改Cgroup Driver
vi /etc/docker/daemon.json
# 修改daemon.json，新增：
"exec-opts": ["native.cgroupdriver=systemd"]
# 修改cgroupdriver是为了消除安装k8s集群时的告警：
# [WARNING IsDockerSystemdCheck]: 
# detected “cgroupfs” as the Docker cgroup driver. The recommended driver is “systemd”. 
# Please follow the guide at https://kubernetes.io/docs/setup/cri/......
#重启docker服务：
systemctl daemon-reload 
systemctl restart docker
#查看修改后状态：
docker info | grep Cgroup


# 查看docker的信息
docker -v
docker info
```



### 使用kubeadm安装k8s

| 软件 | kubeadm                        | kubelet                                                    | kubectl                             | docker-ce |
| ---- | ------------------------------ | ---------------------------------------------------------- | ----------------------------------- | --------- |
| 版本 | 初始化集群管理.<br/>版本1.17.5 | 用于接收api-server指令对pod生命周期进行管理<br/>版本1.17.5 | 集群命令行管理工具<br/>版本：1.17.5 | 18.09.8   |

```sh
# 1. 安装yum源
vi /etc/yum.repos.d/kubernates.repo
# 加入内容
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
       
# 2. 更新缓存
yum clean all
yum -y makecache

# 3. 验证是否可用
yum list | grep kubeadm
#如果提示要验证yum-key.gpg是否可用，输入y。
#查找到kubeadm。显示版本

# 4. 查看k8s的版本
yum list kubelet --showduplicates | sort -r

# 5. 安装k8s-1.17.5
yum install -y kubelet-1.17.5 kubeadm-1.17.5 kubectl-1.17.5


# 6. 设置kubelet
# 增加配置信息
# 如果不配置kubelet，可能会导致K8S集群无法启动。为实现docker使用的cgroupdriver与kubelet使用的cgroup的一致性。
vi /etc/sysconfig/kubelet
KUBELET_EXTRA_ARGS="--cgroup-driver=systemd"


# 7. 设置开机启动
systemctl enable kubelet
```



### 初始化镜像

如果是首次安装k8s，手里没有备份好的镜像，需要执行此操作，如果已经已经有了镜像备份，请跳过本章节

```sh
# 查看安装集群需要镜像
kubeadm config images list
# 可得到如下这样一个列表
k8s.gcr.io/kube-apiserver:v1.17.5
k8s.gcr.io/kube-controller-manager:v1.17.5
k8s.gcr.io/kube-scheduler:v1.17.5
k8s.gcr.io/kube-proxy:v1.17.5
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.4.3-0
k8s.gcr.io/coredns:1.6.5
```

使用脚本下载镜像

```sh
mkdir -p /data/k8s
cd /data/k8s
# 编辑内容
vi images.sh
```

脚本内容:

从阿里云下载镜像，再下载的镜像应该去除"k8s.gcr.io"的前缀，版本换成kubeadm config images list命令获取到的版本

```shell
#!/bin/bash
images=(
   kube-apiserver:v1.17.5
   kube-controller-manager:v1.17.5
   kube-scheduler:v1.17.5
   kube-proxy:v1.17.5
   pause:3.1
   etcd:3.4.3-0
   coredns:1.6.5
)
for imageName in ${images[@]} ; 
do
   docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
   docker tag  registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName
   docker rmi  registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
done
```



执行脚本保存镜像

```sh
cd /data/k8s
# 给脚本授权
chmod +x images.sh 
# 执行脚本
./images.sh 

# 查看镜像
docker images

# 保存镜像
docker save -o k8s-1.17.5-all.image           \
k8s.gcr.io/kube-proxy:v1.17.5                 \
k8s.gcr.io/kube-controller-manager:v1.17.5    \
k8s.gcr.io/kube-apiserver:v1.17.5             \
k8s.gcr.io/kube-scheduler:v1.17.5             \
k8s.gcr.io/coredns:1.6.5                      \
k8s.gcr.io/etcd:3.4.3-0                       \
k8s.gcr.io/pause:3.1                          
```

此时基础的环境已经安装完成，可以做一个系统快照。以便安装翻车的时候恢复快照。



### 初始化集群网络

准备镜像

```sh
# 官网下载地址：
https://docs.projectcalico.org/v3.14/manifests/calico.yaml
# github地址：
https://github.com/projectcalico/calico


# 1. 镜像下载：
docker pull calico/cni:v3.14.2
docker pull calico/pod2daemon-flexvol:v3.14.2
docker pull calico/node:v3.14.2
docker pull calico/kube-controllers:v3.14.2

# 2. 初始化集群信息: calico网络,此在master节点上执行
kubeadm init --apiserver-advertise-address=192.168.5.50 --kubernetes-version v1.17.5 --service-cidr=10.1.0.0/16 --pod-network-cidr=10.81.0.0/16


# 3. 执行配制命令
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# 4. node节点加入集群信息 此命令，需要在3个node节点上执行
kubeadm join 192.168.5.50:6443 --token h1bc1a.q9i19q7smxl2pzom \
    --discovery-token-ca-cert-hash sha256:91d21fad4c1f4cdd3a7a01cae4d69b03805486ee90a4c2e77ac6b63f3ae66295 

# 5. kubectl命令自动补全,每个节点都执行下
echo "source <(kubectl completion bash)" >> ~/.bash_profile
source ~/.bash_profile
#在 bash 中设置当前 shell 的自动补全，要先安装 bash-completion 包。
echo "unset MAILCHECK">> /etc/profile
source /etc/profile
#在你的 bash shell 中永久的添加自动补全


# 6.yum-key.gpg验证未通过的相关执行
wget https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
wget https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
rpm --import yum-key.gpg
rpm --import rpm-package-key.gpg


# 查看集群的状态,集群的状态，同样是在master节点才能查看
kubectl get nodes

# 7. 下载网格的初始化文件
wget https://docs.projectcalico.org/v3.14/manifests/calico.yaml

# 执行网络初始化,此初始化，仅需要在master节点执行。
kubectl apply -f calico.yaml

# 查看集群的状态,同样是在master节点才能查看
kubectl get nodes
```

集群初始化:

```sh
[root@k8s-master-50 ~]# kubeadm init --apiserver-advertise-address=192.168.5.50 --kubernetes-version v1.17.5 --service-cidr=10.1.0.0/16 --pod-network-cidr=10.81.0.0/16
W1227 11:01:55.994044    3131 validation.go:28] Cannot validate kube-proxy config - no validator is available
W1227 11:01:55.994077    3131 validation.go:28] Cannot validate kubelet config - no validator is available
[init] Using Kubernetes version: v1.17.5
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master-50 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.1.0.1 192.168.5.50]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master-50 localhost] and IPs [192.168.5.50 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master-50 localhost] and IPs [192.168.5.50 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
W1227 11:01:57.949710    3131 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[control-plane] Creating static Pod manifest for "kube-scheduler"
W1227 11:01:57.950137    3131 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 15.012704 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.17" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master-50 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node k8s-master-50 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: h1bc1a.q9i19q7smxl2pzom
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.5.50:6443 --token h1bc1a.q9i19q7smxl2pzom \
    --discovery-token-ca-cert-hash sha256:91d21fad4c1f4cdd3a7a01cae4d69b03805486ee90a4c2e77ac6b63f3ae66295 
```

节点加入集群的日志：

```sh
[root@k8s-node51 ~]# kubeadm join 192.168.5.50:6443 --token h1bc1a.q9i19q7smxl2pzom \
>     --discovery-token-ca-cert-hash sha256:91d21fad4c1f4cdd3a7a01cae4d69b03805486ee90a4c2e77ac6b63f3ae66295 
W1227 11:04:05.199752    3301 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
[preflight] Running pre-flight checks
        [WARNING Hostname]: hostname "k8s-node51" could not be reached
        [WARNING Hostname]: hostname "k8s-node51": lookup k8s-node51 on 192.168.3.1:53: no such host
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

经过以上的步骤，集群相关的安装已经完成：

查看集群的一个状态:

```sh
[root@k8s-master-50 ~]# kubectl get nodes
NAME            STATUS     ROLES    AGE     VERSION
k8s-master-50   NotReady   master   10m     v1.17.5
k8s-node51      NotReady   <none>   8m53s   v1.17.5
k8s-node52      NotReady   <none>   8m49s   v1.17.5
k8s-node53      NotReady   <none>   8m35s   v1.17.5
```

集群网络初始化：

```sh
[root@k8s-master-50 k8s]# kubectl apply -f calico.yaml
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
```

集群网络初始化成功后的一个状态:

```sh
[root@k8s-master-50 k8s]# kubectl get nodes
NAME            STATUS   ROLES    AGE   VERSION
k8s-master-50   Ready    master   16m   v1.17.5
k8s-node51      Ready    <none>   14m   v1.17.5
k8s-node52      Ready    <none>   14m   v1.17.5
k8s-node53      Ready    <none>   14m   v1.17.5
```





## k8s之NameSpace







