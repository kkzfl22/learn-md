# K8S笔记

## Kubernetes安装

**硬件要求**

| 硬件 | 要求    |
| ---- | ------- |
| CPU  | 至少2核 |
| 内存 | 至少3G  |
| 硬件 | 至少50G |

**节点信息**

| 主机名        | IP           |
| ------------- | ------------ |
| k8s-master-50 | 192.168.5.50 |
| k8s-node-51   | 192.168.5.51 |
| k8s-node-52   | 192.168.5.52 |
| k8s-node-53   | 192.168.5.53 |

系统要求：

推荐使用centos7.7及以上版本

国内建议使用阿里云下载

```http
http://mirrors.aliyun.com/centos/7/isos/x86_64/
```

### 配制阿里云yum源

```sh
# 1. 下载安装wget
yum install -y wget

#2.备份默认的yum
mv /etc/yum.repos.d /etc/yum.repos.d.backup

#3.设置新的yum目录
mkdir -p /etc/yum.repos.d

#4.下载阿里yum配置到该目录中，选择对应版本
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo

#5.更新epel源为阿里云epel源
mv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backup
mv /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backup

wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo

#6.重建缓存
yum clean all
yum makecache

#7.看一下yum仓库有多少包
yum repolist
yum update
```

可以做一个系统快照版本，防止后面的翻车，以做好回滚，不用从头开始。做系统快照建议关机做，关机后的快照特别小。

### 升级系统内核

默认此处安装的7.9版本，内核版本为3.10，需要做下升级,此安装完成后，系统内核版本需要升级到了`4.4`，但别太高，不然操作不一致。

```sh
# 导入仓库源
rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm

# 查看可安装的软件包
yum --enablerepo="elrepo-kernel" list --showduplicates | sort -r | grep kernel-lt.x86_64

# 升级到最新内核版本的方法
yum --enablerepo=elrepo-kernel install -y kernel-lt


grep initrd16 /boot/grub2/grub.cfg
grub2-set-default 0

reboot
```

### 查看相关的系统信息

```sh
# 查看centos系统内核命令
uname -r
uname -a

# 查看CPU
lscpu

# 查看内存
free 
free -g

# 查看硬盘信息
fdisk -l
```

### centos7系统配制

```sh
# 1. 开发环境可以直接关闭防火墙，但生产环境，切记，万不能关
systemctl stop firewalld
systemctl disable firewalld


# 2. 关闭selinux
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
setenforce 0


# 3. 网桥过滤
vi /etc/sysctl.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-arptables = 1
net.ipv4.ip_forward=1
net.ipv4.ip_forward_use_pmtu = 0
#生效命令
sysctl --system
#查看效果
sysctl -a|grep "ip_forward"



# 4.  开启IPVS
#安装IPVS
yum -y install ipset ipvsadm -y
#编译ipvs.modules文件
vi /etc/sysconfig/modules/ipvs.modules
#文件内容如下
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
# 在4.4内核版本中使用nf_conntrack_ipv4
modprobe -- nf_conntrack_ipv4
#赋予权限并执行
chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack
#重启电脑，检查是否生效
reboot
lsmod | grep ip_vs_rr



# 5. 同步时间
#安装软件
yum -y install ntpdate
#向阿里云服务器同步时间
ntpdate time1.aliyun.com
#删除本地时间并设置时区为上海
rm -rf /etc/localtime
ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
#查看时间
date -R || date



# 6.命令补全
#安装bash-completion
yum -y install bash-completion bash-completion-extras
#使用bash-completion
source /etc/profile.d/bash_completion.sh



# 7. 关闭swap分区
#临时关闭：
swapoff -a
#永久关闭：
vi /etc/fstab
将文件中的/dev/mapper/centos-swap这行代码注释掉
#/dev/mapper/centos-swap swap swap defaults 0 0
#确认swap已经关闭：若swap行都显示 0 则表示关闭成功
free -m


# 8. hosts配置
#文件内容如下:
cat <<EOF >> /etc/hosts
192.168.5.50 k8s-master-50
192.168.5.51 k8s-node-51
192.168.5.52 k8s-node-52
192.168.5.53 k8s-node-53
EOF

cat /etc/hosts
```

### 安装docker

```sh
# 1. 安装docker的前置条件
yum install -y yum-utils device-mapper-persistent-data lvm2

# 2. 添加源
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum makecache fast

# 3. 查看docker更新版本
yum list docker-ce --showduplicates | sort -r


# 4. 安装docker版本,这里需要指定版本为18.09.8
yum -y install docker-ce-18.09.8
# 如果需要安装最新版本，使用
# yum -y install docker-ce
# 开启docker服务开机启动
systemctl start docker
systemctl status docker
# 查看docker版本
docker version
# docker-client版本：当前最新版本
# docker-server需要为版本为：18.09.8

# 5. 阿里云镜像加速器地址
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://ys2mfbsh.mirror.aliyuncs.com"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker

# 6. 设置为开机自启动
systemctl enable docker

# 7. 修改Cgroup Driver
vi /etc/docker/daemon.json
# 修改daemon.json，新增：
"exec-opts": ["native.cgroupdriver=systemd"]
# 修改cgroupdriver是为了消除安装k8s集群时的告警：
# [WARNING IsDockerSystemdCheck]: 
# detected “cgroupfs” as the Docker cgroup driver. The recommended driver is “systemd”. 
# Please follow the guide at https://kubernetes.io/docs/setup/cri/......
#重启docker服务：
systemctl daemon-reload 
systemctl restart docker
#查看修改后状态：
docker info | grep Cgroup


# 查看docker的信息
docker -v
docker info
```



### 使用kubeadm安装k8s

| 软件 | kubeadm                        | kubelet                                                    | kubectl                             | docker-ce |
| ---- | ------------------------------ | ---------------------------------------------------------- | ----------------------------------- | --------- |
| 版本 | 初始化集群管理.<br/>版本1.17.5 | 用于接收api-server指令对pod生命周期进行管理<br/>版本1.17.5 | 集群命令行管理工具<br/>版本：1.17.5 | 18.09.8   |

```sh
# 1. 安装yum源
vi /etc/yum.repos.d/kubernates.repo
# 加入内容
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
       
# 2. 更新缓存
yum clean all
yum -y makecache

# 3. 验证是否可用
yum list | grep kubeadm
#如果提示要验证yum-key.gpg是否可用，输入y。
#查找到kubeadm。显示版本

# 4. 查看k8s的版本
yum list kubelet --showduplicates | sort -r

# 5. 安装k8s-1.17.5
yum install -y kubelet-1.17.5 kubeadm-1.17.5 kubectl-1.17.5


# 6. 设置kubelet
# 增加配置信息
# 如果不配置kubelet，可能会导致K8S集群无法启动。为实现docker使用的cgroupdriver与kubelet使用的cgroup的一致性。
vi /etc/sysconfig/kubelet
KUBELET_EXTRA_ARGS="--cgroup-driver=systemd"


# 7. 设置开机启动
systemctl enable kubelet
```



### 初始化镜像

如果是首次安装k8s，手里没有备份好的镜像，需要执行此操作，如果已经已经有了镜像备份，请跳过本章节

```sh
# 查看安装集群需要镜像
kubeadm config images list
# 可得到如下这样一个列表
k8s.gcr.io/kube-apiserver:v1.17.5
k8s.gcr.io/kube-controller-manager:v1.17.5
k8s.gcr.io/kube-scheduler:v1.17.5
k8s.gcr.io/kube-proxy:v1.17.5
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.4.3-0
k8s.gcr.io/coredns:1.6.5
```

使用脚本下载镜像

```sh
mkdir -p /data/k8s
cd /data/k8s
# 编辑内容
vi images.sh
```

脚本内容:

从阿里云下载镜像，再下载的镜像应该去除"k8s.gcr.io"的前缀，版本换成kubeadm config images list命令获取到的版本

```shell
#!/bin/bash
images=(
   kube-apiserver:v1.17.5
   kube-controller-manager:v1.17.5
   kube-scheduler:v1.17.5
   kube-proxy:v1.17.5
   pause:3.1
   etcd:3.4.3-0
   coredns:1.6.5
)
for imageName in ${images[@]} ; 
do
   docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
   docker tag  registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName
   docker rmi  registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
done
```



执行脚本保存镜像

```sh
cd /data/k8s
# 给脚本授权
chmod +x images.sh 
# 执行脚本
./images.sh 

# 查看镜像
docker images

# 保存镜像
docker save -o k8s-1.17.5-all.image           \
k8s.gcr.io/kube-proxy:v1.17.5                 \
k8s.gcr.io/kube-controller-manager:v1.17.5    \
k8s.gcr.io/kube-apiserver:v1.17.5             \
k8s.gcr.io/kube-scheduler:v1.17.5             \
k8s.gcr.io/coredns:1.6.5                      \
k8s.gcr.io/etcd:3.4.3-0                       \
k8s.gcr.io/pause:3.1                          
```

此时基础的环境已经安装完成，可以做一个系统快照。以便安装翻车的时候恢复快照。



### 初始化集群网络

准备镜像

```sh
# 官网下载地址：
https://docs.projectcalico.org/v3.14/manifests/calico.yaml
# github地址：
https://github.com/projectcalico/calico


# 1. 镜像下载：
docker pull calico/cni:v3.14.2
docker pull calico/pod2daemon-flexvol:v3.14.2
docker pull calico/node:v3.14.2
docker pull calico/kube-controllers:v3.14.2

# 2. 初始化集群信息: calico网络,此在master节点上执行
kubeadm init --apiserver-advertise-address=192.168.5.50 --kubernetes-version v1.17.5 --service-cidr=10.1.0.0/16 --pod-network-cidr=10.81.0.0/16


# 3. 执行配制命令
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# 4. node节点加入集群信息 此命令，需要在3个node节点上执行
kubeadm join 192.168.5.50:6443 --token h1bc1a.q9i19q7smxl2pzom \
    --discovery-token-ca-cert-hash sha256:91d21fad4c1f4cdd3a7a01cae4d69b03805486ee90a4c2e77ac6b63f3ae66295 

# 5. kubectl命令自动补全,每个节点都执行下
echo "source <(kubectl completion bash)" >> ~/.bash_profile
source ~/.bash_profile
#在 bash 中设置当前 shell 的自动补全，要先安装 bash-completion 包。
echo "unset MAILCHECK">> /etc/profile
source /etc/profile
#在你的 bash shell 中永久的添加自动补全


# 6.yum-key.gpg验证未通过的相关执行
wget https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
wget https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
rpm --import yum-key.gpg
rpm --import rpm-package-key.gpg


# 查看集群的状态,集群的状态，同样是在master节点才能查看
kubectl get nodes

# 7. 下载网格的初始化文件
wget https://docs.projectcalico.org/v3.14/manifests/calico.yaml

# 执行网络初始化,此初始化，仅需要在master节点执行。
kubectl apply -f calico.yaml

# 查看集群的状态,同样是在master节点才能查看
kubectl get nodes
```

集群初始化:

```sh
[root@k8s-master-50 ~]# kubeadm init --apiserver-advertise-address=192.168.5.50 --kubernetes-version v1.17.5 --service-cidr=10.1.0.0/16 --pod-network-cidr=10.81.0.0/16
W1227 11:01:55.994044    3131 validation.go:28] Cannot validate kube-proxy config - no validator is available
W1227 11:01:55.994077    3131 validation.go:28] Cannot validate kubelet config - no validator is available
[init] Using Kubernetes version: v1.17.5
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master-50 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.1.0.1 192.168.5.50]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master-50 localhost] and IPs [192.168.5.50 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master-50 localhost] and IPs [192.168.5.50 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
W1227 11:01:57.949710    3131 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[control-plane] Creating static Pod manifest for "kube-scheduler"
W1227 11:01:57.950137    3131 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 15.012704 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.17" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master-50 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node k8s-master-50 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: h1bc1a.q9i19q7smxl2pzom
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.5.50:6443 --token h1bc1a.q9i19q7smxl2pzom \
    --discovery-token-ca-cert-hash sha256:91d21fad4c1f4cdd3a7a01cae4d69b03805486ee90a4c2e77ac6b63f3ae66295 
```

节点加入集群的日志：

```sh
[root@k8s-node51 ~]# kubeadm join 192.168.5.50:6443 --token h1bc1a.q9i19q7smxl2pzom \
>     --discovery-token-ca-cert-hash sha256:91d21fad4c1f4cdd3a7a01cae4d69b03805486ee90a4c2e77ac6b63f3ae66295 
W1227 11:04:05.199752    3301 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
[preflight] Running pre-flight checks
        [WARNING Hostname]: hostname "k8s-node51" could not be reached
        [WARNING Hostname]: hostname "k8s-node51": lookup k8s-node51 on 192.168.3.1:53: no such host
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

经过以上的步骤，集群相关的安装已经完成：

查看集群的一个状态:

```sh
[root@k8s-master-50 ~]# kubectl get nodes
NAME            STATUS     ROLES    AGE     VERSION
k8s-master-50   NotReady   master   10m     v1.17.5
k8s-node51      NotReady   <none>   8m53s   v1.17.5
k8s-node52      NotReady   <none>   8m49s   v1.17.5
k8s-node53      NotReady   <none>   8m35s   v1.17.5
```

集群网络初始化：

```sh
[root@k8s-master-50 k8s]# kubectl apply -f calico.yaml
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
```

集群网络初始化成功后的一个状态:

```sh
[root@k8s-master-50 k8s]# kubectl get nodes
NAME            STATUS   ROLES    AGE   VERSION
k8s-master-50   Ready    master   16m   v1.17.5
k8s-node51      Ready    <none>   14m   v1.17.5
k8s-node52      Ready    <none>   14m   v1.17.5
k8s-node53      Ready    <none>   14m   v1.17.5
```





## k8s之NameSpace

中文名称：命名空间，可以认为namespace是kubernets集群中的虚拟化集群。在一个kubernetes集群中可以拥有多个命名空间，它们逻辑上彼此隔离。可以为你提供组织、安全、甚至性能方面的帮助。

Namespace是一对资源和对象的抽象集合。比如可以用系来将系统内部的对象划分为不同的项目组或用户组。常见的是pods,servers,replication controllers和deployment和deployments等都属于某一个namespace(默认是default)，而node，persistentVolumnets等则不属性任何namespace。

大多数的kubernetes中集群默认会有一个default的namespace。实际上应该是4个

- default:  资源默认被创建于default命名空间。
- kube-system： kubernetes系统组件使用。
- kube-node-lease: kubernetes集群节点租约状态。v1.13加入
- kube-public: 公共资源使用。但实际上现在并不常用。

这个默认（default）的namespace并没有什么特别，但你不能删除它。这很适合刚刚开始使用kubernetes和一些小的产品系统。但不建议应用于大型生产系统。因为，这种复杂系统中。团队会非常容易意外地或者无意识的重写或者中断其他service。相反，请创建多个命名空间来把你的service（服务）分割成一时半会儿 容易管理的块。

作用：多租户情况下，实现资源的隔离。

属于逻辑隔离。

属于管理边界。

不属性网络边界。

可以针对每个namespace做资源配额。

**查看命名空间**

```sh
kubectl get namespace
# 简写命令
kubectl get ns

# 查看所有命名空间的pod资源
kubectl get pod --all-namepsace
kubectl get pod -A
```

输出 ：

```sh
[root@k8s-master-50 ~]# kubectl get namespace
NAME              STATUS   AGE
default           Active   22h
kube-node-lease   Active   22h
kube-public       Active   22h
kube-system       Active   22h
[root@k8s-master-50 ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   22h
kube-node-lease   Active   22h
kube-public       Active   22h
kube-system       Active   22h
[root@k8s-master-50 ~]# kubectl get pod --all-namespaces
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6b94766748-hfbjs   1/1     Running   3          22h
kube-system   calico-node-27kqm                          1/1     Running   3          22h
kube-system   calico-node-8w2pp                          1/1     Running   3          22h
kube-system   calico-node-brwl7                          1/1     Running   3          22h
kube-system   calico-node-s6d49                          1/1     Running   4          22h
kube-system   coredns-6955765f44-rq92j                   1/1     Running   3          22h
kube-system   coredns-6955765f44-wmr68                   1/1     Running   3          22h
kube-system   etcd-k8s-master-50                         1/1     Running   3          22h
kube-system   kube-apiserver-k8s-master-50               1/1     Running   4          22h
kube-system   kube-controller-manager-k8s-master-50      1/1     Running   3          22h
kube-system   kube-proxy-4cbsx                           1/1     Running   3          22h
kube-system   kube-proxy-l6k6x                           1/1     Running   3          22h
kube-system   kube-proxy-pq57w                           1/1     Running   4          22h
kube-system   kube-proxy-tlb5j                           1/1     Running   3          22h
kube-system   kube-scheduler-k8s-master-50               1/1     Running   3          22h
[root@k8s-master-50 ~]# kubectl get pod -A
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6b94766748-hfbjs   1/1     Running   3          22h
kube-system   calico-node-27kqm                          1/1     Running   3          22h
kube-system   calico-node-8w2pp                          1/1     Running   3          22h
kube-system   calico-node-brwl7                          1/1     Running   3          22h
kube-system   calico-node-s6d49                          1/1     Running   4          22h
kube-system   coredns-6955765f44-rq92j                   1/1     Running   3          22h
kube-system   coredns-6955765f44-wmr68                   1/1     Running   3          22h
kube-system   etcd-k8s-master-50                         1/1     Running   3          22h
kube-system   kube-apiserver-k8s-master-50               1/1     Running   4          22h
kube-system   kube-controller-manager-k8s-master-50      1/1     Running   3          22h
kube-system   kube-proxy-4cbsx                           1/1     Running   3          22h
kube-system   kube-proxy-l6k6x                           1/1     Running   3          22h
kube-system   kube-proxy-pq57w                           1/1     Running   4          22h
kube-system   kube-proxy-tlb5j                           1/1     Running   3          22h
kube-system   kube-scheduler-k8s-master-50               1/1     Running   3          22h
```

命名空间的说明：

```sh
default: 用户创建的pod默认都在此命名空间。
kube-public 所有用户均可以访问，包括未认证用户。
kube-node-lease kubernetes集群节点租约状态。
kube-system kubernetes集群在使用。
```

**创建和删除Namespace**

```sh
kubectl create namespace nullnull
kubectl delete namespace nullnull

# 简写命令
kubectl create ns nullnull
kubectl delete ns nullnull

```

输出:

```sh
[root@k8s-master-50 ~]# kubectl create namespace nullnull
namespace/nullnull created
[root@k8s-master-50 ~]# kubectl get namespace
NAME              STATUS   AGE
default           Active   23h
kube-node-lease   Active   23h
kube-public       Active   23h
kube-system       Active   23h
nullnull          Active   20s
[root@k8s-master-50 ~]# kubectl delete namespace nullnull
namespace "nullnull" deleted
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get namespace
NAME              STATUS   AGE
default           Active   23h
kube-node-lease   Active   23h
kube-public       Active   23h
kube-system       Active   23h
[root@k8s-master-50 ~]# kubectl create ns nullnull
namespace/nullnull created
[root@k8s-master-50 ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   23h
kube-node-lease   Active   23h
kube-public       Active   23h
kube-system       Active   23h
nullnull          Active   5s
[root@k8s-master-50 ~]# kubectl delete ns nullnull
namespace "nullnull" deleted
[root@k8s-master-50 ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   23h
kube-node-lease   Active   23h
kube-public       Active   23h
kube-system       Active   23h
```



## K8S之POD



![image-20231228102017454](.\images\image-20231228102017454.png)

pod是kubernetes集群能够调度的最小单元。POD是容器的封装。

在kubernetes集群中，Pod是所有业务类型的基础，也是K8S管理的最小单位级，它是一个或多个容器的组合。这些容器共享存储，网络和命名空间，以及如何运行的规范。在Pod中，所有容器都被同一安排和调度，并运行在共享的上下文中。对于具体应用而言，Pod是它们的逻辑主机，Pod包含业务相关的多个应用容器。

**pod的两个必知特点**

`网络`：每一个Pod都会被指派唯一的一个IP地址，在Pod中，每一个容器共享网络命名空间，包括IP地址和网络端口。在同一个Pod中的容器可以和localhost进行互相通信。当Pod中的容器需要与Pod外的实体进行通信时，则需要通过端口等共享的网络资源。

`存储`：Pod能够被指定共享存储卷的集合，在Pod中所有的容器能够访问共享存储卷，允许这些容器共享数据。存储卷也允许在一个Pod持久化数据，以防止其中的容器需要被重启。

**Pod的工作方式**

k8s一般不直接创建Pod，而是通过控制器和模板配置来管理和调度。

- Pod模板。

后续章节会介绍Pod模板。

- Pod重启

在Pod中的容器可能会由于异常等原因导致其终止退出，Kubernetes提供了重启策略以重启容器。重启策略对同一个Pod的所有容器起作用，容器的重启由Node上的kubelet执行。Pod支持三种重启策略：

1. Always: 只要退出就会重启。
2. OnFailure: 只有在失败退出时（exit code不等于0）时，才会重启。
3. Never: 只要退出，就不再重启。

注意：重启是指在Pod宿主Node上进行本地重启，而不是调度到其它的Node上。

- 资源限制

kubernetes通过cgroups限制容器的CPU和内存等计算资源，包括requests（请求，调度器保证调度到资源充足的Node上）和limits(上限)等。

**查看Pod**

```sh
# 查看default命名空间下的Pods
kubectl get pods
# 查看kube-system命名空间下的pods
kubectl get pods -n kube-system
# 查看所有命名空间下的pods
kubectl get pod --all-namespace
kubectl get pod -A
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl get pods
No resources found in default namespace.
[root@k8s-master-50 ~]# kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6b94766748-hfbjs   1/1     Running   3          23h
calico-node-27kqm                          1/1     Running   3          23h
calico-node-8w2pp                          1/1     Running   3          23h
calico-node-brwl7                          1/1     Running   3          23h
calico-node-s6d49                          1/1     Running   4          23h
coredns-6955765f44-rq92j                   1/1     Running   3          23h
coredns-6955765f44-wmr68                   1/1     Running   3          23h
etcd-k8s-master-50                         1/1     Running   3          23h
kube-apiserver-k8s-master-50               1/1     Running   4          23h
kube-controller-manager-k8s-master-50      1/1     Running   3          23h
kube-proxy-4cbsx                           1/1     Running   3          23h
kube-proxy-l6k6x                           1/1     Running   3          23h
kube-proxy-pq57w                           1/1     Running   4          23h
kube-proxy-tlb5j                           1/1     Running   3          23h
kube-scheduler-k8s-master-50               1/1     Running   3          23h
[root@k8s-master-50 ~]# kubectl get pod --all-namespaces
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6b94766748-hfbjs   1/1     Running   3          23h
kube-system   calico-node-27kqm                          1/1     Running   3          23h
kube-system   calico-node-8w2pp                          1/1     Running   3          23h
kube-system   calico-node-brwl7                          1/1     Running   3          23h
kube-system   calico-node-s6d49                          1/1     Running   4          23h
kube-system   coredns-6955765f44-rq92j                   1/1     Running   3          23h
kube-system   coredns-6955765f44-wmr68                   1/1     Running   3          23h
kube-system   etcd-k8s-master-50                         1/1     Running   3          23h
kube-system   kube-apiserver-k8s-master-50               1/1     Running   4          23h
kube-system   kube-controller-manager-k8s-master-50      1/1     Running   3          23h
kube-system   kube-proxy-4cbsx                           1/1     Running   3          23h
kube-system   kube-proxy-l6k6x                           1/1     Running   3          23h
kube-system   kube-proxy-pq57w                           1/1     Running   4          23h
kube-system   kube-proxy-tlb5j                           1/1     Running   3          23h
kube-system   kube-scheduler-k8s-master-50               1/1     Running   3          23h
[root@k8s-master-50 ~]# kubectl get pod -A
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6b94766748-hfbjs   1/1     Running   3          23h
kube-system   calico-node-27kqm                          1/1     Running   3          23h
kube-system   calico-node-8w2pp                          1/1     Running   3          23h
kube-system   calico-node-brwl7                          1/1     Running   3          23h
kube-system   calico-node-s6d49                          1/1     Running   4          23h
kube-system   coredns-6955765f44-rq92j                   1/1     Running   3          23h
kube-system   coredns-6955765f44-wmr68                   1/1     Running   3          23h
kube-system   etcd-k8s-master-50                         1/1     Running   3          23h
kube-system   kube-apiserver-k8s-master-50               1/1     Running   4          23h
kube-system   kube-controller-manager-k8s-master-50      1/1     Running   3          23h
kube-system   kube-proxy-4cbsx                           1/1     Running   3          23h
kube-system   kube-proxy-l6k6x                           1/1     Running   3          23h
kube-system   kube-proxy-pq57w                           1/1     Running   4          23h
kube-system   kube-proxy-tlb5j                           1/1     Running   3          23h
kube-system   kube-scheduler-k8s-master-50               1/1     Running   3          23h
[root@k8s-master-50 ~]# 
```

**创建Pod**

```sh
# 下载镜像
# K8S集群的每一个节点都需要下载镜像，选择不同的镜像，下载镜像的大小也不同。
docker pull tomcat:9.0.20-jre8-alpine
docker pull tomcat:9.0.37-jdk8-openjdk-slim
docker pull tomcat:9.0.37-jdk8


# 在defalt命名空间中创建一个pod副本的deployment
kubectl run tomcat9-run --image=tomcat:9.0.20-jre8-alpine --port=8080

kubectl get pod
kubectl get pod -o wide

# 使用pod的ip访问容器
curl ****:8080
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl run tomcat9-run --image=tomcat:9.0.20-jre8-alpine --port=8080
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
deployment.apps/tomcat9-run created
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get pod
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9-run-6595fb5f85-fls4k   1/1     Running   0          72s
[root@k8s-master-50 ~]# kubectl get pod -w
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9-run-6595fb5f85-fls4k   1/1     Running   0          75s
^[[A^[[A^C[root@k8s-master-50 ~]# kubectl get pod -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP            NODE         NOMINATED NODE   READINESS GATES
tomcat9-run-6595fb5f85-fls4k   1/1     Running   0          85s   10.81.92.65   k8s-node51   <none>           <none>
[root@k8s-master-50 ~]# curl 10.81.92.65:8080
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        ......
    </body>

</html>
```

**扩容**

```sh
# 查看当前部署信息
kubectl get deployment
kubectl get deployment -o wide

# 将一个容器扩容至3个
kubectl scale --replicas=3 deployment/tomcat9-run

# 查看当前部署信息
kubectl get deployment
kubectl get deployment -o wide

# 查看pod信息
kubectl get pod
kubectl get pod -o wide
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl get deployment
NAME          READY   UP-TO-DATE   AVAILABLE   AGE
tomcat9-run   1/1     1            1           7m1s
[root@k8s-master-50 ~]# kubectl get deployment -o wide
NAME          READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS    IMAGES                      SELECTOR
tomcat9-run   1/1     1            1           7m1s   tomcat9-run   tomcat:9.0.20-jre8-alpine   run=tomcat9-run
[root@k8s-master-50 ~]# kubectl scale --replicas=3 deployment/tomcat9-run
deployment.apps/tomcat9-run scaled
[root@k8s-master-50 ~]# kubectl get deployment
NAME          READY   UP-TO-DATE   AVAILABLE   AGE
tomcat9-run   3/3     3            3           7m13s
[root@k8s-master-50 ~]# kubectl get deployment -o wide
NAME          READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS    IMAGES                      SELECTOR
tomcat9-run   3/3     3            3           7m13s   tomcat9-run   tomcat:9.0.20-jre8-alpine   run=tomcat9-run
[root@k8s-master-50 ~]# kubectl get  pod
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9-run-6595fb5f85-fls4k   1/1     Running   0          21m
tomcat9-run-6595fb5f85-gtxdp   1/1     Running   0          14m
tomcat9-run-6595fb5f85-m4drb   1/1     Running   0          14m
[root@k8s-master-50 ~]# kubectl get  pod -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
tomcat9-run-6595fb5f85-fls4k   1/1     Running   0          22m   10.81.92.65     k8s-node51   <none>           <none>
tomcat9-run-6595fb5f85-gtxdp   1/1     Running   0          15m   10.81.237.193   k8s-node52   <none>           <none>
tomcat9-run-6595fb5f85-m4drb   1/1     Running   0          15m   10.81.149.193   k8s-node53   <none>           <none>
[root@k8s-master-50 ~]# 
```

通过观察pod的一个输出可以发现，它们家口的网段是一致的，并且被分配了不同的节点上，10.81.92.65这个IP就是在node51这个节点上，其他的IP也对应了不同的节点。

而且使用此IP，加上8080端口，就可以直接访问

```sh
[root@k8s-master-50 ~]# curl 10.81.237.193:8080



<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        <div id="wrapper">
            <div id="navigation" class="curved container">
                <span id="nav-home"><a href="https://tomcat.apache.org/">Home</a></span>
                <span id="nav-hosts"><a href="/docs/">Documentation</a></span>
                <span id="nav-config"><a href="/docs/config/">Configuration</a></span>
                <span id="nav-examples"><a href="/examples/">Examples</a></span>
                <span id="nav-wiki"><a href="https://wiki.apache.org/tomcat/FrontPage">Wiki</a></span>
                <span id="nav-lists"><a href="https://tomcat.apache.org/lists.html">Mailing Lists</a></span>
                <span id="nav-help"><a href="https://tomcat.apache.org/findhelp.html">Find Help</a></span>
                <br class="separator" />
                ......
            </div>
       </div>
    </body>

</html>
```



**创建服务**

只有创建了服务才能被外网所访问

```sh
# 将之前部署的deployment（tomcat9-run）与service产生关系，外部便可以访问了。
kubectl expose deployment tomcat9-run --name=tomcat9-svc --port=8888 --target-port=8080 --protocol=TCP --type=NodePort
# 这个端口号是对内，对K8S集群内其他应该暴露的一个端口号
--port=8888
# deployment容器的端口号，
--target-port=8080

kubectl get service

# 简写
kubectl get svc
kubectl get svc -o wide

# 使用服务端口访问
curl 10.1.210.157:8888

# 外部使用浏览器访问
http://192.168.5.50:32395/
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl expose deployment tomcat9-run --name=tomcat9-svc --port=8888 --target-port=8080 --protocol=TCP --type=NodePort
service/tomcat9-svc exposed
[root@k8s-master-50 ~]# kubectl get service
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes    ClusterIP   10.1.0.1       <none>        443/TCP          29h
tomcat9-svc   NodePort    10.1.210.157   <none>        8888:32395/TCP   24m
[root@k8s-master-50 ~]# kubectl get svc
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes    ClusterIP   10.1.0.1       <none>        443/TCP          29h
tomcat9-svc   NodePort    10.1.210.157   <none>        8888:32395/TCP   16s
[root@k8s-master-50 ~]# kubectl get svc -o wide
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE   SELECTOR
kubernetes    ClusterIP   10.1.0.1       <none>        443/TCP          29h   <none>
tomcat9-svc   NodePort    10.1.210.157   <none>        8888:32395/TCP   48s   run=tomcat9-run
[root@k8s-master-50 ~]# curl 10.1.210.157:8888



<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        <div id="wrapper">
		......
		</div>
	</body>
</html>
```

浏览器打开测试：

![image-20231228163416398](.\images\image-20231228163416398.png)





## kubectl常用命令

语法规则：

```sh
kubectl [command] [TYPE] [NAME] [flags] [options]
```

- command: 指定一个或者多个资源执行的操作，例：`create`、`get `、`delete`。
- TYPE： 指定资源类型。 资源类型不区分大小写，可以指定单数、复数或缩写形式

```sh
kubectl get pod tomcat9-run-6595fb5f85-fls4k
kubectl get pods tomcat9-run-6595fb5f85-fls4k
kubectl get po tomcat9-run-6595fb5f85-fls4k
```

- NAME: 指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息`kubectl get pods`,对多个资源执行操作时，可以按类型和名称指定每个资源，或者一个或多个文件。
  - 要按类型和名称指定资源：
    - 要对所有类型相同的资源进行分组，可以执行： `TYPE1 name1 name2 name<#>`。
    - 分别指定多个资源类型：`TYPE1/name1 TYPE2/name2 TYPE3/name3 TYPE<#>/name<#>`。例如`kubectl get pod/example-pod1 replicationcontroller/example-rc1`
  - 用一个或多个文件指定资源：`-f file -f file2 -f file<#>`
    - 使用YAML而不是JSON，是因为YAML更容易使用，特别用于文件配制时。例：`kubectl get pod -f ./pod`
- flags: 指定可选参数，例如，可以使用-S或者-server参数指定Kubernetes API服务器的地址和端口。

注意：从命令行指定参数会覆盖默认值和任何相应的环境变量。

相关的帮助信息可以使用`kubectl help`查看

### **get命令**

`kubectl get` - 列出一个或者多个资源。

```sh
# 查看集群的状态信息
kubectl cluster-info

# 查看集群的状态
kubectl get cs

# 查看集群的节点信息
kubectl get nodes

# 查看集群命名空间
kubectl get ns

# 查看指定命名空间的服务
kubectl get svc -n kube-system

# 以纯文本列出所有Pod
kubectl get pods

# 以纯文本列出所有的Pod,并包含附加信息（如节点名）
kubectl get pods -o wide

# 以纯文本格式列出具有指定名称的副本控制器。提示：可以使用别名‘rc’缩短和替换‘replicationcontroller’资源类型。
kubectl get replicationcontroller <rc-name>
kubectl get rc <rc-name>

# 以纯文本格式列出所有副本控制器和服务
kubectl get rc,servers

# 以纯文本格式列出所有守护程序集，包括未初始化的守护程序集
# 在 Kubernetes 中，--include-uninitialized 标志在 Kubernetes 版本 1.19 之前是有效的，但在 Kubernetes 1.19 版本之后被废弃并删除。因此，如果你的 Kubernetes 版本为 1.19 或更新版本，则无法使用该标志。
kubectl get pods --include-uninitialized

# 列出在节点server01上运行的所有pod
kubectl get pods --field-selector=spec.nodeName=k8s-node51
```

输出：

```sh
[root@k8s-master-50 ~]# kubectl cluster-info
Kubernetes master is running at https://192.168.5.50:6443
KubeDNS is running at https://192.168.5.50:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get nodes
NAME            STATUS   ROLES    AGE   VERSION
k8s-master-50   Ready    master   46h   v1.17.5
k8s-node51      Ready    <none>   46h   v1.17.5
k8s-node52      Ready    <none>   46h   v1.17.5
k8s-node53      Ready    <none>   46h   v1.17.5
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   47h
kube-node-lease   Active   47h
kube-public       Active   47h
kube-system       Active   47h
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get svc -n kube-system
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.1.0.10    <none>        53/UDP,53/TCP,9153/TCP   47h
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9-run-6595fb5f85-fls4k   1/1     Running   1          17h
tomcat9-run-6595fb5f85-gtxdp   1/1     Running   1          17h
tomcat9-run-6595fb5f85-m4drb   1/1     Running   1          17h
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get pods -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
tomcat9-run-6595fb5f85-fls4k   1/1     Running   1          17h   10.81.92.66     k8s-node51   <none>           <none>
tomcat9-run-6595fb5f85-gtxdp   1/1     Running   1          17h   10.81.237.194   k8s-node52   <none>           <none>
tomcat9-run-6595fb5f85-m4drb   1/1     Running   1          17h   10.81.149.194   k8s-node53   <none>           <none>
[root@k8s-master-50 ~]#
[root@k8s-master-50 ~]# kubectl get replicationcontroller 
No resources found in default namespace.
[root@k8s-master-50 ~]# kubectl get rc
No resources found in default namespace.
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get rc,services
NAME                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/kubernetes    ClusterIP   10.1.0.1       <none>        443/TCP          47h
service/tomcat9-svc   NodePort    10.1.210.157   <none>        8888:32395/TCP   17h
[root@k8s-master-50 ~]# 

[root@k8s-master-50 ~]# kubectl get pods --field-selector=spec.nodeName=k8s-node51
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9-run-6595fb5f85-fls4k   1/1     Running   1          18h

```



### **describe命令**

kubectl describe 命令显示一个或多个资源的详细状态，默认情况下包括未初始化的资源。

```sh
# 显示名称为<node-name>的节点的详细信息
kubectl describe nodes <node-name>

# 显示名为 <pod-name>的pod的详细信息
kubectl describe pods/<pod-name>

# 显示名为<rc-name>的副本控制器管理的所有pod的详细信息.
# 副本控制器创建的任何pod都以复制控制器的名称为前缀
kubectl describe pods <rc-name>

# 描述所有的Pod，不包括未初始化的pod
kubectl describe pods --include-uninitialized=false
# 在当前1.17.5版本中，此参数已经不生产效了
#Error: unknown flag: --include-uninitialized
#See 'kubectl describe --help' for usage.
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl describe nodes k8s-master-50
Name:               k8s-master-50
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=k8s-master-50
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/master=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 192.168.5.50/24
                    projectcalico.org/IPv4IPIPTunnelAddr: 10.81.230.0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 27 Dec 2023 11:02:11 +0800
Taints:             node-role.kubernetes.io/master:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  k8s-master-50
  AcquireTime:     <unset>
  RenewTime:       Fri, 29 Dec 2023 10:40:43 +0800
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Fri, 29 Dec 2023 09:54:24 +0800   Fri, 29 Dec 2023 09:54:24 +0800   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Fri, 29 Dec 2023 10:38:53 +0800   Wed, 27 Dec 2023 11:02:09 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Fri, 29 Dec 2023 10:38:53 +0800   Wed, 27 Dec 2023 11:02:09 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Fri, 29 Dec 2023 10:38:53 +0800   Wed, 27 Dec 2023 11:02:09 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Fri, 29 Dec 2023 10:38:53 +0800   Wed, 27 Dec 2023 11:17:04 +0800   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.5.50
  Hostname:    k8s-master-50
Capacity:
  cpu:                2
  ephemeral-storage:  59313872Ki
  hugepages-2Mi:      0
  memory:             16388392Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  54663664345
  hugepages-2Mi:      0
  memory:             16285992Ki
  pods:               110
System Info:
  Machine ID:                 c849151047d18342826600c28ebd79c1
  System UUID:                c8491510-47d1-8342-8266-00c28ebd79c1
  Boot ID:                    cf221679-f52f-4ab2-b804-91308f2f665f
  Kernel Version:             5.4.265-1.el7.elrepo.x86_64
  OS Image:                   CentOS Linux 7 (Core)
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://18.9.8
  Kubelet Version:            v1.17.5
  Kube-Proxy Version:         v1.17.5
PodCIDR:                      10.81.0.0/24
PodCIDRs:                     10.81.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  kube-system                 calico-kube-controllers-6b94766748-hfbjs    0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 calico-node-brwl7                           250m (12%)    0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 coredns-6955765f44-rq92j                    100m (5%)     0 (0%)      70Mi (0%)        170Mi (1%)     47h
  kube-system                 coredns-6955765f44-wmr68                    100m (5%)     0 (0%)      70Mi (0%)        170Mi (1%)     47h
  kube-system                 etcd-k8s-master-50                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 kube-apiserver-k8s-master-50                250m (12%)    0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 kube-controller-manager-k8s-master-50       200m (10%)    0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 kube-proxy-l6k6x                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 kube-scheduler-k8s-master-50                100m (5%)     0 (0%)      0 (0%)           0 (0%)         47h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                1 (50%)     0 (0%)
  memory             140Mi (0%)  340Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From                       Message
  ----    ------                   ----               ----                       -------
  Normal  Starting                 46m                kubelet, k8s-master-50     Starting kubelet.
  Normal  NodeHasSufficientMemory  46m (x8 over 46m)  kubelet, k8s-master-50     Node k8s-master-50 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    46m (x8 over 46m)  kubelet, k8s-master-50     Node k8s-master-50 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     46m (x7 over 46m)  kubelet, k8s-master-50     Node k8s-master-50 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  46m                kubelet, k8s-master-50     Updated Node Allocatable limit across pods
  Normal  Starting                 46m                kube-proxy, k8s-master-50  Starting kube-proxy.
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl describe pods/tomcat9-run-6595fb5f85-gtxdp
Name:         tomcat9-run-6595fb5f85-gtxdp
Namespace:    default
Priority:     0
Node:         k8s-node52/10.0.2.15
Start Time:   Thu, 28 Dec 2023 16:25:57 +0800
Labels:       pod-template-hash=6595fb5f85
              run=tomcat9-run
Annotations:  cni.projectcalico.org/podIP: 10.81.237.194/32
              cni.projectcalico.org/podIPs: 10.81.237.194/32
Status:       Running
IP:           10.81.237.194
IPs:
  IP:           10.81.237.194
Controlled By:  ReplicaSet/tomcat9-run-6595fb5f85
Containers:
  tomcat9-run:
    Container ID:   docker://7a6a24e758c62d5a5e9b8075256bb1d63f252c3b438e1484b62d160b7f4dfa7f
    Image:          tomcat:9.0.20-jre8-alpine
    Image ID:       docker-pullable://tomcat@sha256:17accf0afeeecce0310d363490cd60a788aa4630ab9c9c802231d6fbd4bb2375
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 29 Dec 2023 09:54:33 +0800
    Last State:     Terminated
      Reason:       Error
      Exit Code:    143
      Started:      Thu, 28 Dec 2023 16:25:59 +0800
      Finished:     Thu, 28 Dec 2023 17:36:13 +0800
    Ready:          True
    Restart Count:  1
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason          Age                From                 Message
  ----    ------          ----               ----                 -------
  Normal  SandboxChanged  48m (x2 over 49m)  kubelet, k8s-node52  Pod sandbox changed, it will be killed and re-created.
  Normal  Pulled          48m                kubelet, k8s-node52  Container image "tomcat:9.0.20-jre8-alpine" already present on machine
  Normal  Created         48m                kubelet, k8s-node52  Created container tomcat9-run
  Normal  Started         48m                kubelet, k8s-node52  Started container tomcat9-run
[root@k8s-master-50 ~]# 

[root@k8s-master-50 ~]# kubectl describe pods  tomcat9-run-6595fb5f85-fls4k
Name:         tomcat9-run-6595fb5f85-fls4k
Namespace:    default
Priority:     0
Node:         k8s-node51/10.0.2.15
Start Time:   Thu, 28 Dec 2023 16:18:50 +0800
Labels:       pod-template-hash=6595fb5f85
              run=tomcat9-run
Annotations:  cni.projectcalico.org/podIP: 10.81.92.66/32
              cni.projectcalico.org/podIPs: 10.81.92.66/32
Status:       Running
IP:           10.81.92.66
IPs:
  IP:           10.81.92.66
Controlled By:  ReplicaSet/tomcat9-run-6595fb5f85
Containers:
  tomcat9-run:
    Container ID:   docker://180f29f74c2ff39f40c7618b08bf735c684109beec52e60a0aad4d0188ec4279
    Image:          tomcat:9.0.20-jre8-alpine
    Image ID:       docker-pullable://tomcat@sha256:17accf0afeeecce0310d363490cd60a788aa4630ab9c9c802231d6fbd4bb2375
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 29 Dec 2023 09:53:52 +0800
    Last State:     Terminated
      Reason:       Error
      Exit Code:    143
      Started:      Thu, 28 Dec 2023 16:18:51 +0800
      Finished:     Thu, 28 Dec 2023 17:36:13 +0800
    Ready:          True
    Restart Count:  1
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>
[root@k8s-master-50 ~]# 
```

说明：

`kubectl get`命令通常用于检索同一资源类型的一个或者多个资源，它具有丰富的参数，允许使用-o或者--output参数自定义输出格式，也可以指定-w或者--watch开始观察特定对象的一时半会儿。

`kubectl describe`命令更侧重于描述指定资源的许多方面，它可以调用对API服务的多个API调用来为用户构建视图。例如: `kubectl describe node`命令不仅检索有关节点信息，还检索在其上运行的pod的摘要，为节点生成事件等。

### 进入容器命令

kubectl exec - 对pod中的容器执行命令。 与docker的exec命令非常类似

```sh
# 从pod <pod-name> 中获取运行‘date’的输出。默认情况下，输出来自第一个容器
kubectl exec <pod-name> date


# 在指定的pod(<tpod-name>)和指定的容器(<container-name>)进行输出
kubectl exec <pod-name> -c <container-name> date

# 使用一个交互的TTY并运行/bin/bash.默认情况下，输出来自第一个容器
kubectl exec -it <pod-name> /bin/bash
```



输出:

```sh
[root@k8s-master-50 ~]# kubectl exec tomcat9-run-6595fb5f85-xn8qs date
Sat Dec 30 11:10:22 UTC 2023
[root@k8s-master-50 ~]# kubectl exec tomcat9-run-6595fb5f85-nb2lq -c tomcat9-run date
Sat Dec 30 11:33:12 UTC 2023
[root@k8s-master-50 ~]# kubectl exec -it tomcat9-run-6595fb5f85-xn8qs bash
bash-4.4# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
4: eth0@if3248: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1440 qdisc noqueue state UP 
    link/ether 0a:49:be:8d:69:30 brd ff:ff:ff:ff:ff:ff
    inet 10.81.149.196/32 scope global eth0
       valid_lft forever preferred_lft forever
bash-4.4# exit
exit
```



### logs命令

kubectl logs - 打印Pod中容器的日志

```sh
# 从pod返回日志快照。
kubectl logs <pod-name>

# 从<pod-name>开始流程式传输日志，这类似于linux的'tail -f'命令
kubectl logs -f <pod-name>
```

输出

```sh
[root@k8s-master-50 ~]# kubectl logs tomcat9-run-6595fb5f85-nb2lq
30-Dec-2023 11:24:33.714 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version name:   Apache Tomcat/9.0.20
30-Dec-2023 11:24:33.721 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server built:          May 3 2019 22:26:00 UTC
30-Dec-2023 11:24:33.721 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version number: 9.0.20.0
......
30-Dec-2023 11:24:34.486 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["http-nio-8080"]
30-Dec-2023 11:24:34.491 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["ajp-nio-8009"]
30-Dec-2023 11:24:34.492 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in [482] milliseconds
[root@k8s-master-50 ~]# kubectl logs -f tomcat9-run-6595fb5f85-nb2lq
30-Dec-2023 11:24:33.714 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version name:   Apache Tomcat/9.0.20
30-Dec-2023 11:24:33.721 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server built:          May 3 2019 22:26:00 UTC
30-Dec-2023 11:24:33.721 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version number: 9.0.20.0
......
30-Dec-2023 11:24:34.484 INFO [main] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory [/usr/local/tomcat/webapps/manager] has finished in [50] ms
30-Dec-2023 11:24:34.486 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["http-nio-8080"]
30-Dec-2023 11:24:34.491 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["ajp-nio-8009"]
30-Dec-2023 11:24:34.492 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in [482] milliseconds
^C
```

### 格式化输出:

```sh
#将pod信息格式化输出到一个yaml文件中
kubectl get pod <pod-name> -o yaml
```

输出：

```yaml
[root@k8s-master-50 ~]# kubectl get pod tomcat9-run-6595fb5f85-nb2lq -o yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    cni.projectcalico.org/podIP: 10.81.92.68/32
    cni.projectcalico.org/podIPs: 10.81.92.68/32
  creationTimestamp: "2023-12-30T11:24:31Z"
  generateName: tomcat9-run-6595fb5f85-
  labels:
    pod-template-hash: 6595fb5f85
    run: tomcat9-run
  name: tomcat9-run-6595fb5f85-nb2lq
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: tomcat9-run-6595fb5f85
    uid: a681fe0d-f5d7-4974-9a6c-1539a55facd8
  resourceVersion: "220905"
  selfLink: /api/v1/namespaces/default/pods/tomcat9-run-6595fb5f85-nb2lq
  uid: c495c84b-4f75-4a84-a7cb-56db195d44a8
spec:
  containers:
  - image: tomcat:9.0.20-jre8-alpine
    imagePullPolicy: IfNotPresent
    name: tomcat9-run
    ports:
    - containerPort: 8080
      protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-bgthv
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: k8s-node51
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: default-token-bgthv
    secret:
      defaultMode: 420
      secretName: default-token-bgthv
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2023-12-30T11:24:31Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2023-12-30T11:24:33Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2023-12-30T11:24:33Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2023-12-30T11:24:31Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://6a4c1088bd02c51c8a54d37e221b9211ee0f84d340b414a9fdc471421abf1666
    image: tomcat:9.0.20-jre8-alpine
    imageID: docker-pullable://tomcat@sha256:17accf0afeeecce0310d363490cd60a788aa4630ab9c9c802231d6fbd4bb2375
    lastState: {}
    name: tomcat9-run
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2023-12-30T11:24:33Z"
  hostIP: 10.0.3.5
  phase: Running
  podIP: 10.81.92.68
  podIPs:
  - ip: 10.81.92.68
  qosClass: BestEffort
  startTime: "2023-12-30T11:24:31Z"
[root@k8s-master-50 ~]#
```



### **delete命令**

kubectl delete 从文件、stdin或者指定标签选择器、名称、资源选择器或资源中删除资源

```sh
# 使用pod.yaml文件中类型和名称删除pod
kubectl delete -f pod.yaml

# 删除标签名=<lab-name>的所有pod和服务
kubectl delete pods,services -l name=<label-name>

# 删除所有具有标签名称=<label-name>的pod和服务，包括未初始化的那些
kubectl delete pods,services -l name=<label-name> --include-uninitialized

# 删除所有pod 包括未初始化的pod
kubectl delete pods --all

# 强制删除一个pod，需要添加参数
--force --grace-period=0
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl delete pod tomcat9-run-6595fb5f85-wdmxg
pod "tomcat9-run-6595fb5f85-wdmxg" deleted

[root@k8s-master-50 ~]# kubectl delete pod  tomcat9-run-6595fb5f85-nb2lq  --force --grace-period=0
warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
pod "tomcat9-run-6595fb5f85-nb2lq" force deleted
```



### 资源缩写

| 资源名                   | 缩写名 | API分组                   | 按命名空间 | 资源类型                |
| ------------------------ | ------ | ------------------------- | ---------- | ----------------------- |
| configmaps               | cm     |                           | true       | ConfigMap               |
| namespaces               | ns     |                           | false      | Namespace               |
| nodes                    | no     |                           | false      | Node                    |
| persistentvolumeclaims   | pvc    |                           | true       | PersistentVolumeClaim   |
| persistentvolumes        | pv     |                           | false      | PersistentVolume        |
| pods                     | po     |                           | true       | Pod                     |
| secrets                  |        |                           | true       | Secret                  |
| serviceaccounts          | sa     |                           | true       | ServiceAccount          |
| services                 | svc    |                           | true       | Service                 |
| daemonsets               | ds     | apps                      | true       | DaemonSet               |
| horizontalpodautoscalers | hpa    | autoscaling               | true       | HorizontalPodAutoscaler |
| cronjobs                 | cj     | batch                     | true       | CronJob                 |
| jobs                     |        | batch                     | true       | Job                     |
| ingresses                | ing    | extensions                | true       | ingress                 |
| poddisruptionbudgets     | pod    | policy                    | true       | PodDisruptionBudget     |
| clusterrolebindings      |        | rbac.authorization.k8s.io | false      | ClusterRoleBinding      |
| Clusterroles             |        | rbac.authorization.k8s.io | false      | clusterRole             |
| rolebindings             |        | rbac.authorization.k8s.io | true       | RoleBinding             |
| roles                    |        | rbac.authorization.k8s.io | true       | Role                    |
| storageclasses           | sc     | storage.k8s.io            | false      | StorageClass            |







## 使用yml文件配制k8s

### 使用yaml文件配制namespace

nullnullnamespace.yml

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: nullnull
```

远程运行

```sh
mkdir -p /data/k8s/namespace
cd /data/k8s/namespace
# 将文件上传到此目录

# 运行创建namespace的yml文件
kubectl apply -f nullnullnamespace.yml


# 可以运行删除Namespace
kubectl delete -f nullnullnamespace.yml
```

输出：

```sh
[root@k8s-master-50 namespace]# kubectl apply -f nullnullnamespace.yml
namespace/nullnull created
[root@k8s-master-50 namespace]# kubectl get namespace
NAME              STATUS   AGE
default           Active   3d13h
kube-node-lease   Active   3d13h
kube-public       Active   3d13h
kube-system       Active   3d13h
nullnull          Active   56s
[root@k8s-master-50 namespace]# kubectl delete -f nullnullnamespace.yml 
namespace "nullnull" deleted
[root@k8s-master-50 namespace]# kubectl get namespace
NAME              STATUS   AGE
default           Active   3d13h
kube-node-lease   Active   3d13h
kube-public       Active   3d13h
kube-system       Active   3d13h
[root@k8s-master-50 namespace]# 
```

### 使用yaml文件创建pod

nullnullpod.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tomcat9
  labels:
    app: tomcat9
 spec:
    containers:
      - name: tomcat9
        image: tomcat:9.0.20-jre8-alpine
        imagePullPolicy: ifNotPresent
    restartPolicy: Always
```

imagePullPolicy:

```sh
# 镜像下载的策略：
imagePullPolicy:
	Always: 总是拉取
	IfNotPresent: 如果本地有镜像，使用本地，如果本地没有镜像，下载镜像。
	Never: 只使用本地镜像，从不拉取
```

restartPolicy:

```sh
restartPolicy:
	Always: 只要退出就重启
	OnFailure: 失败退出时（exit code 不为0）才重启。
	Never: 永不重启
```

远程运行

```sh
mkdir -p /data/k8s/pod
cd /data/k8s/pod
# 将文件上传到此目录

# 运行pod
kubectl apply -f nullnullpod.yml

# 删除pod
kubectl delete -f nullnullpod.yml
```

输出:

```sh
[root@k8s-master-50 pod]# kubectl apply -f nullnullpod.yml
pod/tomcat9 created
[root@k8s-master-50 pod]# kubectl get pod
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9                        1/1     Running   0          12s
tomcat9-run-6595fb5f85-7d2xw   1/1     Running   0          4h45m
tomcat9-run-6595fb5f85-w96pz   1/1     Running   0          4h44m
tomcat9-run-6595fb5f85-xn8qs   1/1     Running   0          5h23m
[root@k8s-master-50 pod]# kubectl get pod -o wide
NAME                           READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES
tomcat9                        1/1     Running   0          28s     10.81.149.197   k8s-node53   <none>           <none>
tomcat9-run-6595fb5f85-7d2xw   1/1     Running   0          4h45m   10.81.92.69     k8s-node51   <none>           <none>
tomcat9-run-6595fb5f85-w96pz   1/1     Running   0          4h44m   10.81.237.197   k8s-node52   <none>           <none>
tomcat9-run-6595fb5f85-xn8qs   1/1     Running   0          5h23m   10.81.149.196   k8s-node53   <none>           <none>
[root@k8s-master-50 pod]# curl 10.81.149.197:8080



<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        <div id="wrapper">
           	......
            <p class="copyright">Copyright &copy;1999-2023 Apache Software Foundation.  All Rights Reserved</p>
        </div>
    </body>

</html>
[root@k8s-master-50 pod]# kubectl delete -f nullnullpod.yml 
pod "tomcat9" deleted
[root@k8s-master-50 pod]# kubectl get pod -o wide
NAME                           READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES
tomcat9-run-6595fb5f85-7d2xw   1/1     Running   0          4h48m   10.81.92.69     k8s-node51   <none>           <none>
tomcat9-run-6595fb5f85-w96pz   1/1     Running   0          4h46m   10.81.237.197   k8s-node52   <none>           <none>
tomcat9-run-6595fb5f85-xn8qs   1/1     Running   0          5h25m   10.81.149.196   k8s-node53   <none>           <none>
```



### 使用yml文件Deployment

nullnulldeployment.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tomcat-deployment
  labels:
    apps: tomcat-deployment
spec:
  replicas: 3
  template:
    metadata:
      name: tomcat-deployment
      labels:
        app: tomcat
    spec:
      containers:
        - name: tomcat-deployment
          image: tomcat:9.0.20-jre8-alpine
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
  selector:
    matchLabels:
      app: tomcat
```

matchLabels

```sh
# 总结：
# 在Deployment中必须写matchLabels
# 在定义模板的时候必须定义labels，因为Deployment.spec.selector是必须的字段，而他又必须和template.labels对应。
```

运行deployment

```sh
mkdir -p /data/k8s/deployment
cd /data/k8s/deployment


kubectl apply -f nullnulldeployment.yml

# 查看deployment
kubectl get deployment -o wide

# 删除deployment
kubectl delete -f  nullnulldeployment.yml

# 再次查看deployment
kubectl get deployment -o wide
```

输出

```sh
[root@k8s-master-50 deployment]# kubectl apply -f nullnulldeployment.yml 
deployment.apps/tomcat-deployment created
[root@k8s-master-50 deployment]# kubectl get deployment -o wide
NAME                READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS          IMAGES                      SELECTOR
tomcat-deployment   3/3     3            3           58s   tomcat-deployment   tomcat:9.0.20-jre8-alpine   app=tomcat
[root@k8s-master-50 deployment]# kubectl delete -f  nullnulldeployment.yml
deployment.apps "tomcat-deployment" deleted
[root@k8s-master-50 deployment]# kubectl get deployment -o wide
No resources found in default namespace.
[root@k8s-master-50 deployment]# 
```

**控制器类型**

| 控制器名称  | 作用                                                         |
| ----------- | ------------------------------------------------------------ |
| Deployment  | 声明式更新控制器，用于发布无状态应用                         |
| ReplicaSet  | 副本集控制器，用于对Pod进行副本规模扩大和剪裁                |
| StatefulSet | 有状态的副本集，用于发布有状态的应用                         |
| DaemonSet   | 在k8s集群每个上node运行一个副本，用于发布监控或者日志收集类的应用。 |
| Job         | 运行一次性作业任务                                           |
| CronJob     | 运行周期性作业任务                                           |

Deployment控制器介绍：

具有上线部署、滚动升级、创建副本、回滚到以前某一版本（成功/稳定）等功能

Deployment包含ReplicaSet，除非需要自定义升级功能或者根本不需要升级Pod，否则还是建议使用Deployment而不是直接使用ReplicaSet



### 使用yml文件创建service

nullnullservice.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tomcat-deploy
  labels:
    app: tomcat-deploy
spec:
  replicas: 1
  template:
    metadata:
      name: tomcat-deploy
      labels:
        app: tomcat-pod
    spec:
      containers:
        - name: tomcat-deploy
          image: tomcat:9.0.20-jre8-alpine
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
      restartPolicy: Always
  selector:
    matchLabels:
      app: tomcat-pod

---
apiVersion: v1
kind: Service
metadata:
  name: tomcat-svc
spec:
  selector:
    app: tomcat-pod
  ports:
    - port: 8888
      targetPort: 8080
      nodePort: 30088
      protocol: TCP
  type: NodePort
```

servicer的selector

```sh
service.spec.selector.app 选择的内容仍是template.label.app的内容，不是deployment控制器的label内容。
```

service类型

```sh
ClusterIP: 默认，分配一个集群内部可以访问的虚拟IP
NodePort： 在每个Node上分配一个端口作业外部访问入口 
LoadBalancer: 工作在特定的Cloud Provider上，例如：Google Cloud,AWS, openStack
ExternalName: 表示把集群外部的服务引入到集群内部中来，即实现了集群内部pod和集群外部的服务进行通信。
```

service参数：

```sh
port: 访问service使用的端口
targetPort: pod中容器端口
nodePort: 通过Node实现外网用户访问k8s集群内service(30000-32767)
```

运行

```sh
mkdir -p /data/k8s/service
cd /data/k8s/service


kubectl apply -f nullnullservice.yml

# 查看service
kubectl get service -o wide

# 访问测试
curl *.*.*.*: xxxx

# 删除servioce
kubectl delete -f  nullnullservice.yml

# 再次查看service
kubectl get service -o wide
```

输出：

```sh
[root@k8s-master-50 service]# kubectl apply -f nullnullservice.yml
deployment.apps/tomcat-deploy created
service/tomcat-svc created
[root@k8s-master-50 service]# kubectl get service -o wide
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE     SELECTOR
kubernetes    ClusterIP   10.1.0.1       <none>        443/TCP          4d13h   <none>
tomcat-svc    NodePort    10.1.77.65     <none>        8888:30088/TCP   25s     app=tomcat-pod
[root@k8s-master-50 service]# curl http://192.168.5.50:30088



<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        <div id="wrapper">
            ......
        </div>
    </body>

</html>
[root@k8s-master-50 service]# kubectl delete -f  nullnullservice.yml
deployment.apps "tomcat-deploy" deleted
service "tomcat-svc" deleted
[root@k8s-master-50 service]# kubectl get service -o wide
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
kubernetes   ClusterIP   10.1.0.1     <none>        443/TCP   4d13h   <none>

```



## 资源清单

### pod

资源清单有5 个顶组的字段组成： apiVersion、kind、metadata、spec、status。

```yaml
# 如果没有给定group名称，那么默认为core,可以使用kubectl apiversions 获取当前k8s版本上所有的apiVersion的版本信息（每个版本可能不同）
apiVersion: group/apiversion 
# 资源类型
kind: 

# 资源元数据信息
metadata:
  name:
  namespace:
  labes:
  # 主要目的是方便用户阅读查找
  annotations:

# 期望的状态(disired state)
spec:

# 当前的状态，本字段有kubernetes自身维护，用户不用去定义
status
```

使用kubectl命令可以查看apiVersion的各个版本信息

```sh
[root@k8s-master-50 ~]# kubectl api-versions
admissionregistration.k8s.io/v1
admissionregistration.k8s.io/v1beta1
apiextensions.k8s.io/v1
apiextensions.k8s.io/v1beta1
apiregistration.k8s.io/v1
apiregistration.k8s.io/v1beta1
apps/v1
authentication.k8s.io/v1
authentication.k8s.io/v1beta1
authorization.k8s.io/v1
authorization.k8s.io/v1beta1
autoscaling/v1
autoscaling/v2beta1
autoscaling/v2beta2
batch/v1
batch/v1beta1
certificates.k8s.io/v1beta1
coordination.k8s.io/v1
coordination.k8s.io/v1beta1
crd.projectcalico.org/v1
discovery.k8s.io/v1beta1
events.k8s.io/v1beta1
extensions/v1beta1
networking.k8s.io/v1
networking.k8s.io/v1beta1
node.k8s.io/v1beta1
policy/v1beta1
rbac.authorization.k8s.io/v1
rbac.authorization.k8s.io/v1beta1
scheduling.k8s.io/v1
scheduling.k8s.io/v1beta1
storage.k8s.io/v1
storage.k8s.io/v1beta1
v1
[root@k8s-master-50 ~]#
```

获取字段设置的帮助文档

```sh
[root@k8s-master-50 ~]# kubectl explain pod
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status       <Object>
     Most recently observed status of the pod. This data may not be up to date.
     Populated by the system. Read-only. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

[root@k8s-master-50 ~]# kubectl explain namespace
KIND:     Namespace
VERSION:  v1

DESCRIPTION:
     Namespace provides a scope for Names. Use of multiple namespaces is
     optional.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Spec defines the behavior of the Namespace. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status       <Object>
     Status describes the current status of a Namespace. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

[root@k8s-master-50 ~]# kubectl explain deployment
KIND:     Deployment
VERSION:  apps/v1

DESCRIPTION:
     Deployment enables declarative updates for Pods and ReplicaSets.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object metadata.

   spec <Object>
     Specification of the desired behavior of the Deployment.

   status       <Object>
     Most recently observed status of the Deployment.

[root@k8s-master-50 ~]# kubectl explain service
KIND:     Service
VERSION:  v1

DESCRIPTION:
     Service is a named abstraction of software service (for example, mysql)
     consisting of local port (for example 3306) that the proxy listens on, and
     the selector that determines which pods will answer requests sent through
     the proxy.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Spec defines the behavior of a service.
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status       <Object>
     Most recently observed status of the service. Populated by the system.
     Read-only. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

[root@k8s-master-50 ~]# 
```

字段配制的格式

```sh
<map[String]string><[]string><[]Object>

apiVersion <string> # 表示字符串类型
metadata <Object> # 表示需要嵌套多层字段
labels <map[String]string> #表示由k:v组成的映射
finalizers <[]string>      # 表示字符串列表
ownerReferences <[]Object> # 表示对象列表
hostPid <boolean>    # 布尔类型
priority <Integer>   # 整型
name <string>  -required- # 如果类型后面接-required- 表示为必填字段
```

![image-20240103123031953](.\images\image-20240103123031953.png)

### init C案例

准备镜像

```sh
docker pull busybox:1.32.0
docker pull nginx:1.17.10-alpine
```

initC的特点：

1. initC总是运行到成功为止。
2. 每个init C容器都必须在下一下init启动之前成功完成。
3. 如果initC容器运行失败，K8S会不断的重启该POD，直到initC容器成功为止
4. 如果pod对应的restartPolicy为never,它就不会重新启动。



initcpod.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
    - name: myapp-container
      image: busybox:1.32.0
      imagePullPolicy: IfNotPresent
      # 整个命令的意思是：打印“the app is running”，然后等待1小时。
      # 如果“echo the app is running”命令成功执行（返回值为0），那么sleep 3600命令才会被执行。
      command: ['sh','-c','echo the app is running && sleep 3600']
  initContainers:
    - name: init-myservice
      image: busybox:1.32.0
      imagePullPolicy: IfNotPresent
      # 等待某个服务变得可用，等待一个 DNS 记录被创建或更新，以便其他服务可以解析并使用它。
      command: ['sh','-c','until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
    - name: init-mydb
      image: busybox:1.32.0
      command: ['sh','-c','until nslookup mydb; do echo waiting for mydb; sleep 2; done;']
```

init-myservice.yml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
```

init-mydb.yml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9377
```



执行命令:

```sh
# 将文件上传k8s服务器，准备执行


# 启动initcpod的服务
kubectl apply -f initcpod.yml

# 查看pod的启动情况
kubectl get pods

# 查看详细的pod的启动情况
kubectl describe pod myapp-pod

# 查看myapp-pod中的第一个initcontainer日志
kubectl logs myapp-pod -c init-myservice


# 运行init-myservice服务
kubectl apply -f init-myservice.yml


# 查看init-myservice服务的运行情况
kubectl get svc

# 查看myapp-pod的运行情况，需要等一会，会发现pod的第一个init已经就绪
kubectl get pods

# 运行mydb服务
kubectl apply -f init-mydb.yml

# 查看mydb服务的运行情况
kubectl get svc

# 检查mydb的运行信息
kubectl get pods -w
```



输出：

```sh
[root@k8s-master-50 initc]# kubectl apply -f initcpod.yml
pod/myapp-pod created
[root@k8s-master-50 initc]# kubectl get pods
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          6s
[root@k8s-master-50 initc]# kubectl describe pod myapp-pod
Name:         myapp-pod
Namespace:    default
Priority:     0
Node:         k8s-node52/10.0.3.6
Start Time:   Wed, 03 Jan 2024 18:50:07 +0800
Labels:       app=myapp
Annotations:  cni.projectcalico.org/podIP: 10.81.237.200/32
              cni.projectcalico.org/podIPs: 10.81.237.200/32
              kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"app":"myapp"},"name":"myapp-pod","namespace":"default"},"spec":{"c...
Status:       Pending
IP:           10.81.237.200
IPs:
  IP:  10.81.237.200
Init Containers:
  init-myservice:
    Container ID:  docker://5293d13fac38790005149034f193d53924e23e99431d0bb21057c67e3b5ebd42
    Image:         busybox:1.32.0
    Image ID:      docker-pullable://busybox@sha256:bde48e1751173b709090c2539fdf12d6ba64e88ec7a4301591227ce925f3c678
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      until nslookup myservice; do echo waiting for myservice; sleep 2; done;
    State:          Running
      Started:      Wed, 03 Jan 2024 18:50:26 +0800
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
  init-mydb:
    Container ID:  
    Image:         busybox:1.32.0
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      until nslookup mydb; do echo waiting for mydb; sleep 2; done;
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Containers:
  myapp-container:
    Container ID:  
    Image:         busybox:1.32.0
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      echo the app is running && sleep 3600
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       False 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From                 Message
  ----    ------     ----  ----                 -------
  Normal  Scheduled  65s   default-scheduler    Successfully assigned default/myapp-pod to k8s-node52
  Normal  Pulling    63s   kubelet, k8s-node52  Pulling image "busybox:1.32.0"
  Normal  Pulled     46s   kubelet, k8s-node52  Successfully pulled image "busybox:1.32.0"
  Normal  Created    46s   kubelet, k8s-node52  Created container init-myservice
  Normal  Started    46s   kubelet, k8s-node52  Started container init-myservice
  
  # 可以发现，正在初始化init-myservice,但由于现在init-myservice还没有启动，肯定是启动不成功的
  
  [root@k8s-master-50 initc]# kubectl logs myapp-pod -c init-myservice
Server:         10.1.0.10
Address:        10.1.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer

waiting for myservice
Server:         10.1.0.10
Address:        10.1.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer

waiting for myservice
Server:         10.1.0.10
Address:        10.1.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer

waiting for myservice
Server:         10.1.0.10
Address:        10.1.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer

waiting for myservice
Server:         10.1.0.10
Address:        10.1.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer

# 可以发现，由于服务没有启动，在不断的打印日志，输出

[root@k8s-master-50 initc]# kubectl apply -f init-myservice.yml
service/init-myservice created
[root@k8s-master-50 initc]# kubectl get svc
NAME             TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
init-myservice   ClusterIP   10.1.11.253   <none>        80/TCP    37s
kubernetes       ClusterIP   10.1.0.1      <none>        443/TCP   7d7h
# 第一个初始化服务已经成功

[root@k8s-master-50 ~]# kubectl get pods 
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          6m58s

# 此时服务还未检测到，第一阶段还是失败状态。

[root@k8s-master-50 ~]# kubectl get pods -w
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          28m
myapp-pod   0/1     Init:1/2   0          28m
myapp-pod   0/1     Init:1/2   0          28m

# 启动了初始化的服务1，init已经成功了一半了

[root@k8s-master-50 initc]# kubectl apply -f init-mydb.yml
service/mydb created
[root@k8s-master-50 initc]# kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.1.0.1       <none>        443/TCP   7d8h
mydb         ClusterIP   10.1.251.136   <none>        80/TCP    6s
myservice    ClusterIP   10.1.191.192   <none>        80/TCP    5m2s
[root@k8s-master-50 initc]# kubectl get pods -w
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:1/2   0          32m
myapp-pod   0/1     PodInitializing   0          33m
myapp-pod   1/1     Running           0          33m
  
```

至此initC的案例已经成功的实现了。

### readinessProbe(准备就绪)

readinessprobepod.yml

```yam
apiVersion: v1
kind: Pod
metadata:
  name: readinessprobe-pod
  labels:
    app: readinessprobe-pod
spec:
  containers:
    - name: readinessprobe-pod
      image: nginx:1.17.10-alpine
      imagePullPolicy: IfNotPresent
      readinessProbe:
        httpGet:
          port: 80
          path: /index1.html
        initialDelaySeconds: 1
        periodSeconds: 3
  restartPolicy: Always
```

执行命令:

```sh
# 创建pod
kubectl apply -f readinessprobepod.yml

#检查pod状态，虽然pod状态显示为runnind，但是ready显示为0/1，因为就绪检查未通过
kubectl get pods

# 查看pod的详细信息，文件最后一行显示readiness probe failed...
kubectl describe pod readinessprobe-pod

# 容器内部文件 
kubectl exec -it readinessprobe-pod sh
cd /usr/share/nginx/html
echo "hello World!" >> index1.html
exit

# 查看pod的状态
kubectl get pods 

```

输出：

```sh
[root@k8s-master-50 initc]# kubectl apply -f readinessprobepod.yml
pod/readinessprobe-pod created
[root@k8s-master-50 initc]# kubectl get pods
NAME                 READY   STATUS              RESTARTS   AGE
myapp-pod            1/1     Running             1          2d3h
readinessprobe-pod   0/1     ContainerCreating   0          8s
[root@k8s-master-50 initc]# kubectl describe pod readinessprobe-pod
Name:         readinessprobe-pod
Namespace:    default
Priority:     0
Node:         k8s-node51/10.0.3.5
Start Time:   Fri, 05 Jan 2024 22:04:13 +0800
Labels:       app=readinessprobe-pod
Annotations:  cni.projectcalico.org/podIP: 10.81.92.72/32
              cni.projectcalico.org/podIPs: 10.81.92.72/32
              kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"app":"readinessprobe-pod"},"name":"readinessprobe-pod","namespace"...
Status:       Running
IP:           10.81.92.72
IPs:
  IP:  10.81.92.72
Containers:
  readinessprobe-pod:
    Container ID:   docker://fe03c9f9948cca1b50e20b4fafa8cc5a7cbe8a21432ebaa6b7435fbc46de1823
    Image:          nginx:1.17.10-alpine
    Image ID:       docker-pullable://nginx@sha256:763e7f0188e378fef0c761854552c70bbd817555dc4de029681a2e972e25e30e
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Fri, 05 Jan 2024 22:04:55 +0800
    Ready:          False
    Restart Count:  0
    Readiness:      http-get http://:80/index1.html delay=1s timeout=1s period=3s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age               From                 Message
  ----     ------     ----              ----                 -------
  Normal   Scheduled  <unknown>         default-scheduler    Successfully assigned default/readinessprobe-pod to k8s-node51
  Normal   Pulling    56s               kubelet, k8s-node51  Pulling image "nginx:1.17.10-alpine"
  Normal   Pulled     16s               kubelet, k8s-node51  Successfully pulled image "nginx:1.17.10-alpine"
  Normal   Created    15s               kubelet, k8s-node51  Created container readinessprobe-pod
  Normal   Started    15s               kubelet, k8s-node51  Started container readinessprobe-pod
  Warning  Unhealthy  1s (x5 over 13s)  kubelet, k8s-node51  Readiness probe failed: HTTP probe failed with statuscode: 404
[root@k8s-master-50 initc]# kubectl exec -it readinessprobe-pod sh
/ # cd /usr/share/nginx/html
/usr/share/nginx/html # echo "hello World!" >> index1.html
/usr/share/nginx/html # exit
[root@k8s-master-50 initc]# kubectl get pods 
NAME                 READY   STATUS    RESTARTS   AGE
myapp-pod            1/1     Running   1          2d3h
readinessprobe-pod   1/1     Running   0          2m26s
```

到此容器已经成功的开始运行。



### livenessProbe(存活检查) 案例一

livenessprobepod.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: livenessprobe-pod
  labels:
    app: livenessprobe-pod
spec:
  containers:
  
    - name: livenessprobe-pod
      image: busybox:1.32.0
      imagePullPolicy: IfNotPresent
      command: ["/bin/sh","-c","touch /tmp/livenesspod ; sleep 30 ; rm -rf /tmp/livenesspod; sleep 3600"]
      livenessProbe:
        exec:
          command: ["text","-e","/tmp/livenesspod"]
        initialDelaySeconds: 1
        periodSeconds: 3
  restartPolicy: Always
```

执行命令:

```sh
# 创建pod
kubectl apply -f livenessprobepod.yml

# 监控pod的状态变化，容器正常启动
kubectl get pod -w

# 等待30秒后，发现pod的RESTARTS值从0变为1，说明pod已经启动一次。

```

输出:

```sh
[root@k8s-master-50 initc]# kubectl apply -f livenessprobepod.yml
pod/livenessprobe-pod created
[root@k8s-master-50 initc]# kubectl get pod -w
NAME                READY   STATUS              RESTARTS   AGE
livenessprobe-pod   0/1     ContainerCreating   0          7s
livenessprobe-pod   1/1     Running             0          41s
livenessprobe-pod   1/1     Running             1          80s
```



### livenessProbe(存活检查) 案例二

livenessprobenginxpod.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: livenessprobenginx-pod
  labels:
    app: livenessprobenginx-pod
spec:
  containers:
    - name: livenessprobenginx-pod
      image: nginx:1.17.10-alpine
      imagePullPolicy: IfNotPresent
      ports:
        - containerPort: 80
          name: nginxhttpget
      livenessProbe:
        httpGet:
          port: 80
          path: /index.html
        initialDelaySeconds: 1
        periodSeconds: 3
        timeoutSeconds: 10
  restartPolicy: Always
```

操作

```sh
# 创建pod
kubectl apply -f livenessprobenginxpod.yml

# 查看pod状态
kubectl get pod

# 查看容器IP访问index.html，可正常访问
kubectl get pod -o wide
curl 10.81.X.X

#删除容器中的index.html
kubectl exec -it livenessprobenginx-pad -- rm -rf /usr/share/nginx/html/index.html

# 再次监控的状态，等一段时间后，poad的RESTARTS值从1变成2.说明pod已经重启一次。
kubectl get pod -w

#因为liveness监控index.html页面已经被删除，所以pod需要重新启动，重启后又重新 创建nginx镜像。nginx镜像中默认有index.html页面。

```

输出：

```sh
[root@k8s-master-50 initc]# kubectl apply -f livenessprobenginxpod.yml 
pod/livenessprobenginx-pod created
[root@k8s-master-50 initc]# kubectl get pod -w
NAME                     READY   STATUS              RESTARTS   AGE
livenessprobenginx-pod   0/1     ContainerCreating   0          7s
[root@k8s-master-50 initc]# kubectl describe pod livenessprobenginx-pod 
Name:         livenessprobenginx-pod
Namespace:    default
Priority:     0
Node:         k8s-node52/10.0.3.6
Start Time:   Fri, 05 Jan 2024 23:38:16 +0800
Labels:       app=livenessprobenginx-pod
Annotations:  cni.projectcalico.org/podIP: 10.81.237.202/32
              cni.projectcalico.org/podIPs: 10.81.237.202/32
              kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"app":"livenessprobenginx-pod"},"name":"livenessprobenginx-pod","na...
Status:       Running
IP:           10.81.237.202
IPs:
  IP:  10.81.237.202
Containers:
  livenessprobenginx-pod:
    Container ID:   docker://f98b23299eeb422798ca8b6aed9772f527ecb43f5c0624fe028f45d3e2378e00
    Image:          nginx:1.17.10-alpine
    Image ID:       docker-pullable://nginx@sha256:763e7f0188e378fef0c761854552c70bbd817555dc4de029681a2e972e25e30e
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 05 Jan 2024 23:39:43 +0800
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:80/index.html delay=1s timeout=10s period=3s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age        From                 Message
  ----    ------     ----       ----                 -------
  Normal  Scheduled  <unknown>  default-scheduler    Successfully assigned default/livenessprobenginx-pod to k8s-node52
  Normal  Pulling    99s        kubelet, k8s-node52  Pulling image "nginx:1.17.10-alpine"
  Normal  Pulled     14s        kubelet, k8s-node52  Successfully pulled image "nginx:1.17.10-alpine"
  Normal  Created    14s        kubelet, k8s-node52  Created container livenessprobenginx-pod
  Normal  Started    14s        kubelet, k8s-node52  Started container livenessprobenginx-pod
[root@k8s-master-50 initc]# kubectl get pod -o wide
NAME                     READY   STATUS    RESTARTS   AGE    IP              NODE         NOMINATED NODE   READINESS GATES
livenessprobenginx-pod   1/1     Running   0          114s   10.81.237.202   k8s-node52   <none>           <none>
[root@k8s-master-50 initc]# curl 10.81.237.202
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
[root@k8s-master-50 initc]# kubectl exec -it livenessprobenginx-pod -- rm -rf /usr/share/nginx/html/index.html
[root@k8s-master-50 initc]# kubectl get pod -w
NAME                     READY   STATUS    RESTARTS   AGE
livenessprobenginx-pod   1/1     Running   0          4m53s
livenessprobenginx-pod   1/1     Running   1          4m57s
[root@k8s-master-50 ~]# kubectl get pods -o wide
NAME                     READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
livenessprobenginx-pod   1/1     Running   2          10h   10.81.237.203   k8s-node52   <none>           <none>
[root@k8s-master-50 ~]#  curl 10.81.237.203
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
[root@k8s-master-50 ~]# kubectl describe pod livenessprobenginx-pod
Name:         livenessprobenginx-pod
Namespace:    default
Priority:     0
Node:         k8s-node52/10.0.3.6
Start Time:   Fri, 05 Jan 2024 23:38:16 +0800
Labels:       app=livenessprobenginx-pod
Annotations:  cni.projectcalico.org/podIP: 10.81.237.203/32
              cni.projectcalico.org/podIPs: 10.81.237.203/32
              kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"app":"livenessprobenginx-pod"},"name":"livenessprobenginx-pod","na...
Status:       Running
IP:           10.81.237.203
IPs:
  IP:  10.81.237.203
Containers:
  livenessprobenginx-pod:
    Container ID:   docker://cf8d25f2f2f13352daaa5b4f6f1b3b05cb9b539a06b10dcf81d156d997b26e87
    Image:          nginx:1.17.10-alpine
    Image ID:       docker-pullable://nginx@sha256:763e7f0188e378fef0c761854552c70bbd817555dc4de029681a2e972e25e30e
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Sat, 06 Jan 2024 09:32:25 +0800
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 05 Jan 2024 23:43:12 +0800
      Finished:     Sat, 06 Jan 2024 00:17:55 +0800
    Ready:          True
    Restart Count:  2
    Liveness:       http-get http://:80/index.html delay=1s timeout=10s period=3s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason          Age                From                 Message
  ----     ------          ----               ----                 -------
  Normal   Scheduled       <unknown>          default-scheduler    Successfully assigned default/livenessprobenginx-pod to k8s-node52
  Normal   Pulling         10h                kubelet, k8s-node52  Pulling image "nginx:1.17.10-alpine"
  Normal   Pulled          10h                kubelet, k8s-node52  Successfully pulled image "nginx:1.17.10-alpine"
  Normal   Created         10h (x2 over 10h)  kubelet, k8s-node52  Created container livenessprobenginx-pod
  Normal   Started         10h (x2 over 10h)  kubelet, k8s-node52  Started container livenessprobenginx-pod
  Warning  Unhealthy       10h (x3 over 10h)  kubelet, k8s-node52  Liveness probe failed: HTTP probe failed with statuscode: 404
  Normal   Pulled          10h                kubelet, k8s-node52  Container image "nginx:1.17.10-alpine" already present on machine
  Normal   Killing         10h                kubelet, k8s-node52  Container livenessprobenginx-pod failed liveness probe, will be restarted
  Warning  FailedMount     21m                kubelet, k8s-node52  MountVolume.SetUp failed for volume "default-token-bgthv" : failed to sync secret cache: timed out waiting for the condition
  Normal   SandboxChanged  21m                kubelet, k8s-node52  Pod sandbox changed, it will be killed and re-created.
  Normal   Pulled          21m                kubelet, k8s-node52  Container image "nginx:1.17.10-alpine" already present on machine
  Normal   Created         21m                kubelet, k8s-node52  Created container livenessprobenginx-pod
  Normal   Started         21m                kubelet, k8s-node52  Started container livenessprobenginx-pod
```

通过观察可以发现此pod在index.html文件被删除后，进行一次容器的重新创建，通过`describe`观察也可以证实是重新创建了容器，则不是重启了容器。



### livenessprobe(存活检查)案例三

livenessprobenginxpod2.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: livenessprobenginx-pod2
  labels:
    app: livenessprobenginx-pod2
spec:
  containers:
    - name: livenessprobenginx-pod2
      image: nginx:1.17.10-alpine
      imagePullPolicy: IfNotPresent
      livenessProbe:
        tcpSocket:
          # 监测8080端口，如果8080端口没有反馈，重启pod
          port: 8080
        initialDelaySeconds: 10
        periodSeconds: 3
        timeoutSeconds: 5
  restartPolicy: Always
```

操作命令:

```sh
# 创建pod
kubectl apply -f livenessprobenginxpod2.yml

# 查看pod的状态
kubectl get pod -w

# 存活检查监听8080端口，8080端口没有反馈信息后重启pod，RESTARTS值从0变为1
```

输出：

```sh
[root@k8s-master-50 initc]# kubectl apply -f livenessprobenginxpod2.yml 
pod/livenessprobenginx-pod2 created
[root@k8s-master-50 initc]# kubectl get pods -w
NAME                      READY   STATUS              RESTARTS   AGE
livenessprobenginx-pod2   0/1     ContainerCreating   0          17s
livenessprobenginx-pod2   1/1     Running             0          21s
livenessprobenginx-pod2   1/1     Running             1          38s
livenessprobenginx-pod2   1/1     Running             2          56s
```



### 沟子函数案例

lifeclepod.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: lifecle-pod1
  labels:
    app: lifecle-pod1
spec:
  containers:
    - name: lifecle-pod1
      image: busybox:1.32.0
      imagePullPolicy: IfNotPresent
      lifecycle:
        postStart:
          exec:
            #创建/data/kis/目录，在目录下创建index.html文件
            command: ['mkdir','-p','/data/k8s/index.html']
      command: ['sh','-c','sleep 5000']
  restartPolicy: Always
```

命令操作:

```sh
# 创建pod
kubectl apply -f lifeclepod.yml

# 查看pod的状态
kubectl get pod

# 进告诉容器，在 /data/k8s,创建index.html文件
kubectl exec -it lifecle-pod1 sh
mkdir /data/k8s
ls

```

输出:

```sh
[root@k8s-master-50 initc]# kubectl apply -f lifeclepod.yml
pod/lifecle-pod1 created
[root@k8s-master-50 initc]# kubectl get pod
NAME           READY   STATUS    RESTARTS   AGE
lifecle-pod1   1/1     Running   0          9s
[root@k8s-master-50 initc]# kubectl exec -it lifecle-pod1 sh
/ # mkdir -p  /data/k8s
/ # ls
bin   data  dev   etc   home  proc  root  sys   tmp   usr   var
/ # cd data
/data # ls
k8s
/data # cd k8s/
/data/k8s # ls
index.html
/data/k8s # 
```



### 总结

pod对象至创建开始至终止退出的时间范围称为生命周期，在这段时间内，pod会处于多种不同的状态，并执行一些操作，其中，创建主容器为必须的操作，其他可选的操作还包括运行初始化容器（init container）、容器启动后沟子(start hook)、容器的存活探测（liveness probe）、就绪性探测（readiness probe）以及容器终止前沟子（pre stop hook）等，这些操作是否执行取决于pod的定义。

#### pod的相位

使用`kubectl get pod`命令，STATUS被称之为相位（phase）.

无论是手动创建还是通过控制器创建pod，pod对象总是处于生命进程中以下几个相位之一：

- pending： apiserver创建了pod资源对象并存入etcd中，但它沿未被调度完成或者仍处于下载镜像的过程中。
- running: pod已经被调度至某个节点，并且所有容器已经被kubectl创建完成。
- succeeded: pod中的所有的容器都已经成功终止并且不被重启。
- failed: 所有容器都已经终止，但至少有一个容器至少终止失败，即容器返回了非0的退出状态或者已经被系统终止。
- unknown: apiserver无法获取到pod对象的状态信息，通常是由于其无法于所在工作节点kubelet通信所致。

pod的相位是在其生命周期中的宏观概念，而非对容器或者pod对象的综合汇总，而且相位的数量和含义被严格界定。



#### pod的创建过程

pod是k8s的基础单元，以下是一个pod资源对象的典型创建过程：

1. 通用通过kubectl或者其他api客户端提交pod spec给api server。
2.  api server尝试着将pod对象的相关信息存入etcd中，待定入操作执行完成，api server即会返回确认信息至客户端。
3. api server开始反应etcd中的状态变化。
4. 所有k8s的组件均使用watch机制来跟踪检查api server上的相关变动。
5. kube-scheduler通过其watch觉察到api server创建了新的pod对象但尚未绑定至任何工作节点。
6. kube-scheduler为pod对象挑选一个工作节点并将结果信息更新至api server
7. 调度结果信息由api server更新至etcd，而且api server也开始反应此pod对象的调度结果。
8. pod被调度到目标工作节点上的kubelet尝试在当前节点上调用docker启动容器，并将容器的结果状态回送至api server
9. api server将Pod状态存入etcd中
10. 在etcd确认写入操作成功完成后，api server将确认信息发送至相关的kubelet。



#### pod生命周期中的重要行为

除了创建应用容器之外，用户还可以为pod对象定义其生命周期中的多种行为，如初始化容器、存活性探测及就就绪性探测等。

1. 初始化容器

初始化容器即应用程序的主容器启动之前要运行的容器，常用于为主容器执行一些预置操作，它们具有两种典型特征：

（1） 初始化容器必须运行完成至结束，若某初始化容器运行失败，那k8s需要重启它，直到成功为止。

（2） 初始化容器都必须按定义的顺序串运行。

有不少的场景都需要在应用程序启动之前进行部分初始化操作，例如: 等待其他相关联的组件服务可用、基于环境变量或配制模板为应用程序生成配制文件、从配制中心获取配置等。初始化容器的典型应用需求具体包含如下几种：

1）运用特定的工具程序，出于安全等反面原因，这些程序不适于包含在主容器镜像中。

2）提供主容器镜像中不具备的工具程序或者自定义代码

3） 为容器镜像的构建和部署人员提供了分离、独立工作的途径、使得它们不必协同起来制作单个镜像文件。

4）初始化容器和主容器处于不同的文件系统视图中，因此可以分别安全地使用敏感数据，例如secrets资源。

5）初始化容器要先于应用容器串行启动并运行完成，因此可以用于延后应用容器的启动直到其依赖的条件得到满足。

pod资源的spec.initContainers字段以列表形式定义可用的初始容器，其嵌套可用字段类似于spec.containers。



#### pod生命周期的钩子函数

容器生命周期钩子使它能够感知其自身生命周期管理中的事件，并在相应的时刻到来时运行由用户指定的处理程序代码，k8s为容器提供了两种生命周期钩子：

- postStart ： 于容器创建完成之后立即运行的钩子处理器（handler）,不过k8s无法确保它一定会于容器中的entrypoint之前运行。
- preStop: 于容器终止操作之前立即运行的钩子处理器，它以同步的方式调用，因此在其完成之前会阻赛删除容器的调用。

钩子的处理器的实现方法由Exec和HTTP两种，Exec在钩子事件触发时直接在当前容器中运行由用户定义的命令，HTTP则在当前容器中向某个URL发起HTTP请求。postStart和preStop处理器定义在spec.lifecycle嵌套字段中。



#### 容器探测

容器探测时pod对象生命周期中的一项重要的日常任务，它是kubelet对容器周期性执行的健康状态诊断，诊断操作由容器的处理器定义，k8s支持三种容器控探针用于pod探测：

- ExecAction: 在容器中执行一个命令，并根据其返回的状态码进行诊断操作称为Exec探测，状态码为0表示成功，否则即为不健康状态
- TCPSocketAction: 通过与容器的某TCP端口尝试建立连接进行诊断，端口能够成功打开即为正常，否则为不健康状态。
- HTTPGetAcion: 通过向容器IP地址的某指定端口的指定path发起HTTP Get请求进行诊断，响应码大于等于200且小于400时即为成功。

任何一种探测方式都可能存在三种结果：

1） success（成功）容器诊断通过

2） failure(失败)   容器未通过诊断

3）unknown(未知)  诊断失败，因此不会采取任何行动。

kubelet右要活动容器上执行两种类型的检测：

(livenessProbe)存活性检查：用于判断容器是否处于运行状态，一量此检测未通过，kubelet将杀死容器并根据restartPolicy决定是否将其重启，未定义存活性检测的容器的默认状态为success

(readinessProbe)就绪性检查：用于判断容器是否准备就绪并可对外提供服务；未通过检测的容器意味着尚未准备就绪，端点控制会将其IP从所有匹配到此pod对象的service对象的端点列表移除；检测通过之后，会再次将其IP添加至端点列表中。



#### 容器的重启策略

容器程序发生崩溃或者容器申请超出限制的资源等原因都可能会导致 pod对象的终止，此时是否应用重建该pod对象则取决于重启策略（restartPolicy）属性：

- Alwasy: 但凡对象终止就将其重启，此为默认设定。
- OnFailure: 仅在pod对象出现错误时方才将其重启。
- Never: 从不重启。

restartPolicy适用于pod对象中的所有容器。而它仅用于控制在同一节点上重新启动pod对象的相关容器。首次需要重启的容器，将在其需要时立即进行重启，随后两次需要重启的操作将由kubelet延迟一段时间后进行，且反复的重启操作的延迟时长以此为10S、20S、40S、80S、160S、300S，300s是最大时延时长。事实上，一旦绑定到一个节点上，pod对象将永远不会重新绑定到另外一个节点，它要么被重启，要么终止，直到节点发生故障或者删除。



#### pod的终止过程

当用户提交删除请求之后，系统就会进行强制删除操作的宽限期倒计时，并将TERM信息发送给pod对象的每个容器的主进程。宽限期倒计时结束后，这些进程将收到强制终止的KILL信号，pod对象随即也将由api server删除，如果在等待进程终止的过程中，kubelet或者容器管理器发生了重启，那么终止 操作会重新获取一个满额的删除宽限期并重新执行删除操作。

具体流程

1. 用户发送删除pod对象的命令。
2. api服务器中的Pod对象会随着时间的推移而更新，在宽限期内（默认30s），pod被视为dead。
3. 将pod的标识terminating状态。
4. 与第三种同时进行，kubelet在监控pod对象转为terminating状态的同时启动pod关闭过程。
5. 与第三步同时运行，端点控制监控到pod对象的关闭行为时将其从所有匹配的到此端口的service资源的端点列表中移除。
6. 如果当前pod对象定义了proStop钩子处理器，则在其标记为terminating后即会以同步的方式启动执行，若宽限期结束后，preStop仍未执行结束，则第二步会被重新 执行并额外执行并额外获取一个时长为2S的小宽限期。
7. pod对象中的容器进程收到TERM信号。
8. 宽限期结束后，若存在任何一个仍在运行的进程，那么pod对象即会收到SIGKILL信号。
9. kubelet请求api server将此pod资源的宽限期设置为0从而完成删除操作，它变得对用户不再可见。

默认情况下，所有删除操作宽限期都是30S，不过kubectl delete可以使用`--grace-period=`选项自定义其时长，若使用0值，则表示直接强制删除指定资源，不过此时需要同时用命令`--forece`选项。





## k8s-pod控制器

简介：

 Controller Manager由kube-controller-manager和cloud-controller-manager组成，是Kubenetes的大脑，它通过apiserver监控整个集群的状态，并确保集群处于预期的工作状态。

kubectl-controller-manager由一系列的控制器组成

```sh
1 Replication Controller(复制控制器)
2 Node Controller（维护node状态）
3 CronJob Controller(处理Cron表达式的Job)
4 DaemonSet Controller（守护进程集）
5 Deployment Controller(无状态服务部署)
6 Endpoint Controller(依据 service spec 创建 endpoint，依据 podip 更新 endpoint。)
7 Garbage Controller(通过ownerReferences处理级联删除，它的内部会有一个Graph Builder，构建一个父子关系图)
8 Namespace Controller(保证 namespace 删除前该 namespace 下的所有资源都先被删除。)
9 Job Controller（处理 job。）
10 Pod AutoScaler(处理 Pod 的自动缩容/扩容)
11 ReplicaSet(复制集)
12 Service Controller(为 LoadBalancer type 的 service 创建 LB VIP。)
13 ServiceAccount Controller( 确保 serviceaccount 在当前 namespace 存在。)
14 StatefulSet Controller（有状态集）
15 Volume Controller( 依据 PV spec 创建 volume。)
16 Resource quota Controller(在用户使用资源之后，更新状态。)
```

cloud-controller-manager在 Kubernetes启用Cloud Provider的时候才需要， 用来配合云服务,提供商的控制， 也包括一系列的控制器

```sh
1 Node Controller
2 Route Controller
3 Service Controller
```

从v1.6开始，cloud provider已经经历了几次重大重构，以便在不修改Kubernetes核心代码的同时构建,自定义的云服务商支持





### 常见的pod控制器及含义 

1. ReplicaSet: 适合无状态的服务部署

​		用户创建指定数量的pod副本数量，确保pod副本数量符合预期状态，并且支持滚动式自动扩容和缩容功能。

​		ReplicaSet主要三个组件组成：

​			 (1) 用户期望的pod副本数量

​			 (2) 标签选择器，判断哪个pod归自己管理。

​			 (3) 当现存的pod数量不足，会根据pod资源模板进行新建

​		帮助用户管理无状态的pod资源，精确反应用户定义的目标数量，但是ReplicaSet不是直接使用控制器，而是使用Deployment。

2. Deployment： 适合无状态的服务部署

​			工作在ReplicaSet之上，用下载管理无状态的应用，目前来说最好的控制器。支持滚动更新和回滚功能，还提供声明式配置。

3. StatefullSet: 适合有状态的服务部署。需要学习完存储卷后进行系统学习。

4. DaemonSet: 一次部署，所有的节点都会部署，例如一些典型的应用场景：

   - 运行集群存储daemon,例如在每个node上运行glusterd、ceph。
   - 在每个node上运行日志收集daemon,例如fluentd、logstash。
   - 在每个Node上运行监控daemon,例如Prometheus Node Exporter

   用于确保集群中的每一个节点只运行特定的Pod副本，通常用于实现系统组后台任务。例如ELK服务。

   特点：服务是无状态的、服务必须是守护进程。

5. job： 一次性的执行任务。只要完成就立即退出，不需要重启或者重建。

6. Cronjob： 周期性的执行任务。周期性任务控制，不需要持续后台运行。



使用的镜像：

```sh
docker pull nginx:1.17.10-alpine
docker pull nginx:1.18.0-alpine
docker pull nginx:1.19.2-alpine
```



### Replication Controller控制器

Replication Controller简称RC，是kubernetes系统中的核心概念之一，简单来说，它其实定义了一个期望的场景，即场景某种pod的副本的数量在任意时刻都符合某个预期值，所以RC定义包含以下：

- pod期待的副本数量
- 用于筛选目标pod的Label Selector
- 当pod的副本数量小于期望值时，用于创建新的pod的pod模板（template）



**ReplicaSet**

ReplicationController用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的Pod来替代；而如果异常多出来的容器也会自动回收。

在新版本的Kubernetes中建议使用ReplicaSet来取代ReplicationController。ReplicaSet跟ReplicationController没有本质的不同，只是名字不一样，并且ReplicaSet支持集合式的selector。

虽然ReplicaSet可以独立使用，但一般还是建议使用 Deployment 来自动管理ReplicaSet，这样就无需担心跟其他机制的不兼容问题（比如ReplicaSet不支持rolling-update但Deployment支持）。

```yaml
apiVersion: apps/v1 #api版本定义
kind: ReplicaSet #定义资源类型为ReplicaSet
metadata: #元数据定义
  name: myapp
  namespace: default
spec: #ReplicaSet的规格定义
  replicas: 2 #定义副本数量为2个
  selector: #标签选择器，定义匹配pod的标签
    matchLabels:
      app: myapp
      release: canary
  template: #pod的模板定义
    metadata: #pod的元数据定义
      name: myapp-pod #自定义pod的名称
      labels: #定义pod的标签，需要和上面定义的标签一致，也可以多出其他标签
        app: myapp
        release: canary
        environment: qa
    spec: #pod的规格定义
      containers: #容器定义
      - name: myapp-container #容器名称
        image: nginx:1.17.10-alpine #容器镜像
        ports: #暴露端口
        - name: http
          containerPort: 80
```

可以通过kubectl命令行方式获取更加详细信息

```
kubectl explain rs
kubectl explain rs.spec
kubectl explain rs.spec.template.spec
```



controller/replicasetdemo.yml

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicasetdemo
  labels:
    app: replicasetdemo
spec:
  replicas: 3
  template:
    metadata:
      name: replicasetdemo
      labels:
        app: replicasetdemo
    spec:
      containers:
        - name: replicasetdemo
          image: nginx:1.17.10-alpine
          imagePUllPolicy: IfNotPresent
          ports:
            - containerPort: 80
      restartPolicy: Always
  selector:
    matchLabels:
      app: replicasetdemo
```

运行命令:

```sh
# 运行ReplicaSet
kubectl apply -f replicasetdemo.yml

# 查看rs控制器
kubectl get rs

# 查看pod信息
kubectl get pod

# 查看pod的详细信息
kubectl describe pod replicasetdemo-cxqxm

# 测试controller控制器下的pod删除、重新被controller控制器拉起
kubectl  delete pod --all
kubectl get pod


# 以命令行的方式修改pod的副本数量为8个
kubectl scale replicaset replicasetdemo --replicas=8
kubectl get rs
kubectl get pods


# 修改pod的副本数量，通过资源清单方式, 例如修改 replicas,修改为5，进行缩容,修改文件后，直接生效。
kubectl edit replicasets.apps replicasetdemo
kubectl get rs


# 显示pod的标签
kubectl get pod --show-labels
# 修改pod的标签
kubectl label pod replicasetdemo-48bmb app=nullnull --overwrite=True
# 再次显示pod的标签： 发现多了一个pod，原来的rs中又重新拉起一个pod，说明RS是通过label去管理pod
kubectl get pod --show-labels

# 删除rs
kubectl delete rs replicasetdemo
```

输出:

```sh
[root@k8s-master-50 controller]# kubectl apply -f replicasetdemo.yml 
replicaset.apps/replicasetdemo created
[root@k8s-master-50 initc]# kubectl get rs
NAME             DESIRED   CURRENT   READY   AGE
replicasetdemo   3         3         3       33m
[root@k8s-master-50 initc]# kubectl get pod
NAME                   READY   STATUS    RESTARTS   AGE
replicasetdemo-cxqxm   1/1     Running   0          35m
replicasetdemo-rlvh9   1/1     Running   0          35m
replicasetdemo-zpbqd   1/1     Running   0          35m
[root@k8s-master-50 initc]# kubectl describe pod replicasetdemo-cxqxm
Name:         replicasetdemo-cxqxm
Namespace:    default
Priority:     0
Node:         k8s-node52/10.0.3.6
Start Time:   Mon, 08 Jan 2024 09:12:19 +0800
Labels:       app=replicasetdemo
Annotations:  cni.projectcalico.org/podIP: 10.81.237.204/32
              cni.projectcalico.org/podIPs: 10.81.237.204/32
Status:       Running
IP:           10.81.237.204
IPs:
  IP:           10.81.237.204
Controlled By:  ReplicaSet/replicasetdemo
Containers:
  replicasetdemo:
    Container ID:   docker://583250ef6e4b43d52d2d3c0acf6aac42c3ba21a56ef9cc73d1d377d55e0c6bf0
    Image:          nginx:1.17.10-alpine
    Image ID:       docker-pullable://nginx@sha256:763e7f0188e378fef0c761854552c70bbd817555dc4de029681a2e972e25e30e
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Mon, 08 Jan 2024 09:12:19 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From                 Message
  ----    ------     ----  ----                 -------
  Normal  Scheduled  35m   default-scheduler    Successfully assigned default/replicasetdemo-cxqxm to k8s-node52
  Normal  Pulled     35m   kubelet, k8s-node52  Container image "nginx:1.17.10-alpine" already present on machine
  Normal  Created    35m   kubelet, k8s-node52  Created container replicasetdemo
  Normal  Started    35m   kubelet, k8s-node52  Started container replicasetdemo
[root@k8s-master-50 initc]# kubectl  delete pod --all
pod "replicasetdemo-cxqxm" deleted
pod "replicasetdemo-rlvh9" deleted
pod "replicasetdemo-zpbqd" deleted
[root@k8s-master-50 initc]# 
[root@k8s-master-50 initc]# kubectl get pod
NAME                   READY   STATUS    RESTARTS   AGE
replicasetdemo-h59k4   1/1     Running   0          14s
replicasetdemo-sv2v4   1/1     Running   0          14s
replicasetdemo-zr8v4   1/1     Running   0          14s
# 在所有的pod都删除后，由于deployment，还存在，又重新拉起了pod。副本数据为3个
[root@k8s-master-50 initc]# kubectl scale replicaset replicasetdemo --replicas=8
replicaset.apps/replicasetdemo scaled
[root@k8s-master-50 initc]# kubectl get rs
NAME             DESIRED   CURRENT   READY   AGE
replicasetdemo   8         8         8       41m
[root@k8s-master-50 initc]# kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
replicasetdemo-48bmb   1/1     Running   0          14s
replicasetdemo-5mj69   1/1     Running   0          14s
replicasetdemo-clcmk   1/1     Running   0          14s
replicasetdemo-h59k4   1/1     Running   0          4m21s
replicasetdemo-hlfrp   1/1     Running   0          14s
replicasetdemo-sv2v4   1/1     Running   0          4m21s
replicasetdemo-vc5kp   1/1     Running   0          14s
replicasetdemo-zr8v4   1/1     Running   0          4m21s
# 使用命令一键扩容后，即可看到新的容器已经被创建了出来。

[root@k8s-master-50 initc]# kubectl edit replicasets.apps replicasetdemo
Edit cancelled, no changes made.
[root@k8s-master-50 initc]# kubectl get rs
NAME             DESIRED   CURRENT   READY   AGE
replicasetdemo   5         5         5       49m
[root@k8s-master-50 initc]# kubectl get pod
NAME                   READY   STATUS    RESTARTS   AGE
replicasetdemo-48bmb   1/1     Running   0          8m1s
replicasetdemo-h59k4   1/1     Running   0          12m
replicasetdemo-hlfrp   1/1     Running   0          8m1s
replicasetdemo-sv2v4   1/1     Running   0          12m
replicasetdemo-zr8v4   1/1     Running   0          12m
[root@k8s-master-50 initc]# kubectl get pod --show-labels
NAME                   READY   STATUS    RESTARTS   AGE   LABELS
replicasetdemo-48bmb   1/1     Running   0          38m   app=replicasetdemo
replicasetdemo-h59k4   1/1     Running   0          42m   app=replicasetdemo
replicasetdemo-hlfrp   1/1     Running   0          38m   app=replicasetdemo
replicasetdemo-sv2v4   1/1     Running   0          42m   app=replicasetdemo
replicasetdemo-zr8v4   1/1     Running   0          42m   app=replicasetdemo
[root@k8s-master-50 initc]# kubectl label pod replicasetdemo-48bmb app=nullnull --overwrite=True
pod/replicasetdemo-48bmb labeled
[root@k8s-master-50 initc]# kubectl get pod --show-labels
NAME                   READY   STATUS    RESTARTS   AGE   LABELS
replicasetdemo-48bmb   1/1     Running   0          41m   app=nullnull
replicasetdemo-4vs97   1/1     Running   0          5s    app=replicasetdemo
replicasetdemo-h59k4   1/1     Running   0          45m   app=replicasetdemo
replicasetdemo-hlfrp   1/1     Running   0          41m   app=replicasetdemo
replicasetdemo-sv2v4   1/1     Running   0          45m   app=replicasetdemo
replicasetdemo-zr8v4   1/1     Running   0          45m   app=replicasetdemo
# 观察可以发现，当修改了labels后，又新建了一个pod

[root@k8s-master-50 initc]# kubectl delete rs replicasetdemo
replicaset.apps "replicasetdemo" deleted
[root@k8s-master-50 initc]# kubectl get pod --show-labels
NAME                   READY   STATUS        RESTARTS   AGE    LABELS
replicasetdemo-48bmb   1/1     Running       0          42m    app=nullnull
replicasetdemo-4vs97   1/1     Terminating   0          119s   app=replicasetdemo
replicasetdemo-h59k4   1/1     Terminating   0          47m    app=replicasetdemo
replicasetdemo-hlfrp   1/1     Terminating   0          42m    app=replicasetdemo
replicasetdemo-sv2v4   1/1     Terminating   0          47m    app=replicasetdemo
replicasetdemo-zr8v4   1/1     Terminating   0          47m    app=replicasetdemo
[root@k8s-master-50 initc]# kubectl get pod --show-labels
NAME                   READY   STATUS    RESTARTS   AGE   LABELS
replicasetdemo-48bmb   1/1     Running   0          43m   app=nullnull
[root@k8s-master-50 initc]# 
```

**总结**

kubectl命令行工具适用于RC绝大部分命令同样使用于ReplicaSet,此外，我们当前很少单独使用Replicaset，它主要被Demployment这个更高层的资源对象所使用，从而形成一整套pod创建、删除、更新的编排机制，我们在使用Demployment时无需关心它是如何维护和创建ReplicaSet的，这一切都是自自动发生的。

最后，总结下RC（ReplicaSet）的一些特性和作用：

- 在绝大多数情况下，我们通过定义一个RC实现的pod的创建及副本数量的自动控制。
- 在RC里包括完整的Pod定义模板。
- RC通过Label Selector机制实现对pod的副本的自动控制。
- 通过改变RC里的pod副本数量，可以实现pod的扩容和缩容。
- 通过改变RC里pod的模板中的镜像版本，可以实现滚动升级。





### Deployment

Deployment在kubernetes在1.2版本中引入的新概念，用于更好的解决Pod的编排问题，为此Deployment在内部使用了ReplicaSet来实现目的，我们可以把Deployment理解为ReplicaSet的一次升级，两者相似度超过90%

Deployment的使用场景：

- 创建一个Deployment对象来生成对应的ReplicaSet并完成Pod副本的创建。
- 检查Deployment的状态来看部署动作是否完成（Pod副本数量是否达到了预期的值）
- 更新Deployment以创建新的Pod（比如镜像升级）
- 如果当前Deployment不稳定，可以回滚到一个早先的Deployment版本。
- 暂停Deployment以便一次性修改多个PodTemplateSpec的配制项，之后在恢复Deployment,进行发布。
- 扩展Deployment以应对高负载。
- 查看Demployemtn的状态，以此作为发布是否成功的标志。
- 清理不在需要的旧版本ReplicaSet。

Deployment的模板

使用命令行获取更加详细的说明：

```sh
kubectl explain deploy
kubectl explain deploy.spec
kubectl explain deploy.spec.template.spec
```



除了API生命与kind类型的区别，Deployment的定义与ReplicaSet的定义很类似。

controller/demploymentdemo.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploymentdemo1
  labels:
    app: deploymentdemo1
spec:
  replicas: 10
  template:
    metadata:
      name: deploymentdemo1
      labels:
        app: deploymentdemo1
    spec:
      containers:
        - name: demploymentdemo1
          image: nginx:1.17.10-alpine
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80
      restartPolicy: Always
  selector:
    matchLabels:
      app: deploymentdemo1
```

操作命令

```sh
kubectl apply -f deploymentdemo.yml 

# 查看deployment
# 查看rs:demployment名称+hashcode码组成
kubectl get rs

# 查看pod
kubectl get pod
```



输出:

```sh
[root@k8s-master-50 controller]# kubectl apply -f deploymentdemo.yml 
deployment.apps/deploymentdemo1 created
[root@k8s-master-50 controller]# kubectl get rs
NAME                        DESIRED   CURRENT   READY   AGE
deploymentdemo1-787d9c585   10        10        0       6s
[root@k8s-master-50 controller]# kubectl get pod
NAME                              READY   STATUS              RESTARTS   AGE
deploymentdemo1-787d9c585-25gqk   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-csm6n   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-cw6p2   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-dfc4q   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-gbzff   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-hf2g5   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-m5lf5   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-pjvjj   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-s2dw8   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-zfxff   0/1     ContainerCreating   0          11s
[root@k8s-master-50 controller]# kubectl get pod
NAME                               READY   STATUS    RESTARTS   AGE
deploymentdemo1-56bfc59f9b-2qn8p   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-6wxp6   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-8vxpf   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-dxc7b   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-fq9w6   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-gsq8l   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-msg9x   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-n9ns8   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-xtww2   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-zc52j   1/1     Running   0          22s
```



**镜像更新升级**

命令行方式:

```sh
# 升级nginx镜像版本为1.18.0
kubectl set image deployment deploymentdemo1 deploymentdemo1=nginx:1.18.0-alpine


# 查看pod的升级情况
kubectl get pods -w

# 进入某一个pod，查看nginx的版本,检查是否已经成功的升级到了1.18.0
kubectl exec -it kubectl exec -it deploymentdemo1-df6bc5d4c-hgnnp sh
nginx -v
exit
```

操作：

```sh
[root@k8s-master-50 controller]# kubectl set image deployment deploymentdemo1 deploymentdemo1=nginx:1.18.0-alpine
deployment.apps/deploymentdemo1 image updated
[root@k8s-master-50 controller]# kubectl get pods -w
NAME                               READY   STATUS        RESTARTS   AGE
deploymentdemo1-7ccccfbf85-7lm84   0/1     Terminating   0          25s
deploymentdemo1-7ccccfbf85-cg2m2   0/1     Terminating   0          25s
deploymentdemo1-7ccccfbf85-g75bd   0/1     Terminating   0          25s
deploymentdemo1-7ccccfbf85-mg2hq   0/1     Terminating   0          25s
deploymentdemo1-7ccccfbf85-mmm7j   0/1     Terminating   0          25s
deploymentdemo1-7ccccfbf85-xw4vq   0/1     Terminating   0          25s
deploymentdemo1-df6bc5d4c-6tsdk    1/1     Running       0          5s
deploymentdemo1-df6bc5d4c-9l4vv    1/1     Running       0          7s
deploymentdemo1-df6bc5d4c-bnpfm    1/1     Running       0          7s
deploymentdemo1-df6bc5d4c-hgnnp    1/1     Running       0          4s
deploymentdemo1-df6bc5d4c-jv2hl    1/1     Running       0          5s
deploymentdemo1-df6bc5d4c-kthc4    1/1     Running       0          7s
deploymentdemo1-df6bc5d4c-pcksg    1/1     Running       0          7s
deploymentdemo1-df6bc5d4c-rg9ch    1/1     Running       0          5s
deploymentdemo1-df6bc5d4c-z64q2    1/1     Running       0          5s
deploymentdemo1-df6bc5d4c-zg2h6    1/1     Running       0          7s
deploymentdemo1-7ccccfbf85-cg2m2   0/1     Terminating   0          28s
deploymentdemo1-7ccccfbf85-cg2m2   0/1     Terminating   0          28s
deploymentdemo1-7ccccfbf85-7lm84   0/1     Terminating   0          28s
deploymentdemo1-7ccccfbf85-7lm84   0/1     Terminating   0          28s
deploymentdemo1-7ccccfbf85-mg2hq   0/1     Terminating   0          28s
deploymentdemo1-7ccccfbf85-mg2hq   0/1     Terminating   0          28s
deploymentdemo1-7ccccfbf85-xw4vq   0/1     Terminating   0          31s
deploymentdemo1-7ccccfbf85-xw4vq   0/1     Terminating   0          31s
deploymentdemo1-7ccccfbf85-g75bd   0/1     Terminating   0          31s
deploymentdemo1-7ccccfbf85-g75bd   0/1     Terminating   0          31s
deploymentdemo1-7ccccfbf85-mmm7j   0/1     Terminating   0          31s
deploymentdemo1-7ccccfbf85-mmm7j   0/1     Terminating   0          31s
^C[root@k8s-master-50 controller]# kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
deploymentdemo1-df6bc5d4c-6tsdk   1/1     Running   0          38s
deploymentdemo1-df6bc5d4c-9l4vv   1/1     Running   0          40s
deploymentdemo1-df6bc5d4c-bnpfm   1/1     Running   0          40s
deploymentdemo1-df6bc5d4c-hgnnp   1/1     Running   0          37s
deploymentdemo1-df6bc5d4c-jv2hl   1/1     Running   0          38s
deploymentdemo1-df6bc5d4c-kthc4   1/1     Running   0          40s
deploymentdemo1-df6bc5d4c-pcksg   1/1     Running   0          40s
deploymentdemo1-df6bc5d4c-rg9ch   1/1     Running   0          38s
deploymentdemo1-df6bc5d4c-z64q2   1/1     Running   0          38s
deploymentdemo1-df6bc5d4c-zg2h6   1/1     Running   0          40s
[root@k8s-master-50 controller]# kubectl exec -it deploymentdemo1-df6bc5d4c-hgnnp sh
/ # nginx -v
nginx version: nginx/1.18.0
/ # exit
[root@k8s-master-50 controller]# 
```

**yaml文件方式**

```sh
# 升级镜像的版本至1.19.2-alpine,将修改为“- image: nginx:1.19.2-alpine”
kubectl edit deployments.apps deploymentdemo1

# 查看pod升级的情况
kubectl get pods -w

# 进入一个容器查看容器升级后的版本信息
kubectl exec -it deploymentdemo1-584f6b54dd-j2t9l sh
nginx -v
exit
```

输出 ：

```sh
[root@k8s-master-50 controller]# kubectl edit deployments.apps deploymentdemo1
deployment.apps/deploymentdemo1 edited
[root@k8s-master-50 controller]# kubectl get pods -w
NAME                               READY   STATUS        RESTARTS   AGE
deploymentdemo1-584f6b54dd-22h66   1/1     Running       0          6s
deploymentdemo1-584f6b54dd-47d7n   1/1     Running       0          5s
deploymentdemo1-584f6b54dd-4n6xg   1/1     Running       0          5s
deploymentdemo1-584f6b54dd-65cvz   1/1     Running       0          6s
deploymentdemo1-584f6b54dd-7wrbk   1/1     Running       0          4s
deploymentdemo1-584f6b54dd-j2t9l   1/1     Running       0          6s
deploymentdemo1-584f6b54dd-r8ncx   1/1     Running       0          5s
deploymentdemo1-584f6b54dd-vhhsh   1/1     Running       0          6s
deploymentdemo1-584f6b54dd-xqtfm   1/1     Running       0          4s
deploymentdemo1-584f6b54dd-xs7rh   1/1     Running       0          6s
deploymentdemo1-7ccccfbf85-46wl2   0/1     Terminating   0          79s
deploymentdemo1-7ccccfbf85-v5hkk   0/1     Terminating   0          79s
deploymentdemo1-7ccccfbf85-46wl2   0/1     Terminating   0          81s
deploymentdemo1-7ccccfbf85-46wl2   0/1     Terminating   0          81s
deploymentdemo1-7ccccfbf85-v5hkk   0/1     Terminating   0          86s
deploymentdemo1-7ccccfbf85-v5hkk   0/1     Terminating   0          86s
[root@k8s-master-50 controller]# kubectl exec -it deploymentdemo1-584f6b54dd-j2t9l sh
/ # nginx -v
nginx version: nginx/1.19.2
/ # exit
[root@k8s-master-50 controller]# 
```



**Deployment扩容**

```sh
kubectl scale deployment deploymentdemo1 --replicas=16

# 查看当前的pod的信息
kubectl get pod
```

输出：

```sh
[root@k8s-master-50 controller]# kubectl scale deployment deploymentdemo1 --replicas=16
deployment.apps/deploymentdemo1 scaled
[root@k8s-master-50 controller]# kubectl get pod
NAME                               READY   STATUS    RESTARTS   AGE
deploymentdemo1-584f6b54dd-22h66   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-47d7n   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-4n6xg   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-5tv47   1/1     Running   0          3s
deploymentdemo1-584f6b54dd-65cvz   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-7wrbk   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-845dp   1/1     Running   0          3s
deploymentdemo1-584f6b54dd-9rx8s   1/1     Running   0          3s
deploymentdemo1-584f6b54dd-gwfpj   1/1     Running   0          3s
deploymentdemo1-584f6b54dd-j2t9l   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-lql4k   1/1     Running   0          3s
deploymentdemo1-584f6b54dd-qwl4k   1/1     Running   0          3s
deploymentdemo1-584f6b54dd-r8ncx   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-vhhsh   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-xqtfm   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-xs7rh   1/1     Running   0          93m
[root@k8s-master-50 controller]# kubectl scale deployment deploymentdemo1 --replicas=4
deployment.apps/deploymentdemo1 scaled
[root@k8s-master-50 controller]# kubectl get pod -w
NAME                               READY   STATUS        RESTARTS   AGE
deploymentdemo1-584f6b54dd-22h66   1/1     Running       0          93m
deploymentdemo1-584f6b54dd-47d7n   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-5tv47   0/1     Terminating   0          39s
deploymentdemo1-584f6b54dd-65cvz   1/1     Running       0          93m
deploymentdemo1-584f6b54dd-7wrbk   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-9rx8s   0/1     Terminating   0          39s
deploymentdemo1-584f6b54dd-j2t9l   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-lql4k   0/1     Terminating   0          39s
deploymentdemo1-584f6b54dd-qwl4k   0/1     Terminating   0          39s
deploymentdemo1-584f6b54dd-r8ncx   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-vhhsh   1/1     Running       0          93m
deploymentdemo1-584f6b54dd-xqtfm   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-xs7rh   1/1     Running       0          93m
deploymentdemo1-584f6b54dd-qwl4k   0/1     Terminating   0          39s
deploymentdemo1-584f6b54dd-qwl4k   0/1     Terminating   0          39s
deploymentdemo1-584f6b54dd-7wrbk   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-7wrbk   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-lql4k   0/1     Terminating   0          41s
deploymentdemo1-584f6b54dd-lql4k   0/1     Terminating   0          41s
deploymentdemo1-584f6b54dd-r8ncx   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-r8ncx   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-j2t9l   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-j2t9l   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-5tv47   0/1     Terminating   0          42s
deploymentdemo1-584f6b54dd-5tv47   0/1     Terminating   0          42s
deploymentdemo1-584f6b54dd-9rx8s   0/1     Terminating   0          46s
deploymentdemo1-584f6b54dd-9rx8s   0/1     Terminating   0          46s
deploymentdemo1-584f6b54dd-xqtfm   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-xqtfm   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-47d7n   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-47d7n   0/1     Terminating   0          93m
# 可以肉眼可见的速度，在将相应的pod进行停止操作。
[root@k8s-master-50 controller]# kubectl get pod 
NAME                               READY   STATUS    RESTARTS   AGE
deploymentdemo1-584f6b54dd-22h66   1/1     Running   0          95m
deploymentdemo1-584f6b54dd-65cvz   1/1     Running   0          95m
deploymentdemo1-584f6b54dd-vhhsh   1/1     Running   0          95m
deploymentdemo1-584f6b54dd-xs7rh   1/1     Running   0          95m
```





### 滚动更新

滚动更新一般存在4种部署方式：蓝绿部署、滚动部署、灰度发布、金丝雀发布。

1. 蓝绿部署, 是在不停老版本、部署新版本然后进行测试，确认OK，将流量切换到新版本，然后老版本同时也升级到新版本。蓝绿部署无需停机，并且风险极小。
1. 滚动发布，一般是取出一个或者多个服务器停止服务，执行更新，并重新将其投入使用，周而复始，直到集群中所有版本都更新成最新版本。这种部署方式相对于蓝绿部署，更加节约资源---它不需要运行两个集群、两倍的实例数。我们可以部分部署，例如每次只取出集群的20%进行升级。
1. 灰度发布：是指的在黑白之间，能够平滑过渡的一种发布方式。AB Test 就是一种灰度发布，让一部分用户继续用A，一部分用户开始用B，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁到B上面来，灰度发布可以保持整体系统的稳定，在初始灰度的时候，就可以发现、调整问题，以保证其影响度，而我们平常所说的金丝雀也就是灰度发布的一种方式。



#### 滚动更新-金丝雀发布

Deployment控制器支持自定义控制更新过程中的滚动节奏，如“暂停（pause）”或“继续（resume）”更新操作。比如等待第一批新的Pod资源创建完成后立即暂停更新过程，此时仅存在一部分版本的应用，主体部分还是旧的版本。然后再筛选一小部分的用户请求路由到新版本的Pod应用，继续观察能否稳定地按期望的方式运行。确定没有问题之后再继续完成余下的Pod资源滚动更新 ，否则立即回滚更新操作。这就是金丝雀发布（Canary Release）

```sh
# 更新Deployment的nginx:1.18.0-alpine版本，并配置暂停deployment
kubectl set image deployment deploymentdemo1 deploymentdemo1=nginx:1.18.0-alpine && kubectl rollout pause deployment deploymentdemo1

# 观察更新的状态
kubectl rollout status deployments deploymentdemo1

# 监控更新的过程，可以看到已经新增了一个资源，但是并未按照预期的状态去删除一个旧的资源，就是因为使用了pause暂停命令。
kubectl get pods -l app=deploymentdemo1 -w

# 确保更新的pod没问题，继续更新
kubectl rollout resume deploy deploymentdemo1

# 查看更新的情况
kubectl get pods -l app=deploymentdemo1 -w

# 进入某一个pod内部，查看nginx的更新版本信息
kubectl exec -it deploymentdemo1-df6bc5d4c-8mpfv sh
nginx -v
exit

```

输出：

```sh
[root@k8s-master-50 data]# kubectl set image deployment deploymentdemo1 deploymentdemo1=nginx:1.18.0-alpine && kubectl rollout pause deployment deploymentdemo1
deployment.apps/deploymentdemo1 image updated
deployment.apps/deploymentdemo1 paused
[root@k8s-master-50 data]# kubectl rollout status deployments deploymentdemo1
Waiting for deployment "deploymentdemo1" rollout to finish: 2 out of 4 new replicas have been updated...
[root@k8s-master-50 data]# kubectl get pods -l app=deploymentdemo1 -w
NAME                              READY   STATUS    RESTARTS   AGE
deploymentdemo1-df6bc5d4c-88xsz   1/1     Running   0          3m4s
deploymentdemo1-df6bc5d4c-8mpfv   1/1     Running   0          11s
deploymentdemo1-df6bc5d4c-8zxjf   1/1     Running   0          3m3s
deploymentdemo1-df6bc5d4c-ghjhc   1/1     Running   0          11s
# 通过观察时间可以发现，已经有两个pod完成了更新，还有两个未更新，这一兰
[root@k8s-master-50 data]# kubectl exec -it deploymentdemo1-df6bc5d4c-8mpfv sh
/ # nginx -v
nginx version: nginx/1.18.0
/ # exit
[root@k8s-master-50 data]# 
[root@k8s-master-50 data]# kubectl rollout resume deploy deploymentdemo1
deployment.apps/deploymentdemo1 resumed
[root@k8s-master-50 data]# kubectl rollout status deployments deploymentdemo1
deployment "deploymentdemo1" successfully rolled out
```



#### 滚动更新-版本回退

默认情况下，kubernetes会在系统中保存前两次的Deployment的rollout历史记录，以便可以随时回退（可以修改`revision history limit`来更改保存的revision数）

注意：只要Deployment的rollout被触发就会创建一个revision，也就是说当且仅当Deployment的Pod Template（如.spec.template）被更改，例如更新template中的label和容器镜像时，就会创建出一个新的revision

其他的更新，比如扩容Deployment不会被创建revision---因此我们可以很方便的手动或者自动扩容。这意味首当回退到历史revision时，只有Deployment中的pod template部分才会回退。

rollout常见命令

| 子命令  | 功能说明                   |
| ------- | -------------------------- |
| history | 查看rollout操作历史        |
| pause   | 将提供的资源设置为暂停状态 |
| restart | 重启某资源                 |
| resume  | 将某资源从暂停状态恢复正常 |
| status  | 查看rollout操作状态        |
| undo    | 回滚前一rollout            |

**history操作**

```sh
kubectl rollout history deployment deploymentdemo1
```

输出：

```sh
[root@k8s-master-50 ~]# kubectl rollout history deployment deploymentdemo1
deployment.apps/deploymentdemo1 
REVISION  CHANGE-CAUSE
4         <none>
5         <none>
6         <none>
```

**status操作**

```sh
kubectl rollout status deployment deploymentdemo1
```

输出：

```sh
[root@k8s-master-50 ~]# kubectl rollout status deployment deploymentdemo1
deployment "deploymentdemo1" successfully rolled out
```

**undo操作**

```sh
# 回滚版本信息
kubectl rollout undo deployment deploymentdemo1

# 查看pod回滚情况
kubectl get pods -w

# 进入一个pod内部，查看ngninx回滚版本信息
kubectl exec -it deploymentdemo1-df6bc5d4c-ghjhc sh
nginx -v 
exit
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl rollout undo deployment deploymentdemo1
deployment.apps/deploymentdemo1 rolled back
[root@k8s-master-50 ~]# kubectl get pods -w
NAME                               READY   STATUS        RESTARTS   AGE
deploymentdemo1-584f6b54dd-2rzqd   1/1     Running       0          5s
deploymentdemo1-584f6b54dd-7zsxn   1/1     Running       0          7s
deploymentdemo1-584f6b54dd-ddjjn   1/1     Running       0          6s
deploymentdemo1-584f6b54dd-wsh8n   1/1     Running       0          7s
deploymentdemo1-df6bc5d4c-88xsz    0/1     Terminating   1          13h
deploymentdemo1-df6bc5d4c-8zxjf    0/1     Terminating   1          13h
deploymentdemo1-df6bc5d4c-ghjhc    0/1     Terminating   1          13h
deploymentdemo1-df6bc5d4c-88xsz    0/1     Terminating   1          13h
deploymentdemo1-df6bc5d4c-88xsz    0/1     Terminating   1          13h
deploymentdemo1-df6bc5d4c-ghjhc    0/1     Terminating   1          13h
deploymentdemo1-df6bc5d4c-ghjhc    0/1     Terminating   1          13h
deploymentdemo1-df6bc5d4c-8zxjf    0/1     Terminating   1          13h
deploymentdemo1-df6bc5d4c-8zxjf    0/1     Terminating   1          13h
[root@k8s-master-50 ~]# kubectl get pod
NAME                               READY   STATUS    RESTARTS   AGE
deploymentdemo1-584f6b54dd-2rzqd   1/1     Running   0          66s
deploymentdemo1-584f6b54dd-7zsxn   1/1     Running   0          68s
deploymentdemo1-584f6b54dd-ddjjn   1/1     Running   0          67s
deploymentdemo1-584f6b54dd-wsh8n   1/1     Running   0          68s
[root@k8s-master-50 ~]# kubectl exec -it  deploymentdemo1-584f6b54dd-2rzqd  sh
/ # nginx -v
nginx version: nginx/1.19.2
/ # 
#至此可以发现nginx的版本已经回退到上一个版本1.19.2版本，因为之前在做升级测试时，已经成功的升级到了1.18.0
```

至此版本回退演示完毕。



#### deployment更新策略

Deployment可以保证在升级时只有一定数量的Pod是down的，默认的，它会保证至少 有比期望的POd数量少一个是Up状态（最多一个不可用）

Deployment同时也可以确保只创建出超过期望数量的一定数量的Pod，默认的，它会确保最多比期望的Pod数量多一个的Pod是Up的（最多1个surge）

Kubernetes版本V1.17.5中，从1-1变成25%-25%

```sh
kubectl describe deployments.apps deploymentdemo1
# 观察属性RollingUpdateStrategy:  25% max unavailable, 25% max surge
```

输出：

```sh
[root@k8s-master-50 ~]# kubectl describe deployments.apps deploymentdemo1
Name:                   deploymentdemo1
Namespace:              default
CreationTimestamp:      Tue, 09 Jan 2024 09:29:50 +0800
Labels:                 app=deploymentdemo1
Annotations:            deployment.kubernetes.io/revision: 7
                        kubectl.kubernetes.io/last-applied-configuration:
                          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"deploymentdemo1"},"name":"deploymentdemo1","name...
Selector:               app=deploymentdemo1
Replicas:               4 desired | 4 updated | 4 total | 4 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=deploymentdemo1
  Containers:
   deploymentdemo1:
    Image:        nginx:1.19.2-alpine
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   deploymentdemo1-584f6b54dd (4/4 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  6m44s  deployment-controller  Scaled up replica set deploymentdemo1-584f6b54dd to 1
  Normal  ScalingReplicaSet  6m44s  deployment-controller  Scaled down replica set deploymentdemo1-df6bc5d4c to 3
  Normal  ScalingReplicaSet  6m44s  deployment-controller  Scaled up replica set deploymentdemo1-584f6b54dd to 2
  Normal  ScalingReplicaSet  6m43s  deployment-controller  Scaled down replica set deploymentdemo1-df6bc5d4c to 2
  Normal  ScalingReplicaSet  6m43s  deployment-controller  Scaled up replica set deploymentdemo1-584f6b54dd to 3
  Normal  ScalingReplicaSet  6m42s  deployment-controller  Scaled down replica set deploymentdemo1-df6bc5d4c to 1
  Normal  ScalingReplicaSet  6m42s  deployment-controller  Scaled up replica set deploymentdemo1-584f6b54dd to 4
  Normal  ScalingReplicaSet  6m41s  deployment-controller  Scaled down replica set deploymentdemo1-df6bc5d4c to 0
[root@k8s-master-50 ~]# 
```

#### 总结

Deployment为POd和Replica Set(下一代Replication Controller)提供声明式更新。

只需要在Deployment中描述想要的目录状态是什么，Deployment Controller就会帮您将pod和ReplicaSet的实际状态改变到您的目标状态。也可以定义一个全新的Deployment来创建ReplicaSet或者删除已有的Deployment并创建一个新的来替换。

**Replicas(副本数量)**

`.spec.replicas`是可选字段，指定期望的pod数量，默认是1

**Selector(选择器)**

`.spec.select`是可选字段，用来指定`label.select`,圈定Deployment管理的pod范围，如果被指定，`spec.selector`必须匹配`.spec.template.metadata.labels`,否则它将被API拒绝。如果`.spec.selector`没有被指定，`spec.selector.matchLabels`默认是`.spec.template.metadata.labels`。

在pod的template跟`.spec.template`不同或者数量超过了`.spec.replicas`规定的数量的的情况下，Deployment会杀掉Labels跟selector不同的pod。

**Pod Template(pod模板)**

`.spec.template`是.spec中唯一要求的字段。

`.spec.template`是pod template，它跟pod有一模一样的schema,除了它是嵌套的并且不需要apiVersion和Kind字段。

另外为了划分pod的范围，Deployment中的pod template必须指定适当的label(不要跟其他的controller重复，参考selector)和适当的重启策略。

.spec.template.spec.restartPolicy 可以设置为 Always , 如果不指定的话这就是默认配置。

**strategy（更新策略）**

.spec.strategy 指定新的Pod替换旧的Pod的策略。 .spec.strategy.type 可以是"Recreate"或者是  "RollingUpdate"。"RollingUpdate"是默认值。

Recreate： 重建式更新，就是删一个建一个。类似于ReplicaSet的更新方式，即首先删除现有的 Pod对象，然后由控制器基于新模板重新创建新版本资源对象。

rollingUpdate：滚动更新，简单定义 更新期间pod最多有几个等。可以指定 maxUnavailable  和 maxSurge 来控制 rolling update 进程。

maxSurge： .spec.strategy.rollingUpdate.maxSurge 是可选配置项，用来指定可以超过期 望的Pod数量的最大个数。该值可以是一个绝对值（例如5）或者是期望的Pod数量的百分比（例如 10%）。当 MaxUnavailable 为0时该值不可以为0。通过百分比计算的绝对值向上取整。默认值是1。

例如，该值设置成30%，启动rolling update后新的ReplicatSet将会立即扩容，新老Pod的总数不 能超过期望的Pod数量的130%。旧的Pod被杀掉后，新的ReplicaSet将继续扩容，旧的ReplicaSet会进 一步缩容，确保在升级的所有时刻所有的Pod数量和不会超过期望Pod数量的130%。

maxUnavailable： .spec.strategy.rollingUpdate.maxUnavailable 是可选配置项，用来指 定在升级过程中不可用Pod的最大数量。该值可以是一个绝对值（例如5），也可以是期望Pod数量的百 分比（例如10%）。通过计算百分比的绝对值向下取整。 如 果 .spec.strategy.rollingUpdate.maxSurge 为0时，这个值不可以为0。默认值是1。

例如，该值设置成30%，启动rolling update后旧的ReplicatSet将会立即缩容到期望的Pod数量的 70%。新的Pod ready后，随着新的ReplicaSet的扩容，旧的ReplicaSet会进一步缩容确保在升级的所有 时刻可以用的Pod数量至少是期望Pod数量的70%。

**rollbackTo：**

.spec.rollbackTo 是一个可以选配置项，用来配置Deployment回退的配置。设置该参数将触发 回退操作，每次回退完成后，该值就会被清除。

revision：

 .spec.rollbackTo.revision 是一个可选配置项，用来指定回退到的revision。默认 是0，意味着回退到上一个revision。

**progressDeadlineSeconds：**

.spec.progressDeadlineSeconds 是可选配置项，用来指定在系统报告Deployment的failed  progressing——表现为resource的状态中 type=Progressing 、 Status=False 、  Reason=ProgressDeadlineExceeded 前可以等待的Deployment进行的秒数。Deployment  controller会继续重试该Deployment。未来，在实现了自动回滚后， deployment controller在观察到 这种状态时就会自动回滚。 如果设置该参数，该值必须大于 .spec.minReadySeconds 。

**paused：**

.spec.paused 是可以可选配置项，boolean值。用来指定暂停和恢复Deployment。Paused和没有 paused的Deployment之间的唯一区别就是，所有对paused deployment中的PodTemplateSpec的修 改都不会触发新的rollout。Deployment被创建之后默认是非paused。 





### DaemonSet

DaemonSet确保全部Node上运行一个Pod的副本。当有Node加入集群时，也会为他们增加一个pod。当有Node从集群移除时，这些Pod也会被回收。删除DaemonSet将会删除它创建的所有Pod。

在每一个Node节点上中调度一个Pod，因此它无需指定replicas的个数，比如：

- 在每个node上都运行一个日志采集程序，负责收集node节点本身和node节点之上的各个pod所产生的日志。
- 在每个node上运行一个性能监控程序，采集该node的运行性能数据。

DaemonSet模板说明：

```sh
kubectl explain daemonset
kubectl explain daemonset.spec
kubectl explain daemonset.spec.template.spec
```

部署DaemonSet

controller/daemonsetdemo.yml

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: daemonsetdemo
  labels:
    app: daemonsetdemo
spec:
  template:
    metadata:
      name: daemonsetdemo
      labels:
        app: daemonsetdemo
    spec:
      containers:
        - name: daemonsetdemo
          image: nginx:1.17.10-alpine
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
  selector:
    matchLabels:
      app: daemonsetdemo
```

运行命令:

```sh
# 运行demonset
kubectl apply -f daemonsetdemo.yml

# 查看pod详细信息，只有工作节点创建pod，master不会创建.
kubectl get pods -o wide
```

输出：

```sh
[root@k8s-master-50 controller]# kubectl apply -f daemonsetdemo.yml 
daemonset.apps/daemonsetdemo created
[root@k8s-master-50 controller]# kubectl  get pods -o wide
NAME                  READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
daemonsetdemo-2mk64   1/1     Running   0          9s    10.81.92.119    k8s-node51   <none>           <none>
daemonsetdemo-wnts2   1/1     Running   0          9s    10.81.237.255   k8s-node52   <none>           <none>
daemonsetdemo-zhjhk   1/1     Running   0          9s    10.81.149.254   k8s-node53   <none>           <none>
[root@k8s-master-50 controller]# 
# 可以发现，在每个工作的节点都，都创建了一个daemonsetdemo实例对象。
```

DaemonSet滚动更新

daemonset有两种更新策略：

- OnDelete：这是向后兼容性的默认更新策略。使用 OnDelete 更新策略，在更新DaemonSet模板 后，只有在手动删除旧的DaemonSet pod时才会创建新的DaemonSet pod。这与Kubernetes  1.5或更早版本中DaemonSet的行为相同。
- RollingUpdate：使用 RollingUpdate 更新策略，在更新DaemonSet模板后，旧的DaemonSet  pod将被终止，并且将以受控方式自动创建新的DaemonSet pod。





### Job

适用场景：

- 一次性执行任务，类似Linux中的job
- 应用场景：如离线数据处理，视频解码等业务

使用镜像：

```sh
docker pull perl:slim
```

controller/jobdemo.yml

```sh
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
        - name: pi
          image: perl:slim
          command: ["perl","-Mbignum=bpi","-wle","print bpi(6000)"]
      restartPolicy: Never
  backoffLimit: 4
```

backoffLimit说明:

```sh
.spec.backoffLimit用于设置Job的容错次数，默认值为6。当Job运行的Pod失败次数到达.spec.backoffLimit次时，Job Controller不再新建Pod，直接停止运行这个Job，将其运行结果标记为Failure。另外，Pod运行失败后再次运行的时间间隔呈递增状态，例如10s，20s，
40s。。。
```

操作命令:

```sh
# 运行job
kubectl apply -f jobdemo.yml

# 查看pod日志
kubectl get pods
kubectl logs -f pi-72bdt 

#删除job
kubectl delete -f jobdemo.yml
```

日志:

```sh
[root@k8s-master-50 controller]# kubectl apply -f jobdemo.yml
job.batch/pi created
[root@k8s-master-50 controller]# kubectl get pods
NAME       READY   STATUS      RESTARTS   AGE
pi-72bdt   0/1     Completed   0          65s
[root@k8s-master-50 controller]# kubectl logs -f pi-72bdt 
3.14159265358979323846264338327950288419716939937510582097494459230781640628620899862803482534211706798214808651328230664709384460955058223172535940812848111745028410270193852110555964462294895493038196442881097566593344612847564823378678316527120190914564856692346034861045432664821339360726024914127372458700660631558817488152092096282925409171536436789259036001133053054882046652138414695194151160943305727036575959195309218611738193261179310511854807446237996274956735188575272489122793818301194912983367336244065664308602139494639522473719070217986094370277053921717629317675238467481846766940513200056812714526356082778577134275778960917363717872146844090122495343014654958537105079227968925892354201995611212902196086403441815981362977477130996051870721134999999837297804995105973173281609631859502445945534690830264252230825334468503526193118817101000313783875288658753320838142061717766914730359825349042875546873115956286388235378759375195778185778053217122680661300192787661119590921642019893809525720106548586327886593615338182796823030195203530185296899577362259941389124972177528347913151557485724245415069595082953311686172785588907509838175463746493931925506040092770167113900984882401285836160356370766010471018194295559619894676783744944825537977472684710404753464620804668425906949129331367702898915210475216205696602405803815019351125338243003558764024749647326391419927260426992279678235478163600934172164121992458631503028618297455570674983850549458858692699569092721079750930295532116534498720275596023648066549911988183479775356636980742654252786255181841757467289097777279380008164706001614524919217321721477235014144197356854816136115735255213347574184946843852332390739414333454776241686251898356948556209921922218427255025425688767179049460165346680498862723279178608578438382796797668145410095388378636095068006422512520511739298489608412848862694560424196528502221066118630674427862203919494504712371378696095636437191728746776465757396241389086583264599581339047802759009946576407895126946839835259570982582262052248940772671947826848260147699090264013639443745530506820349625245174939965143142980919065925093722169646151570985838741059788595977297549893016175392846813826868386894277415599185592524595395943104997252468084598727364469584865383673622262609912460805124388439045124413654976278079771569143599770012961608944169486855584840635342207222582848864815845602850601684273945226746767889525213852254995466672782398645659611635488623057745649803559363456817432411251507606947945109659609402522887971089314566913686722874894056010150330861792868092087476091782493858900971490967598526136554978189312978482168299894872265880485756401427047755513237964145152374623436454285844479526586782105114135473573952311342716610213596953623144295248493718711014576540359027993440374200731057853906219838744780847848968332144571386875194350643021845319104848100537061468067491927819119793995206141966342875444064374512371819217999839101591956181467514269123974894090718649423196156794520809514655022523160388193014209376213785595663893778708303906979207734672218256259966150142150306803844773454920260541466592520149744285073251866600213243408819071048633173464965145390579626856100550810665879699816357473638405257145910289706414011097120628043903975951567715770042033786993600723055876317635942187312514712053292819182618612586732157919841484882916447060957527069572209175671167229109816909152801735067127485832228718352093539657251210835791513698820914442100675103346711031412671113699086585163983150197016515116851714376576183515565088490998985998238734552833163550764791853589322618548963213293308985706420467525907091548141654985946163718027098199430992448895757128289059232332609729971208443357326548938239119325974636673058360414281388303203824903758985243744170291327656180937734440307074692112019130203303801976211011004492932151608424448596376698389522868478312355265821314495768572624334418930396864262434107732269780280731891544110104468232527162010526522721116603966655730925471105578537634668206531098965269186205647693125705863566201855810072936065987648611791045334885034611365768675324944166803962657978771855608455296541266540853061434443185867697514566140680070023787765913440171274947042056223053899456131407112700040785473326993908145466464588079727082668306343285878569830523580893306575740679545716377525420211495576158140025012622859413021647155097925923099079654737612551765675135751782966645477917450112996148903046399471329621073404375189573596145890193897131117904297828564750320319869151402870808599048010941214722131794764777262241425485454033215718530614228813758504306332175182979866223717215916077166925474873898665494945011465406284336639379003976926567214638530673609657120918076383271664162748888007869256029022847210403172118608204190004229661711963779213375751149595015660496318629472654736425230817703675159067350235072835405670403867435136222247715891504953098444893330963408780769325993978054193414473774418426312986080998886874132604721569516239658645730216315981931951673538129741677294786724229246543668009806769282382806899640048243540370141631496589794092432378969070697794223625082216889573837986230015937764716512289357860158816175578297352334460428151262720373431465319777741603199066554187639792933441952154134189948544473456738316249934191318148092777710386387734317720754565453220777092120190516609628049092636019759882816133231666365286193266863360627356763035447762803504507772355471058595487027908143562401451718062464362679456127531813407833033625423278394497538243720583531147711992606381334677687969597030983391307710987040859133746414428227726346594704745878477872019277152807317679077071572134447306057007334924369311383504931631284042512192565179806941135280131470130478164378851852909285452011658393419656213491434159562586586557055269049652098580338507224264829397285847831630577775606888764462482468579260395352773480304802900587607582510474709164396136267604492562742042083208566119062545433721315359584506877246
[root@k8s-master-50 controller]# kubectl delete -f jobdemo.yml
job.batch "pi" deleted
```



### StatefulSet

在kubernetes系统中，Pod的管理对象RC，Deployment，DaemonSet和Job都面向无状态的服务，但 现实中有很多服务时有状态的，比如一些集群服务，例如mysql集群，集群一般都会有这四个特点：

1. 每个节点都是有固定的身份ID，集群中的成员可以相互发现并通信 
2.  集群的规模是比较固定的，集群规模不能随意变动 
3.  集群中的每个节点都是有状态的，通常会持久化数据到永久存储中
4. 如果磁盘损坏，则集群里的某个节点无法正常运行，集群功能受损

如果你通过RC或Deployment控制Pod副本数量来实现上述有状态的集群，就会发现第一点是无法满足 的，因为Pod名称和ip是随机产生的，并且各Pod中的共享存储中的数据不能都动，因此StatefulSet在 这种情况下就派上用场了，那么StatefulSet具有以下特性：

- StatefulSet里的每个Pod都有稳定，唯一的网络标识，可以用来发现集群内的其它成员，假设， StatefulSet的名称为lagou，那么第1个Pod叫lagou-0，第2个叫lagou-1，以此类推。
- StatefulSet控制的Pod副本的启停顺序是受控的，操作第N个Pod时，前N-1个Pod已经是运行且准 备状态。
- StatefulSet里的Pod采用稳定的持久化存储卷，通过PV或PVC来实现，删除Pod时默认不会删除与 StatefulSet相关的存储卷（为了保证数据的安全）

StatefulSet除了要与PV卷捆绑使用以存储Pod的状态数据，还要与Headless，Service配合使用，每个 StatefulSet定义中都要生命它属于哪个Handless Service，Handless Service与普通Service的关键区别 在于，它没有Cluster IP。



## service

简介:

通过以前的学习，我们已经能够通过Deployment来创建一组Pod来提供具有高可用性的服务。虽然每 个Pod都会分配一个单独的Pod IP，然而却存在如下两问题：

- pod的IP仅集群内可见的虚拟IP，外部无法访问。
- podIP会随着pod的销毁而消失，当Deployment对pod进行动态伸缩时，pod IP可能随时随地都会变化，这样对于我们的访问这个服务带来了难度。
- service提供负载均衡的能力，但是在使用上有以下限制。提供4层负载均衡能力，而没有七层功能，但有时我们需要更多的匹配规则来转发服务，这点上4层负载均衡是不支持的。

因此，Kubernetes中的Service对象就是解决以上问题的实现服务发现核心关键。

Service 在 K8s 中有以下四种类型:

- ClusterIp：默认类型，自动分配一个仅 Cluster 内部可以访问的虚拟 IP
- NodePort：在 ClusterIP 基础上为 Service 在每台机器上绑定一个端口，这样就可以通过 :  NodePort 来访问该服务。
- LoadBalancer：在 NodePort 的基础上，借助 cloud provider 创建一个外部负载均衡器，并将请 求转发 到NodePort。是付费服务，而且价格不菲。
- ExternalName：把集群外部的服务引入到集群内部来，在集群内部直接使用。没有任何类型代理 被创建， 这只有 kubernetes 1.7 或更高版本的 kube-dns 才支持 

详解4 种 Service 类型:

Services 和 Pods

Kubernetes的Pods是有生命周期的。他们可以被创建，而且销毁不会再启动。如果您使用 Deployment来运行您的应用程序，则它可以动态创建和销毁 Pod。

 一个Kubernetes的Service是一种抽象，它定义了一组Pods的逻辑集合和一个用于访问它们的策略 - 有 的时候被称之为微服务。一个Service的目标Pod集合通常是由Label Selector 来决定的 

举个例子，想象一个处理图片的后端运行了三个副本。这些副本都是可以替代的 - 前端不关心它们使用 的是哪一个后端。尽管实际组成后端集合的Pod可能会变化，前端的客户端却不需要知道这个变化，也 不需要自己有一个列表来记录这些后端服务。Service抽象能让你达到这种解耦。 

不像 Pod 的 IP 地址，它实际路由到一个固定的目的地，Service 的 IP 实际上不能通过单个主机来进行 应答。相反，我们使用 iptables（Linux 中的数据包处理逻辑）来定义一个虚拟IP地址（VIP），它可以 根据需要透明地进行重定向。当客户端连接到 VIP 时，它们的流量会自动地传输到一个合适的  Endpoint。环境变量和 DNS，实际上会根据 Service 的 VIP 和端口来进行填充。 kube-proxy支持三种代理模式: 用户空间，iptables和IPVS；它们各自的操作略有不同。

- Userspace代理模式

Client Pod要访问Server Pod时,它先将请求发给本机内核空间中的service规则，由它再将请求,转给监 听在指定套接字上的kube-proxy，kube-proxy处理完请求，并分发请求到指定Server Pod后,再将请求 递交给内核空间中的service,由service将请求转给指定的Server Pod。由于其需要来回在用户空间和内 核空间交互通信，因此效率很差 。

 当一个客户端连接到一个 VIP，iptables 规则开始起作用，它会重定向该数据包到 Service代理 的端 口。Service代理 选择一个 backend，并将客户端的流量代理到 backend 上。 

这意味着 Service 的所有者能够选择任何他们想使用的端口，而不存在冲突的风险。客户端可以简单地 连接到一个 IP 和端口，而不需要知道实际访问了哪些 Pod。

- iptables代理模式

当一个客户端连接到一个 VIP，iptables 规则开始起作用。一个 backend 会被选择（或者根据会话亲和性，或者随机），数据包被重定向到这个 backend。不像 userspace 代理，数据包从来不拷贝到用户 空间，kube-proxy 不是必须为该 VIP 工作而运行，并且客户端 IP 是不可更改的。当流量打到 Node 的 端口上，或通过负载均衡器，会执行相同的基本流程，但是在那些案例中客户端 IP 是可以更改的。

- IPVS代理模式

在大规模集群（例如10,000个服务）中，iptables 操作会显着降低速度。IPVS 专为负载平衡而设计， 并基于内核内哈希表。因此，您可以通过基于 IPVS 的 kube-proxy 在大量服务中实现性能一致性。同 时，基于 IPVS 的 kube-proxy 具有更复杂的负载平衡算法（最小连接，局部性，加权，持久性）。 

在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。 kube-proxy 负责为 Service 实现了一 种 VIP（虚拟 IP）的形式，而不是 ExternalName 的形式。 在 Kubernetes v1.0 版本，代理完全在  userspace。在 Kubernetes v1.1 版本，新增了 iptables 代理，但并不是默认的运行模式。 从 Kubernetes v1.2 起， 默认就是 iptables 代理。 在 Kubernetes v1.8.0-beta.0 中，添加了 ipvs 代理在 Kubernetes 1.14 版本开始默认使用 ipvs 代理。在 Kubernetes v1.0 版本， Service 是 “4层”（TCP/UDP over IP）概念。 在  Kubernetes v1.1 版本，新增了Ingress API（beta 版），用来表示 “7层”（HTTP）服务 。  这种模式，kube-proxy 会监视 Kubernetes Service 对象和 Endpoints ，调用 netlink 接口以相应地创 建 ipvs 规则并定期与 Kubernetes Service 对象和 Endpoints 对象同步 ipvs 规则，以确保 ipvs 状态与期 望一 致。访问服务时，流量将被重定向到其中一个后端 Pod与 iptables 类似，ipvs 于 netﬁlter 的 hook 功 能，但使用哈希表作为底层数据结构并在内核空间中工作。这意味着 ipvs 可以更快地重定向流量，并 且在同步代理规则时具有更好的性能。此外，ipvs 为负载均衡算法提供了更多选项。 



### ClusterIP

类型为ClusterIP的service，这个service有一个Cluster-IP，其实就一个VIP。具体实现原理依靠 kubeproxy组件，通过iptables或是ipvs实现。

  clusterIP 主要在每个 node 节点使用 iptables，将发向 clusterIP 对应端口的数据，转发到 kube-proxy  中。然 后 kube-proxy 自己内部实现有负载均衡的方法，并可以查询到这个 service 下对应 pod 的地址和端 口，进而把 数据转发给对应的 pod 的地址和端口  

 这种类型的service 只能在集群内访问。

使用镜像

```sh
docker pull tomcat:9.0.20-jre8-alpine
```

部署service

service/clusteripdemo.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: clusteripdemo
  labels:
    app: clusteripdemo
spec:
  replicas: 3
  template:
    metadata:
      name: clusteripdemo
      labels:
        app: clusteripdemo
  spec:
    containers:
      - name: clusteripdemo
        image: tomcat:9.0.20-jre8-alpine
        imagePullPolicy: ifNotPresent
        ports:
          - containerPort: 8080
    restartPolicy: Always
  selector:
    matchLabels:
      app: clusteripdemo
---
apiVersion: v1
kind: Service
metadata:
  name: clusterip-svc
spec:
  selector:
    app: clusteripdemo
  ports:
    - port: 8080
      targetPort: 8080
  type: ClusterIP

```

运行service

```sh
# 运行服务
kubectl apply -f clusteripdemo.yaml

# 查看服务
kubectl get svc

# 访问服务
curl curl 10.1.165.192:8080

# 删除服务
kubectl delete -f clusteripdemo.yaml
```

运行日志:

```sh
[root@k8s-master-50 service]# kubectl apply -f clusteripdemo.yaml
deployment.apps/clusteripdemo created
service/clusterip-svc unchanged
[root@k8s-master-50 service]# kubectl get svc
NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
clusterip-svc   ClusterIP   10.1.165.192   <none>        8080/TCP   32s
kubernetes      ClusterIP   10.1.0.1       <none>        443/TCP    14d
[root@k8s-master-50 service]# curl 10.1.165.192:8080



<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        <div id="wrapper">
            ......
        </div>
    </body>

</html>
[root@k8s-master-50 service]# kubectl delete -f clusteripdemo.yaml
deployment.apps "clusteripdemo" deleted
service "clusterip-svc" deleted
[root@k8s-master-50 service]# 
```



### NodePort

我们的场景不全是集群内访问，也需要集群外业务访问。那么ClusterIP就满足不了了。NodePort当然 是其中的一种实现方案。nodePort 的原理在于在 node 上开了一个端口，将向该端口的流量导入到  kube-proxy，然后由 kube-proxy 进一步到给对应的 pod 。

镜像：

```sh
docker pull tomcat:9.0.20-jre8-alpine
```

部署服务

service/nodeportdemo.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodeportdemo
  labels:
    app: nodeportdemo
spec:
  replicas: 3
  template:
    metadata:
      name: nodeportdemo
      labels:
        app: nodeportdemo
    spec:
      containers:
        - name: nodeportdemo
          image: tomcat:9.0.20-jre8-alpine
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
  selector:
    matchLabels:
      app: nodeportdemo
---
apiVersion: v1
kind: Service
metadata:
  name: nodeport-svc
spec:
  selector:
    app: nodeportdemo
  ports:
    - port: 8080
      targetPort: 8080
      nodePort: 30088
  type: NodePort
```

运行service

```sh
# 运行服务
kubectl apply -f nodeportdemo.yml

# 查看服务
kubectl get svc

# 访问服务
curl 10.1.103.246:8080

# 也可以通过外网访问
http://192.168.5.50:30088/
```

运行日志:

```sh
[root@k8s-master-50 service]# kubectl apply -f nodeportdemo.yml
deployment.apps/nodeportdemo created
service/nodeport-svc created
[root@k8s-master-50 service]# kubectl get svc
NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes     ClusterIP   10.1.0.1       <none>        443/TCP          15d
nodeport-svc   NodePort    10.1.103.246   <none>        8080:30088/TCP   7s
[root@k8s-master-50 service]# curl 10.1.103.246:8080



<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        <div id="wrapper">
            ......
        </div>
    </body>

</html>
[root@k8s-master-50 service]# 
```



### LoadBalancer

LoadBalancer类型的service 是可以实现集群外部访问服务的另外一种解决方案。不过并不是所有的 k8s集群都会支持，大多是在公有云托管集群中会支持该类型。负载均衡器是异步创建的，关于被提供 的负载均衡器的信息将会通过Service的status.loadBalancer字段被发布出去。

创建 LoadBalancer service 的yaml 如下：

```yaml
apiVersion: v1
kind: Service
metadata:
 name: service-nullnull
spec:
 ports:
 - port: 3000
   protocol: TCP
   targetPort: 443
   nodePort: 30080
 selector:
   run: pod-nullnull
 type: LoadBalancer
```



### ExternalName

类型为 ExternalName 的service将服务映射到 DNS 名称，而不是典型的选择器，例如my-service或者 cassandra。您可以使用spec.externalName参数指定这些服务。 这种类型的 Service 通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容( 例 如： hub.lagouedu.com )。ExternalName Service 是 Service 的特例，它没有 selector，也没有定义任何 的端口和 Endpoint。相反的，对于运行在集群外部的服务，它通过返回该外部服务的cname(别名)这种方式来提 供服务 

创建 ExternalName 类型的服务的 yaml 如下：

```yaml
kind: Service
apiVersion: v1
metadata:
 name: service-nullnull
spec:
 ports:
 - port: 3000
   protocol: TCP
   targetPort: 443
 type: ExternalName
 externalName: www.nullnull.com
```



## ingress网络

什么是Ingress？

K8s集群对外暴露服务的方式目前只有三种：loadblancer、nodeport、ingress。前两种熟悉起来比较 快，而且使用起来也比较方便，前面课程已经讲解过两种技术。在此就不进行介绍了。 

下面详细讲解下ingress这个服务，ingress由两部分组成：ingress controller和ingress服务。 

其中ingress controller目前主要有两种：基于nginx服务的ingress controller和基于traeﬁk的ingress  controller。 

而其中traeﬁk的ingress controller，目前支持http和https协议。由于对nginx比较熟悉，而且需要使用 TCP负载，所以在此我们选择的是基于nginx服务的ingress controller。  

在kubernetes集群中，我们知道service和pod的ip仅在集群内部访问。如果外部应用要访问集群内的服 务，集群外部的请求需要通过负载均衡转发到service在Node上暴露的NodePort上，然后再由kubeproxy组件将其转发给相关的pod。

 而Ingress就是为进入集群的请求提供路由规则的集合，通俗点就是提供外部访问集群的入口，将外部 的HTTP或者HTTPS请求转发到集群内部service上。

**Ingress-nginx组成**

Ingress-nginx一般由三个组件组成：

- 反向代理负载均衡器：通常以service的port方式运行，接收并按照ingress定义的规则进行转发， 常用的有nginx，Haproxy，Traeﬁk等，本次实验中使用的就是nginx。
- Ingress Controller：监听APIServer，根据用户编写的ingress规则（编写ingress的yaml文件）， 动态地去更改nginx服务的配置文件，并且reload重载使其生效，此过程是自动化的（通过lua脚本 来实现）。
- Ingress：将nginx的配置抽象成一个Ingress对象，当用户每添加一个新的服务，只需要编写一个 新的ingress的yaml文件即可。

**Ingress-nginx的工作原理**

- ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化。然后读取 它，按照自定义的规则，规则就是写明了那个域名对应哪个service，生成一段nginx配置。
- 再写到nginx-ingress-controller的pod里，这个Ingress controller的pod里运行着一个Nginx服 务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中。然后reload一下使配置生效，以此达到分配和动态更新问题。

官网地址  基于nginx服务的ingress controller根据不同的开发公司，又分为k8s社区的ingres-nginx和nginx 公司的nginx-ingress。 



相关的地址信息：

基于nginx服务的ingress controller根据不同的开发公司，又分为k8s社区的ingres-nginx和nginx 公司的nginx-ingress。 

```sh
Ingress-Nginx github 地址：
https://github.com/kubernetes/ingress-nginx


Ingress-Nginx 官方网站：
https://kubernetes.github.io/ingress-nginx/
```

根据github上的活跃度和关注人数，选择的是k8s社区的ingres-nginx。

```sh
#下载ingrees-controller

# 打开github官网：选择某一个具体版本后进入deploy/static/目录中，复制mandatory.yaml文件内容。
https://github.com/kubernetes/ingress-nginx/tree/nginx0.30.0/deploy/static/mandatory.yaml
```

操作

```sh
# 1. 下载ingress服务
https://github.com/kubernetes/ingress-nginx/blob/nginx0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml
```



下载镜像pull到k8s集群中的各节点上

```sh
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.30.0

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.30.0 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0

docker rmi -f registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.30.0
```



ingress与ingress-controller

 要理解ingress，需要区分两个概念，ingress和ingress-controller：

- ingress对象： 指的是k8s中的一个api对象，一般用yaml配置。作用是定义请求如何转发到service的规则，可以 理解为配置模板。
- ingress-controller： 具体实现反向代理及负载均衡的程序，对ingress定义的规则进行解析，根据配置的规则来实现请 求转发。

简单来说，ingress-controller才是负责具体转发的组件，通过各种方式将它暴露在集群入口，外部对集 群的请求流量会先到ingress-controller，而ingress对象是用来告诉ingress-controller该如何转发请 求，比如哪些域名哪些path要转发到哪些服务等等。

ingress-controller

ingress-controller并不是k8s自带的组件，实际上ingress-controller只是一个统称，用户可以选择不同 的ingress-controller实现，目前，由k8s维护的ingress-controller只有google云的GCE与ingress-nginx 两个，其他还有很多第三方维护的ingress-controller，具体可以参考官方文档。但是不管哪一种 ingress-controller，实现的机制都大同小异，只是在具体配置上有差异。一般来说，ingress-controller 的形式都是一个pod，里面跑着daemon程序和反向代理程序。daemon负责不断监控集群的变化，根 据ingress对象生成配置并应用新配置到反向代理，比如nginx-ingress就是动态生成nginx配置，动态更 新upstream，并在需要的时候reload程序应用新配置。

ingress

ingress是一个API对象，和其他对象一样，通过yaml文件来配置。ingress通过http或https暴露集群内 部service，给service提供外部URL、负载均衡、SSL/TLS能力以及基于host的方向代理。ingress要依 靠ingress-controller来具体实现以上功能。

### ingress网络实验一

使用镜像

```sh
docker pull tomcat:9.0.20-jre8-alpine
docker pull quay.io/kubernetes-ingress-controller/nginx-ingresscontroller:0.30.0
```

编辑ingress-controller配制文件

```sh
# 将https://github.com/kubernetes/ingress-nginx/tree/nginx0.30.0/deploy/static/mandatory.yaml内容下载本地

# 在mandatory.yaml文件的Deployment资源中增加属性sepc.template.sepc.hostNetWork
hostNetwork: true
# hostNetwork网络，这是一种直接定义pod的网络的方式，如果pod使用hostNetwork: true配制网络，那么pod运行的应用程序可以直接使用node节点的端口。
```

以下为完整的内容

mandatory.yaml

```yaml
# 创建一个命名空间
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
  # 定义资源的标签，以key: value的形式出现
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
# 配制信息，一些键值对的信息，主要用作配制文件。
kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---

# 证书
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
# 基于角色的管理,当前分配的是一个集群的角色
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses/status
    verbs:
      - update

---
# 给用户授以某种角色。
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---
# ingress的控制器，控制器的类型为deployment类型
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
      annotations:
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
    spec:
      # 在部署到k8s的集群时，使用的是节点的网络。
      hostNetwork: true
      # wait up to five minutes for the drain of connections
      terminationGracePeriodSeconds: 300
      serviceAccountName: nginx-ingress-serviceaccount
      nodeSelector:
        kubernetes.io/os: linux
      containers:
        - name: nginx-ingress-controller
          # 由于国内网络的问题，需要需先在各节点装载此镜像。
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
            # www-data -> 101
            runAsUser: 101
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: https
              containerPort: 443
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          lifecycle:
            preStop:
              exec:
                command:
                  - /wait-shutdown

---
# 配制的内存和CPU的使用情况
apiVersion: v1
kind: LimitRange
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  limits:
  - min:
      memory: 90Mi
      cpu: 100m
    type: Container
```

ingress服务

直接下载`https://github.com/kubernetes/ingress-nginx/blob/nginx0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml`内容即可

service-nodeport.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  type: NodePort
  ports:
    - name: http
      port: 80
      targetPort: 80
      protocol: TCP
    - name: https
      port: 443
      targetPort: 443
      protocol: TCP
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---


```

部署一个tomcat服务

tomcat-service.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tomcat-deployment
  labels:
    app: tomcat-deployment
spec:
  replicas: 1
  template:
    metadata:
      name: tomcat-deployment
      labels:
        app: tomcat-deployment
    spec:
      containers:
        - name: tomcat-deployment
          image: tomcat:9.0.20-jre8-alpine
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
      restartPolicy: Always
  selector:
    matchLabels:
      app: tomcat-deployment

---
apiVersion: v1
kind: Service
metadata:
  name: tomcat-svc
spec:
  # 使用pod的标签，即spec.template.metadata.labels.app
  selector:
    app: tomcat-deployment
  ports:
    - port: 8080
      # 容器的端口号
      targetPort: 8080
      #  nodePort的端口号
      nodePort: 30088
  type: NodePort
```

ingress的规则文件

ingress-tomcat.yml

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx-ingress-test
spec:
  backend:
    # 对哪个服务启作用，就配制哪个服务的名称
    serviceName: tomcat-svc
    # 服务的端口号,指定的是容器的端口号，即tomcat容器的端口号,
    # 即tomcat-service.yml文件中tomcat-deployment的spec.template.spec.containers.ports.containerPort
    servicePort: 8080

```



在以上文件都编辑完成后，上传至服务器中，进行运行

```sh
mkdir -p /data/k8s/ingress
cd /data/k8s/ingress
# 将所有文件都上传到此目录中

# 此命令将会把目录下的所有文件都进行执行
kubectl apply -f .


# 查看ingress部署到了哪一个节点上
kubectl  get pods -A
kubectl get pod -n ingress-nginx 
# 此就可以看到具体部署到了哪一个节点上
kubectl get pod -n ingress-nginx -o wide
# 查看对外的端口信息
kubectl  get svc -n ingress-nginx 

# 使用端口在浏览器中访问
192.168.5.50:30012
```

输入信息:

```sh
[root@k8s-master-50 ingress]# kubectl apply -f .
ingress.extensions/nginx-ingress-test created
namespace/ingress-nginx created
configmap/nginx-configuration created
configmap/tcp-services created
configmap/udp-services created
serviceaccount/nginx-ingress-serviceaccount created
clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole created
role.rbac.authorization.k8s.io/nginx-ingress-role created
rolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding created
clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding created
deployment.apps/nginx-ingress-controller created
limitrange/ingress-nginx created
service/ingress-nginx created
deployment.apps/tomcat-deployment created
service/tomcat-svc created
[root@k8s-master-50 ingress]# 

[root@k8s-master-50 ingress]# kubectl  get pods -A
NAMESPACE       NAME                                        READY   STATUS    RESTARTS   AGE
default         tomcat-deployment-5ff64fdb6f-qvfzk          1/1     Running   0          98s
ingress-nginx   nginx-ingress-controller-6ffc8fdf96-mnlw6   1/1     Running   0          98s
kube-system     calico-kube-controllers-6b94766748-hfbjs    1/1     Running   22         29d
kube-system     calico-node-27kqm                           1/1     Running   31         29d
kube-system     calico-node-8w2pp                           1/1     Running   32         29d
kube-system     calico-node-brwl7                           1/1     Running   27         29d
kube-system     calico-node-s6d49                           1/1     Running   32         29d
kube-system     coredns-6955765f44-rq92j                    1/1     Running   22         29d
kube-system     coredns-6955765f44-wmr68                    1/1     Running   22         29d
kube-system     etcd-k8s-master-50                          1/1     Running   22         29d
kube-system     kube-apiserver-k8s-master-50                1/1     Running   28         29d
kube-system     kube-controller-manager-k8s-master-50       1/1     Running   22         29d
kube-system     kube-proxy-4cbsx                            1/1     Running   22         29d
kube-system     kube-proxy-l6k6x                            1/1     Running   22         29d
kube-system     kube-proxy-pq57w                            1/1     Running   23         29d
kube-system     kube-proxy-tlb5j                            1/1     Running   22         29d
kube-system     kube-scheduler-k8s-master-50                1/1     Running   22         29d
[root@k8s-master-50 ingress]# kubectl get pod -n ingress-nginx
NAME                                        READY   STATUS    RESTARTS   AGE
nginx-ingress-controller-6ffc8fdf96-mnlw6   1/1     Running   0          2m9s
[root@k8s-master-50 ingress]# kubectl get pod -n ingress-nginx -o wide
NAME                                        READY   STATUS    RESTARTS   AGE     IP         NODE         NOMINATED NODE   READINESS GATES
nginx-ingress-controller-6ffc8fdf96-mnlw6   1/1     Running   0          2m27s   10.0.3.5   k8s-node51   <none>           <none>
[root@k8s-master-50 ingress]# kubectl  get svc -n ingress-nginx 
NAME            TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx   NodePort   10.1.245.56   <none>        80:30012/TCP,443:32198/TCP   3m9s
[root@k8s-master-50 ingress]# 
```



### ingress网络实验二

上边案例的部署方式只能通过ingress-controller部署的节点访问。集群内其他节点无法访问ingress规 则。本章节通过修改mandatory.yaml文件的控制类类型，让集群内每一个节点都可以正常访问ingress 规则。

**ingress-controller**

mandatory.yaml

```yaml
# 修改mandatory.yaml配置文件
#1.将 Deployment 类型控制器修改为：DaemonSet
#2.属性：replicas: 1  # 删除这行
```

**service-nodeport固定端口**

service-nodeport.yml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  type: NodePort
  ports:
    - name: http
      port: 80
      targetPort: 80
      nodePort: 31188
      protocol: TCP
    - name: https
      port: 443
      targetPort: 443
      nodePort: 31443
      protocol: TCP
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---

```

**域名访问ingress规则**

ingress-tomcat.yml

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx-ingress-test
spec:
  rules:
    - host: ingress-tomcat.nullnull.com
      http:
        paths:
          - path: /
            backend:
              serviceName: tomcat-svc
              servicePort: 8080
```

**修改宿主机hosts文件**

C:\Windows\System32\drivers\etc\hosts

```tex
192.168.5.30 ingress-tomcat.nullnull.com
```



**部署服务**

将文件上传至k8s集群的管理节点

```sh
kubectl apply -f .
```

输出：

```sh
[root@k8s-master-50 ingress]# kubectl apply -f .
ingress.extensions/nginx-ingress-test created
namespace/ingress-nginx created
configmap/nginx-configuration created
configmap/tcp-services created
configmap/udp-services created
serviceaccount/nginx-ingress-serviceaccount created
clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole created
role.rbac.authorization.k8s.io/nginx-ingress-role created
rolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding created
clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding created
daemonset.apps/nginx-ingress-controller created
limitrange/ingress-nginx created
service/ingress-nginx created
deployment.apps/tomcat-deployment created
service/tomcat-svc created
[root@k8s-master-50 ingress]# 
```



**测试访问**

可通过浏览器直接访问

```sh
http://ingress-tomcat.nullnull.com:31188/
```

操作

```sh
# 查看ingress-nginx 命名空间下的pod

kubectl get pods -n ingress-nginx 

#进入ingress-nginx 的pod
kubectl exec -it nginx-ingress-controller-6rmf8 -n ingress-nginx sh


# 查看nginx反向代理域名ingress-tomcat.nullnull.com
cat nginx.conf

```



输出:

```sh
[root@k8s-master-50 ingress]# kubectl get pods -n ingress-nginx 
NAME                             READY   STATUS    RESTARTS   AGE
nginx-ingress-controller-6rmf8   1/1     Running   0          5m29s
nginx-ingress-controller-hj2bk   1/1     Running   0          5m29s
nginx-ingress-controller-kxbcv   1/1     Running   0          5m29s
[root@k8s-master-50 ingress]# kubectl exec -it nginx-ingress-controller-6rmf8 -n ingress-nginx sh
/etc/nginx $ cat nginx.conf
# Configuration checksum: 6103508086549277951

# setup custom paths that do not require root access
pid /tmp/nginx.pid;

daemon off;

worker_processes 2;

worker_rlimit_nofile 523264;

worker_shutdown_timeout 240s ;

events {
        multi_accept        on;
        worker_connections  16384;
        use                 epoll;
}

http {
......
        ## start server ingress-tomcat.nullnull.com
        server {
                server_name ingress-tomcat.nullnull.com ;

                listen 80  ;
                listen [::]:80  ;
                listen 443  ssl http2 ;
                listen [::]:443  ssl http2 ;

                set $proxy_upstream_name "-";

                ssl_certificate_by_lua_block {
                        certificate.call()
                }

                location / {

                        set $namespace      "default";
                        set $ingress_name   "nginx-ingress-test";
                        set $service_name   "tomcat-svc";
                        set $service_port   "8080";
                        set $location_path  "/";

                        rewrite_by_lua_block {
                                lua_ingress.rewrite({
                                        force_ssl_redirect = false,
                                        ssl_redirect = true,
                                        force_no_ssl_redirect = false,
                                        use_port_in_redirects = false,
                                })
                                balancer.rewrite()
                                plugins.run()
                        }

                        # be careful with `access_by_lua_block` and `satisfy any` directives as satisfy any
                        # will always succeed when there's `access_by_lua_block` that does not have any lua code doing `ngx.exit(ngx.DECLINED)`
                        # other authentication method such as basic auth or external auth useless - all requests will be allowed.
                        #access_by_lua_block {
                        #}

                        header_filter_by_lua_block {
                                lua_ingress.header()
                                plugins.run()
                        }

                        body_filter_by_lua_block {
                        }

                        log_by_lua_block {
                                balancer.log()

                                monitor.call()

                                plugins.run()
                        }

                        port_in_redirect off;

                        set $balancer_ewma_score -1;
                        set $proxy_upstream_name "default-tomcat-svc-8080";
                        set $proxy_host          $proxy_upstream_name;
                        set $pass_access_scheme  $scheme;

                        set $pass_server_port    $server_port;

                        set $best_http_host      $http_host;
                        set $pass_port           $pass_server_port;

                        set $proxy_alternative_upstream_name "";

                        client_max_body_size                    1m;

                        proxy_set_header Host                   $best_http_host;

                        # Pass the extracted client certificate to the backend

                        # Allow websocket connections
                        proxy_set_header                        Upgrade           $http_upgrade;

                        proxy_set_header                        Connection        $connection_upgrade;

                        proxy_set_header X-Request-ID           $req_id;
                        proxy_set_header X-Real-IP              $remote_addr;

                        proxy_set_header X-Forwarded-For        $remote_addr;

                        proxy_set_header X-Forwarded-Host       $best_http_host;
                        proxy_set_header X-Forwarded-Port       $pass_port;
                        proxy_set_header X-Forwarded-Proto      $pass_access_scheme;

                        proxy_set_header X-Scheme               $pass_access_scheme;

                        # Pass the original X-Forwarded-For
                        proxy_set_header X-Original-Forwarded-For $http_x_forwarded_for;

                        # mitigate HTTPoxy Vulnerability
                        # https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/
                        proxy_set_header Proxy                  "";

                        # Custom headers to proxied server

                        proxy_connect_timeout                   5s;
                        proxy_send_timeout                      60s;
                        proxy_read_timeout                      60s;

                        proxy_buffering                         off;
                        proxy_buffer_size                       4k;
                        proxy_buffers                           4 4k;

                        proxy_max_temp_file_size                1024m;

                        proxy_request_buffering                 on;
                        proxy_http_version                      1.1;

                        proxy_cookie_domain                     off;
                        proxy_cookie_path                       off;

                        # In case of errors try the next upstream server before returning an error
                        proxy_next_upstream                     error timeout;
                        proxy_next_upstream_timeout             0;
                        proxy_next_upstream_tries               3;

                        proxy_pass http://upstream_balancer;

                        proxy_redirect                          off;

                }

        }
        ## end server ingress-tomcat.nullnull.com
......
}
```





## Volume存储卷

### mariadb数据库案例

准备镜像 

```sh
docker pull mariadb:10.5.2
```

mariadb.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mariadb-deploy
  labels:
    app: mariadb-deploy
spec:
  replicas: 1
  template:
    metadata:
      name: mariadb-deploy
      labels:
        app: mariadb-deploy
    spec:
      containers:
        - name: mariadb-deploy
          image: mariadb:10.5.2
          imagePullPolicy: IfNotPresent
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: admin # 这是mysql的root用户的密码
            - name: TZ
              value: Asia/Shanghai
          args:
            - "--character-set-server=utf8mb4"
            - "--collation-server=utf8mb4_unicode_ci"
          ports:
            - containerPort: 3306
  selector:
    matchLabels:
      app: mariadb-deploy

---
apiVersion: v1
kind: Service
metadata:
  name: mariadb-svc
spec:
  selector:
    app: mariadb-deploy
  ports:
    - port: 3306
      targetPort: 3306
      nodePort: 30036
  type: NodePort
```



**在服务器上执行**

```sh
kubectl apply -f .
```

输出:

```sh
[root@k8s-master-50 mariadb]# kubectl apply -f .
deployment.apps/mariadb-deploy created
service/mariadb-svc created
```



客户端测试连接

```sh
ip: 192.168.5.50
port: 30036
username: root
password: admin
```

删除服务

```sh
kubectl delete -f .
```



### secret

Secret 解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或 者 Pod Spec中。Secret 可以以 Volume 或者环境变量的方式使用 。

Secret 有三种类型： 

- Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod  的/run/secrets/kubernetes.io/serviceaccount 目录中。
- Opaque ：base64编码格式的Secret，用来存储密码、密钥等
- kubernetes.io/dockerconfigjson ：用来存储私有 docker registry 的认证信息



**Service Account**

Service Account简称sa， Service Account 用来访问 Kubernetes API，由 Kubernetes 自动创建，并 且会自动挂载到Pod的`/run/secrets/kubernetes.io/serviceaccount`目录中。

```sh
# 查询命名空间为kube-system的pod信息
kubectl get pod -n kube-system

# 进入pod
kubectl exec -it kube-proxy-4cbsx -n kube-system sh

cd /run/secrets/kubernetes.io/serviceaccount

cat ca.crt
cat namespace
cat token
```

输出：

```sh
[root@k8s-master-50 mariadb]# kubectl get pod -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6b94766748-hfbjs   1/1     Running   24         34d
calico-node-27kqm                          1/1     Running   33         34d
calico-node-8w2pp                          1/1     Running   34         34d
calico-node-brwl7                          1/1     Running   29         34d
calico-node-s6d49                          1/1     Running   34         34d
coredns-6955765f44-rq92j                   1/1     Running   24         34d
coredns-6955765f44-wmr68                   1/1     Running   24         34d
etcd-k8s-master-50                         1/1     Running   24         34d
kube-apiserver-k8s-master-50               1/1     Running   31         34d
kube-controller-manager-k8s-master-50      1/1     Running   24         34d
kube-proxy-4cbsx                           1/1     Running   24         34d
kube-proxy-l6k6x                           1/1     Running   24         34d
kube-proxy-pq57w                           1/1     Running   25         34d
kube-proxy-tlb5j                           1/1     Running   24         34d
kube-scheduler-k8s-master-50               1/1     Running   24         34d
[root@k8s-master-50 mariadb]# kubectl exec -it kube-proxy-4cbsx -n kube-system sh
# cd /run/secrets/kubernetes.io/serviceaccount
# ls
ca.crt  namespace  token
# cat ca.crt
-----BEGIN CERTIFICATE-----
MIICyDCCAbCgAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl
cm5ldGVzMB4XDTIzMTIyNzAzMDE1NloXDTMzMTIyNDAzMDE1NlowFTETMBEGA1UE
AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAL2k
CjqRiPA6Oukt4FISgmxPqJzNiyhJYShoa73i8JnwX+/yXFAHbovSJ8uGVrBNLs9/
/XuxXYSGfe5fR8Vi1utdI9pCn0uZ75X4Y3BpPEzaSHi5BE4xykTrlntgjtz9BD5v
wYDmIf29V3s175zKU+MinBtGjzVot1fLqA/OL9d2OTObz2u/RKzCZ1bWKKgp4/iz
cuRm5yAwk7wPpvAOUEcq0spjCKcHLiBv153HSOd1CTL2VCTYO8MwfPu9e2Ck0jbK
U2g9N8Vlvy8F9evvIUcN0fjjH6mFAwgiWgM1DmLl8h2cHEoXABXeyrUkjZXts6fu
79/D+sIfkvuQxK94tX8CAwEAAaMjMCEwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB
/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBABTCdq8C2s3EXDAuTZqy0Klt6IY4
9b1b1Ra9VU6WtBwNQ0x9hMv7k33Y0F9YfQf2XQbfWFkBsrcnGPgduSquFyOqDMma
xb8mEnU55OPaHHcEqQ/ySJDjsgS2TPcF1yCeQoyZhfpilWqLfl5n08+Zh0rly3b+
4HVsVQtOsJnzPoe5m2W43Td5BpVMaYNX/D12Xo00lwBZEpw8b1z2d1j7/yOBQ0s7
ATRuR4Q0hUSAOOVyI0XHLaCrNmoGNUjaiN7OK7o1ncxWYpVc9D5+fSV17YQMU6Mw
Wr5yAjRTGLbvqCMvrcUUIBumOmtC9FreAqhNTAdmphGGJqHYsmpLens/UjY=
-----END CERTIFICATE-----
# cat namespace
kube-system# 
# cat token
eyJhbGciOiJSUzI1NiIsImtpZCI6IkZEbm1HZUJUZTNxS2Z1YXFMQU1hT2NlT0htVGlDV01WWWpla1ctbGZUT2MifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlLXByb3h5LXRva2VuLWg1YndkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Imt1YmUtcHJveHkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI4YzRhZjY4ZS05YWU0LTQyMjQtODc2Yi0wZGE1NGY2Mzc2MzkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06a3ViZS1wcm94eSJ9.OyM_YT40BlPRSL5vc87F5X-7Bbr7urstsDn-UxVbtdB9AsTStagtOtgNHmIQSftIzl7CcuJp7d_Fo0ujai396yY5vxj6u0q2ywRqsfFLtx0jaLAMyjXrYl2CjJEFxrhoCNeXF-tJkJNk0VKyCrnsr2FyrZwF0ET0Jpeer7Qwf4S1FZOfhPD16nxY0JUIInqk3k-JlQR3P3HEA6BBpfmnBLok4iuuXm8obR4hYapulJZnydO0PaEOqqH2jh8XmpJCx5I_il39NLdvZXnOy48SYvk2RVdxcQUohkQKby_maqAc2DB7NdEi3nVtokVjoerPawORIn64KbKuBNPvdGc_wQ# 
# 
```

### Opaque Secret

Opaque 类型的数据是一个 map 类型，要求 value 是 base64 编码格式。

加密、解密

使用命令行方式对需要加密的字符串进行加密。例如:mysql数据库的密码。

```sh
# 对admin字符串进行base64加密:获得admin的加密字符串"YWRtaW4="
echo -n "admin" | base64

# base64解密：对加密后的字符串进行解密
echo -n "YWRtaW4=" | base64 -d

```

输出:

```sh
[root@k8s-master-50 ~]# echo -n "admin" | base64
YWRtaW4=
[root@k8s-master-50 ~]# echo -n "YWRtaW4=" | base64 -d
admin[root@k8s-master-50 ~]# echo -n "root" | base64
cm9vdA==
```



资源文件方式创建

对mariadb数据库密码进行加密

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mariadbsecret
type: Opaque
data:
  password: YWRtaW4=
  #mariadb的用户名root加密，用于演示.
  username: cm9vdA==
```



完整的资源信息:

mariadb.yml

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mariadbsecret
type: Opaque
data:
  password: YWRtaW4=
  #mariadb的用户名root加密，用于演示.
  username: cm9vdA==

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mariadb-deploy
  labels:
    app: mariadb-deploy
spec:
  replicas: 1
  template:
    metadata:
      name: mariadb-deploy
      labels:
        app: mariadb-deploy
    spec:
      containers:
        - name: mariadb-deploy
          image: mariadb:10.5.2
          imagePullPolicy: IfNotPresent
          env:
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: mariadbsecret
            - name: TZ
              value: Asia/Shanghai
          args:
            - "--character-set-server=utf8mb4"
            - "--collation-server=utf8mb4_unicode_ci"
          ports:
            - containerPort: 3306
  selector:
    matchLabels:
      app: mariadb-deploy

---
apiVersion: v1
kind: Service
metadata:
  name: mariadb-svc
spec:
  selector:
    app: mariadb-deploy
  ports:
    - port: 3306
      targetPort: 3306
      nodePort: 30036
  type: NodePort
```

运行服务

```sh
kubectl apply -f .

kubectl get secret

kubectl get svc
```

操作

```sh
[root@k8s-master-50 opaqueSecret]# kubectl apply -f .
secret/mariadbsecret created
deployment.apps/mariadb-deploy created
service/mariadb-svc created
[root@k8s-master-50 opaqueSecret]# kubectl get Secret
NAME                  TYPE                                  DATA   AGE
default-token-bgthv   kubernetes.io/service-account-token   3      36d
mariadbsecret         Opaque                                2      28s
[root@k8s-master-50 opaqueSecret]# kubectl get svc
\NAME          TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE
kubernetes    ClusterIP   10.1.0.1     <none>        443/TCP          36d
mariadb-svc   NodePort    10.1.43.35   <none>        3306:30036/TCP   2m22s
```

测试连接

```sh
ip: 192.168.5.50
port: 30036
username: root
password: admin
```

通过客户端可以发现，使用此可以成功的连接mariadb



删除服务

```sh
kubectl delete -f .

kubectl get secret

kubectl get svc
```

输出：

```sh
[root@k8s-master-50 opaqueSecret]# kubectl delete -f .
secret "mariadbsecret" deleted
deployment.apps "mariadb-deploy" deleted
service "mariadb-svc" deleted
[root@k8s-master-50 opaqueSecret]# kubectl get secret
NAME                  TYPE                                  DATA   AGE
default-token-bgthv   kubernetes.io/service-account-token   3      36d
[root@k8s-master-50 opaqueSecret]# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.1.0.1     <none>        443/TCP   36d
[root@k8s-master-50 opaqueSecret]# 
```



### 安装harbor私服

harbor官网地址：

```sh
#harbor官网地址：
https://goharbor.io/

#github官网地址：
https://github.com/goharbor/harbor
```

docker-compose

```sh
# 验证docker-compose
docker-compose -v
```

安装harbor

```sh
# 1.解压软件
mkdir /data
cd /data
tar zxf harbor-offline-installer-v1.9.4.tgz
# 2.进入安装目录
cd harbor
# 3.修改配置文件
vi harbor.yml
# 3.1修改私服镜像地址
hostname: 192.168.5.21
# 3.2修改镜像地址访问端口号
port: 5000
# 3.3harbor管理员登录系统密码
harbor_admin_password: Harbor12345

# 3.4修改harbor映射卷目录
data_volume: /data/harbor/data


#4.安装harbor
# 4.1执行启动脚本,经过下述3个步骤后，成功安装harbor私服
./install.sh
# 4.2准备安装环境：检查docker版本和docker-compose版本
#4.3加载harbor需要的镜像
#4.4准备编译环境
#4.5启动harbor。通过docker-compose方式启动服务
#4.6google浏览器访问harbor私服
# http://192.168.5.21:5000
   username: admin
   password: Harbor12345

```

新建一个公开的项目: `nullnull-edu`

配制私服

```sh
# k8s集群master节点配置docker私服：master节点用于上传镜像。其余工作节点暂时不要配置私服地址。
vi /etc/docker/daemon.json
# 添加
"insecure-registries":["192.168.5.21:5000"]

# 重启docker服务：
systemctl daemon-reload
systemctl restart docker

```

输出:

```sh
[root@k8s-master-50 ~]# vi /etc/docker/daemon.json
[root@k8s-master-50 ~]# cat  /etc/docker/daemon.json
{
  "registry-mirrors": ["https://ys2mfbsh.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"],
  "insecure-registries":["192.168.5.21:5000"]
}
[root@k8s-master-50 ~]# systemctl daemon-reload
[root@k8s-master-50 ~]# systemctl restart docker
```

登录私服:

```sh
# 登录私服
docker login -u admin -p Harbor12345 192.168.5.21:5000

# 退出私服
docker logout 192.168.5.21:5000
```

输出:

```sh
[root@k8s-master-50 ~]# docker login -u admin -p Harbor12345 192.168.5.21:5000
WARNING! Using --password via the CLI is insecure. Use --password-stdin.
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
[root@k8s-master-50 ~]# docker logout 192.168.5.21:5000
Removing login credentials for 192.168.5.21:5000
[root@k8s-master-50 ~]# 
```

上传镜像

```sh
# 登录服务器
docker login -u admin -p Harbor12345 192.168.5.21:5000

# 给images打标签
docker tag mariadb:10.5.2 192.168.5.21:5000/nullnull-edu/mariadb:10.5.2

# 推送
docker push 192.168.5.21:5000/nullnull-edu/mariadb:10.5.2

# 删除本地镜像标签
docker rmi 192.168.5.21:5000/nullnull-edu/mariadb:10.5.2

```

输出:

```sh
[root@k8s-master-50 ~]# docker login -u admin -p Harbor12345 192.168.5.21:5000
WARNING! Using --password via the CLI is insecure. Use --password-stdin.
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
[root@k8s-master-50 ~]# docker tag mariadb:10.5.2 192.168.5.21:5000/nullnull-edu/mariadb:10.5.2
[root@k8s-master-50 ~]# docker push 192.168.5.21:5000/nullnull-edu/mariadb:10.5.2
The push refers to repository [192.168.5.21:5000/nullnull-edu/mariadb]
2a75ca7bbb37: Pushed 
ab30662e1c24: Pushed 
dfce0ddc1750: Pushed 
d0abe7e5ebab: Pushed 
2df470f82b36: Pushed 
e0b9a9a4c57f: Pushed 
78452794b5bd: Pushed 
8179bbf82947: Pushed 
fadf5ecbe4d4: Pushed 
28ba7458d04b: Pushed 
838a37a24627: Pushed 
a6ebef4a95c3: Pushed 
b7f7d2967507: Pushed 
10.5.2: digest: sha256:5d8f0d6ef1de0d626fc26355f2ed8965f91f7eb91273087d89e3321e27f16dd7 size: 3034
[root@k8s-master-50 ~]# docker images
REPOSITORY                                                                     TAG                        IMAGE ID       CREATED       SIZE
perl                                                                           slim                       225d090f3731   4 weeks ago   180MB
nginx                                                                          1.18.0-alpine              684dbf9f01f3   2 years ago   21.9MB
tomcat                                                                         9.0.37-jdk8-openjdk-slim   d60b68827676   3 years ago   305MB
tomcat                                                                         9.0.37-jdk8                9c7be7b021c3   3 years ago   531MB
nginx                                                                          1.19.2-alpine              6f715d38cfe0   3 years ago   22.1MB
calico/node                                                                    v3.14.2                    780a7bc34ed2   3 years ago   262MB
calico/pod2daemon-flexvol                                                      v3.14.2                    9dfa8f25b51c   3 years ago   22.8MB
calico/cni                                                                     v3.14.2                    e6189009f081   3 years ago   119MB
calico/kube-controllers                                                        v3.14.2                    4815e4106d26   3 years ago   52.8MB
192.168.5.21:5000/nullnull-edu/mariadb                                         10.5.2                     fd055a110f74   3 years ago   360MB
mariadb                                                                        10.5.2                     fd055a110f74   3 years ago   360MB
nginx                                                                          1.17.10-alpine             89ec9da68213   3 years ago   19.9MB
k8s.gcr.io/kube-proxy                                                          v1.17.5                    e13db435247d   3 years ago   116MB
k8s.gcr.io/kube-controller-manager                                             v1.17.5                    fe3d691efbf3   3 years ago   161MB
k8s.gcr.io/kube-apiserver                                                      v1.17.5                    f640481f6db3   3 years ago   171MB
k8s.gcr.io/kube-scheduler                                                      v1.17.5                    f648efaff966   3 years ago   94.4MB
quay.io/kubernetes-ingress-controller/nginx-ingress-controller                 0.30.0                     89ccad40ce8e   3 years ago   323MB
registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller   0.30.0                     89ccad40ce8e   3 years ago   323MB
k8s.gcr.io/coredns                                                             1.6.5                      70f311871ae1   4 years ago   41.6MB
k8s.gcr.io/etcd                                                                3.4.3-0                    303ce5db0e90   4 years ago   288MB
tomcat                                                                         9.0.20-jre8-alpine         387f9d021d3a   4 years ago   108MB
k8s.gcr.io/pause                                                               3.1                        da86e6ba6ca1   6 years ago   742kB
[root@k8s-master-50 ~]# docker rmi 192.168.5.21:5000/nullnull-edu/mariadb:10.5.2
Untagged: 192.168.5.21:5000/nullnull-edu/mariadb:10.5.2
Untagged: 192.168.5.21:5000/nullnull-edu/mariadb@sha256:5d8f0d6ef1de0d626fc26355f2ed8965f91f7eb91273087d89e3321e27f16dd7
[root@k8s-master-50 ~]# 
```















## 结束



