# K8S笔记

## Kubernetes安装

**硬件要求**

| 硬件 | 要求    |
| ---- | ------- |
| CPU  | 至少2核 |
| 内存 | 至少3G  |
| 硬件 | 至少50G |

**节点信息**

| 主机名        | IP           |
| ------------- | ------------ |
| k8s-master-50 | 192.168.5.50 |
| k8s-node-51   | 192.168.5.51 |
| k8s-node-52   | 192.168.5.52 |
| k8s-node-53   | 192.168.5.53 |

系统要求：

推荐使用centos7.7及以上版本

国内建议使用阿里云下载

```http
http://mirrors.aliyun.com/centos/7/isos/x86_64/
```

### 配制阿里云yum源

```sh
# 1. 下载安装wget
yum install -y wget

#2.备份默认的yum
mv /etc/yum.repos.d /etc/yum.repos.d.backup

#3.设置新的yum目录
mkdir -p /etc/yum.repos.d

#4.下载阿里yum配置到该目录中，选择对应版本
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo

#5.更新epel源为阿里云epel源
mv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backup
mv /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backup

wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo

#6.重建缓存
yum clean all
yum makecache

#7.看一下yum仓库有多少包
yum repolist
yum update
```

可以做一个系统快照版本，防止后面的翻车，以做好回滚，不用从头开始。做系统快照建议关机做，关机后的快照特别小。

### 升级系统内核

默认此处安装的7.9版本，内核版本为3.10，需要做下升级,此安装完成后，系统内核版本需要升级到了`4.4`，但别太高，不然操作不一致。

```sh
# 导入仓库源
rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm

# 查看可安装的软件包
yum --enablerepo="elrepo-kernel" list --showduplicates | sort -r | grep kernel-lt.x86_64

# 升级到最新内核版本的方法
yum --enablerepo=elrepo-kernel install -y kernel-lt


grep initrd16 /boot/grub2/grub.cfg
grub2-set-default 0

reboot
```

### 查看相关的系统信息

```sh
# 查看centos系统内核命令
uname -r
uname -a

# 查看CPU
lscpu

# 查看内存
free 
free -g

# 查看硬盘信息
fdisk -l
```

### centos7系统配制

```sh
# 1. 开发环境可以直接关闭防火墙，但生产环境，切记，万不能关
systemctl stop firewalld
systemctl disable firewalld


# 2. 关闭selinux
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
setenforce 0


# 3. 网桥过滤
vi /etc/sysctl.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-arptables = 1
net.ipv4.ip_forward=1
net.ipv4.ip_forward_use_pmtu = 0
#生效命令
sysctl --system
#查看效果
sysctl -a|grep "ip_forward"



# 4.  开启IPVS
#安装IPVS
yum -y install ipset ipvsadm -y
#编译ipvs.modules文件
vi /etc/sysconfig/modules/ipvs.modules
#文件内容如下
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
# 在4.4内核版本中使用nf_conntrack_ipv4
modprobe -- nf_conntrack_ipv4
#赋予权限并执行
chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack
#重启电脑，检查是否生效
reboot
lsmod | grep ip_vs_rr



# 5. 同步时间
#安装软件
yum -y install ntpdate
#向阿里云服务器同步时间
ntpdate time1.aliyun.com
#删除本地时间并设置时区为上海
rm -rf /etc/localtime
ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
#查看时间
date -R || date



# 6.命令补全
#安装bash-completion
yum -y install bash-completion bash-completion-extras
#使用bash-completion
source /etc/profile.d/bash_completion.sh



# 7. 关闭swap分区
#临时关闭：
swapoff -a
#永久关闭：
vi /etc/fstab
将文件中的/dev/mapper/centos-swap这行代码注释掉
#/dev/mapper/centos-swap swap swap defaults 0 0
#确认swap已经关闭：若swap行都显示 0 则表示关闭成功
free -m


# 8. hosts配置
#文件内容如下:
cat <<EOF >> /etc/hosts
192.168.5.50 k8s-master-50
192.168.5.51 k8s-node-51
192.168.5.52 k8s-node-52
192.168.5.53 k8s-node-53
EOF

cat /etc/hosts
```

### 安装docker

```sh
# 1. 安装docker的前置条件
yum install -y yum-utils device-mapper-persistent-data lvm2

# 2. 添加源
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum makecache fast

# 3. 查看docker更新版本
yum list docker-ce --showduplicates | sort -r


# 4. 安装docker版本,这里需要指定版本为18.09.8
yum -y install docker-ce-18.09.8
# 如果需要安装最新版本，使用
# yum -y install docker-ce
# 开启docker服务开机启动
systemctl start docker
systemctl status docker
# 查看docker版本
docker version
# docker-client版本：当前最新版本
# docker-server需要为版本为：18.09.8

# 5. 阿里云镜像加速器地址
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://ys2mfbsh.mirror.aliyuncs.com"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker

# 6. 设置为开机自启动
systemctl enable docker

# 7. 修改Cgroup Driver
vi /etc/docker/daemon.json
# 修改daemon.json，新增：
"exec-opts": ["native.cgroupdriver=systemd"]
# 修改cgroupdriver是为了消除安装k8s集群时的告警：
# [WARNING IsDockerSystemdCheck]: 
# detected “cgroupfs” as the Docker cgroup driver. The recommended driver is “systemd”. 
# Please follow the guide at https://kubernetes.io/docs/setup/cri/......
#重启docker服务：
systemctl daemon-reload 
systemctl restart docker
#查看修改后状态：
docker info | grep Cgroup


# 查看docker的信息
docker -v
docker info
```



### 使用kubeadm安装k8s

| 软件 | kubeadm                        | kubelet                                                    | kubectl                             | docker-ce |
| ---- | ------------------------------ | ---------------------------------------------------------- | ----------------------------------- | --------- |
| 版本 | 初始化集群管理.<br/>版本1.17.5 | 用于接收api-server指令对pod生命周期进行管理<br/>版本1.17.5 | 集群命令行管理工具<br/>版本：1.17.5 | 18.09.8   |

```sh
# 1. 安装yum源
vi /etc/yum.repos.d/kubernates.repo
# 加入内容
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
       
# 2. 更新缓存
yum clean all
yum -y makecache

# 3. 验证是否可用
yum list | grep kubeadm
#如果提示要验证yum-key.gpg是否可用，输入y。
#查找到kubeadm。显示版本

# 4. 查看k8s的版本
yum list kubelet --showduplicates | sort -r

# 5. 安装k8s-1.17.5
yum install -y kubelet-1.17.5 kubeadm-1.17.5 kubectl-1.17.5


# 6. 设置kubelet
# 增加配置信息
# 如果不配置kubelet，可能会导致K8S集群无法启动。为实现docker使用的cgroupdriver与kubelet使用的cgroup的一致性。
vi /etc/sysconfig/kubelet
KUBELET_EXTRA_ARGS="--cgroup-driver=systemd"


# 7. 设置开机启动
systemctl enable kubelet
```



### 初始化镜像

如果是首次安装k8s，手里没有备份好的镜像，需要执行此操作，如果已经已经有了镜像备份，请跳过本章节

```sh
# 查看安装集群需要镜像
kubeadm config images list
# 可得到如下这样一个列表
k8s.gcr.io/kube-apiserver:v1.17.5
k8s.gcr.io/kube-controller-manager:v1.17.5
k8s.gcr.io/kube-scheduler:v1.17.5
k8s.gcr.io/kube-proxy:v1.17.5
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.4.3-0
k8s.gcr.io/coredns:1.6.5
```

使用脚本下载镜像

```sh
mkdir -p /data/k8s
cd /data/k8s
# 编辑内容
vi images.sh
```

脚本内容:

从阿里云下载镜像，再下载的镜像应该去除"k8s.gcr.io"的前缀，版本换成kubeadm config images list命令获取到的版本

```shell
#!/bin/bash
images=(
   kube-apiserver:v1.17.5
   kube-controller-manager:v1.17.5
   kube-scheduler:v1.17.5
   kube-proxy:v1.17.5
   pause:3.1
   etcd:3.4.3-0
   coredns:1.6.5
)
for imageName in ${images[@]} ; 
do
   docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
   docker tag  registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName
   docker rmi  registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
done
```



执行脚本保存镜像

```sh
cd /data/k8s
# 给脚本授权
chmod +x images.sh 
# 执行脚本
./images.sh 

# 查看镜像
docker images

# 保存镜像
docker save -o k8s-1.17.5-all.image           \
k8s.gcr.io/kube-proxy:v1.17.5                 \
k8s.gcr.io/kube-controller-manager:v1.17.5    \
k8s.gcr.io/kube-apiserver:v1.17.5             \
k8s.gcr.io/kube-scheduler:v1.17.5             \
k8s.gcr.io/coredns:1.6.5                      \
k8s.gcr.io/etcd:3.4.3-0                       \
k8s.gcr.io/pause:3.1                          
```

此时基础的环境已经安装完成，可以做一个系统快照。以便安装翻车的时候恢复快照。



### 初始化集群网络

准备镜像

```sh
# 官网下载地址：
https://docs.projectcalico.org/v3.14/manifests/calico.yaml
# github地址：
https://github.com/projectcalico/calico


# 1. 镜像下载：
docker pull calico/cni:v3.14.2
docker pull calico/pod2daemon-flexvol:v3.14.2
docker pull calico/node:v3.14.2
docker pull calico/kube-controllers:v3.14.2

# 2. 初始化集群信息: calico网络,此在master节点上执行
kubeadm init --apiserver-advertise-address=192.168.5.50 --kubernetes-version v1.17.5 --service-cidr=10.1.0.0/16 --pod-network-cidr=10.81.0.0/16


# 3. 执行配制命令
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# 4. node节点加入集群信息 此命令，需要在3个node节点上执行
kubeadm join 192.168.5.50:6443 --token h1bc1a.q9i19q7smxl2pzom \
    --discovery-token-ca-cert-hash sha256:91d21fad4c1f4cdd3a7a01cae4d69b03805486ee90a4c2e77ac6b63f3ae66295 

# 5. kubectl命令自动补全,每个节点都执行下
echo "source <(kubectl completion bash)" >> ~/.bash_profile
source ~/.bash_profile
#在 bash 中设置当前 shell 的自动补全，要先安装 bash-completion 包。
echo "unset MAILCHECK">> /etc/profile
source /etc/profile
#在你的 bash shell 中永久的添加自动补全


# 6.yum-key.gpg验证未通过的相关执行
wget https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
wget https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
rpm --import yum-key.gpg
rpm --import rpm-package-key.gpg


# 查看集群的状态,集群的状态，同样是在master节点才能查看
kubectl get nodes

# 7. 下载网格的初始化文件
wget https://docs.projectcalico.org/v3.14/manifests/calico.yaml

# 执行网络初始化,此初始化，仅需要在master节点执行。
kubectl apply -f calico.yaml

# 查看集群的状态,同样是在master节点才能查看
kubectl get nodes
```

集群初始化:

```sh
[root@k8s-master-50 ~]# kubeadm init --apiserver-advertise-address=192.168.5.50 --kubernetes-version v1.17.5 --service-cidr=10.1.0.0/16 --pod-network-cidr=10.81.0.0/16
W1227 11:01:55.994044    3131 validation.go:28] Cannot validate kube-proxy config - no validator is available
W1227 11:01:55.994077    3131 validation.go:28] Cannot validate kubelet config - no validator is available
[init] Using Kubernetes version: v1.17.5
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master-50 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.1.0.1 192.168.5.50]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master-50 localhost] and IPs [192.168.5.50 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master-50 localhost] and IPs [192.168.5.50 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
W1227 11:01:57.949710    3131 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[control-plane] Creating static Pod manifest for "kube-scheduler"
W1227 11:01:57.950137    3131 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 15.012704 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.17" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master-50 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node k8s-master-50 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: h1bc1a.q9i19q7smxl2pzom
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.5.50:6443 --token h1bc1a.q9i19q7smxl2pzom \
    --discovery-token-ca-cert-hash sha256:91d21fad4c1f4cdd3a7a01cae4d69b03805486ee90a4c2e77ac6b63f3ae66295 
```

节点加入集群的日志：

```sh
[root@k8s-node51 ~]# kubeadm join 192.168.5.50:6443 --token h1bc1a.q9i19q7smxl2pzom \
>     --discovery-token-ca-cert-hash sha256:91d21fad4c1f4cdd3a7a01cae4d69b03805486ee90a4c2e77ac6b63f3ae66295 
W1227 11:04:05.199752    3301 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
[preflight] Running pre-flight checks
        [WARNING Hostname]: hostname "k8s-node51" could not be reached
        [WARNING Hostname]: hostname "k8s-node51": lookup k8s-node51 on 192.168.3.1:53: no such host
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

经过以上的步骤，集群相关的安装已经完成：

查看集群的一个状态:

```sh
[root@k8s-master-50 ~]# kubectl get nodes
NAME            STATUS     ROLES    AGE     VERSION
k8s-master-50   NotReady   master   10m     v1.17.5
k8s-node51      NotReady   <none>   8m53s   v1.17.5
k8s-node52      NotReady   <none>   8m49s   v1.17.5
k8s-node53      NotReady   <none>   8m35s   v1.17.5
```

集群网络初始化：

```sh
[root@k8s-master-50 k8s]# kubectl apply -f calico.yaml
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
```

集群网络初始化成功后的一个状态:

```sh
[root@k8s-master-50 k8s]# kubectl get nodes
NAME            STATUS   ROLES    AGE   VERSION
k8s-master-50   Ready    master   16m   v1.17.5
k8s-node51      Ready    <none>   14m   v1.17.5
k8s-node52      Ready    <none>   14m   v1.17.5
k8s-node53      Ready    <none>   14m   v1.17.5
```





## k8s之NameSpace

中文名称：命名空间，可以认为namespace是kubernets集群中的虚拟化集群。在一个kubernetes集群中可以拥有多个命名空间，它们逻辑上彼此隔离。可以为你提供组织、安全、甚至性能方面的帮助。

Namespace是一对资源和对象的抽象集合。比如可以用系来将系统内部的对象划分为不同的项目组或用户组。常见的是pods,servers,replication controllers和deployment和deployments等都属于某一个namespace(默认是default)，而node，persistentVolumnets等则不属性任何namespace。

大多数的kubernetes中集群默认会有一个default的namespace。实际上应该是4个

- default:  资源默认被创建于default命名空间。
- kube-system： kubernetes系统组件使用。
- kube-node-lease: kubernetes集群节点租约状态。v1.13加入
- kube-public: 公共资源使用。但实际上现在并不常用。

这个默认（default）的namespace并没有什么特别，但你不能删除它。这很适合刚刚开始使用kubernetes和一些小的产品系统。但不建议应用于大型生产系统。因为，这种复杂系统中。团队会非常容易意外地或者无意识的重写或者中断其他service。相反，请创建多个命名空间来把你的service（服务）分割成一时半会儿 容易管理的块。

作用：多租户情况下，实现资源的隔离。

属于逻辑隔离。

属于管理边界。

不属性网络边界。

可以针对每个namespace做资源配额。

**查看命名空间**

```sh
kubectl get namespace
# 简写命令
kubectl get ns

# 查看所有命名空间的pod资源
kubectl get pod --all-namepsace
kubectl get pod -A
```

输出 ：

```sh
[root@k8s-master-50 ~]# kubectl get namespace
NAME              STATUS   AGE
default           Active   22h
kube-node-lease   Active   22h
kube-public       Active   22h
kube-system       Active   22h
[root@k8s-master-50 ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   22h
kube-node-lease   Active   22h
kube-public       Active   22h
kube-system       Active   22h
[root@k8s-master-50 ~]# kubectl get pod --all-namespaces
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6b94766748-hfbjs   1/1     Running   3          22h
kube-system   calico-node-27kqm                          1/1     Running   3          22h
kube-system   calico-node-8w2pp                          1/1     Running   3          22h
kube-system   calico-node-brwl7                          1/1     Running   3          22h
kube-system   calico-node-s6d49                          1/1     Running   4          22h
kube-system   coredns-6955765f44-rq92j                   1/1     Running   3          22h
kube-system   coredns-6955765f44-wmr68                   1/1     Running   3          22h
kube-system   etcd-k8s-master-50                         1/1     Running   3          22h
kube-system   kube-apiserver-k8s-master-50               1/1     Running   4          22h
kube-system   kube-controller-manager-k8s-master-50      1/1     Running   3          22h
kube-system   kube-proxy-4cbsx                           1/1     Running   3          22h
kube-system   kube-proxy-l6k6x                           1/1     Running   3          22h
kube-system   kube-proxy-pq57w                           1/1     Running   4          22h
kube-system   kube-proxy-tlb5j                           1/1     Running   3          22h
kube-system   kube-scheduler-k8s-master-50               1/1     Running   3          22h
[root@k8s-master-50 ~]# kubectl get pod -A
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6b94766748-hfbjs   1/1     Running   3          22h
kube-system   calico-node-27kqm                          1/1     Running   3          22h
kube-system   calico-node-8w2pp                          1/1     Running   3          22h
kube-system   calico-node-brwl7                          1/1     Running   3          22h
kube-system   calico-node-s6d49                          1/1     Running   4          22h
kube-system   coredns-6955765f44-rq92j                   1/1     Running   3          22h
kube-system   coredns-6955765f44-wmr68                   1/1     Running   3          22h
kube-system   etcd-k8s-master-50                         1/1     Running   3          22h
kube-system   kube-apiserver-k8s-master-50               1/1     Running   4          22h
kube-system   kube-controller-manager-k8s-master-50      1/1     Running   3          22h
kube-system   kube-proxy-4cbsx                           1/1     Running   3          22h
kube-system   kube-proxy-l6k6x                           1/1     Running   3          22h
kube-system   kube-proxy-pq57w                           1/1     Running   4          22h
kube-system   kube-proxy-tlb5j                           1/1     Running   3          22h
kube-system   kube-scheduler-k8s-master-50               1/1     Running   3          22h
```

命名空间的说明：

```sh
default: 用户创建的pod默认都在此命名空间。
kube-public 所有用户均可以访问，包括未认证用户。
kube-node-lease kubernetes集群节点租约状态。
kube-system kubernetes集群在使用。
```

**创建和删除Namespace**

```sh
kubectl create namespace nullnull
kubectl delete namespace nullnull

# 简写命令
kubectl create ns nullnull
kubectl delete ns nullnull

```

输出:

```sh
[root@k8s-master-50 ~]# kubectl create namespace nullnull
namespace/nullnull created
[root@k8s-master-50 ~]# kubectl get namespace
NAME              STATUS   AGE
default           Active   23h
kube-node-lease   Active   23h
kube-public       Active   23h
kube-system       Active   23h
nullnull          Active   20s
[root@k8s-master-50 ~]# kubectl delete namespace nullnull
namespace "nullnull" deleted
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get namespace
NAME              STATUS   AGE
default           Active   23h
kube-node-lease   Active   23h
kube-public       Active   23h
kube-system       Active   23h
[root@k8s-master-50 ~]# kubectl create ns nullnull
namespace/nullnull created
[root@k8s-master-50 ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   23h
kube-node-lease   Active   23h
kube-public       Active   23h
kube-system       Active   23h
nullnull          Active   5s
[root@k8s-master-50 ~]# kubectl delete ns nullnull
namespace "nullnull" deleted
[root@k8s-master-50 ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   23h
kube-node-lease   Active   23h
kube-public       Active   23h
kube-system       Active   23h
```



## K8S之POD



![image-20231228102017454](.\images\image-20231228102017454.png)

pod是kubernetes集群能够调度的最小单元。POD是容器的封装。

在kubernetes集群中，Pod是所有业务类型的基础，也是K8S管理的最小单位级，它是一个或多个容器的组合。这些容器共享存储，网络和命名空间，以及如何运行的规范。在Pod中，所有容器都被同一安排和调度，并运行在共享的上下文中。对于具体应用而言，Pod是它们的逻辑主机，Pod包含业务相关的多个应用容器。

**pod的两个必知特点**

`网络`：每一个Pod都会被指派唯一的一个IP地址，在Pod中，每一个容器共享网络命名空间，包括IP地址和网络端口。在同一个Pod中的容器可以和localhost进行互相通信。当Pod中的容器需要与Pod外的实体进行通信时，则需要通过端口等共享的网络资源。

`存储`：Pod能够被指定共享存储卷的集合，在Pod中所有的容器能够访问共享存储卷，允许这些容器共享数据。存储卷也允许在一个Pod持久化数据，以防止其中的容器需要被重启。

**Pod的工作方式**

k8s一般不直接创建Pod，而是通过控制器和模板配置来管理和调度。

- Pod模板。

后续章节会介绍Pod模板。

- Pod重启

在Pod中的容器可能会由于异常等原因导致其终止退出，Kubernetes提供了重启策略以重启容器。重启策略对同一个Pod的所有容器起作用，容器的重启由Node上的kubelet执行。Pod支持三种重启策略：

1. Always: 只要退出就会重启。
2. OnFailure: 只有在失败退出时（exit code不等于0）时，才会重启。
3. Never: 只要退出，就不再重启。

注意：重启是指在Pod宿主Node上进行本地重启，而不是调度到其它的Node上。

- 资源限制

kubernetes通过cgroups限制容器的CPU和内存等计算资源，包括requests（请求，调度器保证调度到资源充足的Node上）和limits(上限)等。

**查看Pod**

```sh
# 查看default命名空间下的Pods
kubectl get pods
# 查看kube-system命名空间下的pods
kubectl get pods -n kube-system
# 查看所有命名空间下的pods
kubectl get pod --all-namespace
kubectl get pod -A
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl get pods
No resources found in default namespace.
[root@k8s-master-50 ~]# kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6b94766748-hfbjs   1/1     Running   3          23h
calico-node-27kqm                          1/1     Running   3          23h
calico-node-8w2pp                          1/1     Running   3          23h
calico-node-brwl7                          1/1     Running   3          23h
calico-node-s6d49                          1/1     Running   4          23h
coredns-6955765f44-rq92j                   1/1     Running   3          23h
coredns-6955765f44-wmr68                   1/1     Running   3          23h
etcd-k8s-master-50                         1/1     Running   3          23h
kube-apiserver-k8s-master-50               1/1     Running   4          23h
kube-controller-manager-k8s-master-50      1/1     Running   3          23h
kube-proxy-4cbsx                           1/1     Running   3          23h
kube-proxy-l6k6x                           1/1     Running   3          23h
kube-proxy-pq57w                           1/1     Running   4          23h
kube-proxy-tlb5j                           1/1     Running   3          23h
kube-scheduler-k8s-master-50               1/1     Running   3          23h
[root@k8s-master-50 ~]# kubectl get pod --all-namespaces
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6b94766748-hfbjs   1/1     Running   3          23h
kube-system   calico-node-27kqm                          1/1     Running   3          23h
kube-system   calico-node-8w2pp                          1/1     Running   3          23h
kube-system   calico-node-brwl7                          1/1     Running   3          23h
kube-system   calico-node-s6d49                          1/1     Running   4          23h
kube-system   coredns-6955765f44-rq92j                   1/1     Running   3          23h
kube-system   coredns-6955765f44-wmr68                   1/1     Running   3          23h
kube-system   etcd-k8s-master-50                         1/1     Running   3          23h
kube-system   kube-apiserver-k8s-master-50               1/1     Running   4          23h
kube-system   kube-controller-manager-k8s-master-50      1/1     Running   3          23h
kube-system   kube-proxy-4cbsx                           1/1     Running   3          23h
kube-system   kube-proxy-l6k6x                           1/1     Running   3          23h
kube-system   kube-proxy-pq57w                           1/1     Running   4          23h
kube-system   kube-proxy-tlb5j                           1/1     Running   3          23h
kube-system   kube-scheduler-k8s-master-50               1/1     Running   3          23h
[root@k8s-master-50 ~]# kubectl get pod -A
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6b94766748-hfbjs   1/1     Running   3          23h
kube-system   calico-node-27kqm                          1/1     Running   3          23h
kube-system   calico-node-8w2pp                          1/1     Running   3          23h
kube-system   calico-node-brwl7                          1/1     Running   3          23h
kube-system   calico-node-s6d49                          1/1     Running   4          23h
kube-system   coredns-6955765f44-rq92j                   1/1     Running   3          23h
kube-system   coredns-6955765f44-wmr68                   1/1     Running   3          23h
kube-system   etcd-k8s-master-50                         1/1     Running   3          23h
kube-system   kube-apiserver-k8s-master-50               1/1     Running   4          23h
kube-system   kube-controller-manager-k8s-master-50      1/1     Running   3          23h
kube-system   kube-proxy-4cbsx                           1/1     Running   3          23h
kube-system   kube-proxy-l6k6x                           1/1     Running   3          23h
kube-system   kube-proxy-pq57w                           1/1     Running   4          23h
kube-system   kube-proxy-tlb5j                           1/1     Running   3          23h
kube-system   kube-scheduler-k8s-master-50               1/1     Running   3          23h
[root@k8s-master-50 ~]# 
```

**创建Pod**

```sh
# 下载镜像
# K8S集群的每一个节点都需要下载镜像，选择不同的镜像，下载镜像的大小也不同。
docker pull tomcat:9.0.20-jre8-alpine
docker pull tomcat:9.0.37-jdk8-openjdk-slim
docker pull tomcat:9.0.37-jdk8


# 在defalt命名空间中创建一个pod副本的deployment
kubectl run tomcat9-run --image=tomcat:9.0.20-jre8-alpine --port=8080

kubectl get pod
kubectl get pod -o wide

# 使用pod的ip访问容器
curl ****:8080
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl run tomcat9-run --image=tomcat:9.0.20-jre8-alpine --port=8080
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
deployment.apps/tomcat9-run created
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get pod
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9-run-6595fb5f85-fls4k   1/1     Running   0          72s
[root@k8s-master-50 ~]# kubectl get pod -w
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9-run-6595fb5f85-fls4k   1/1     Running   0          75s
^[[A^[[A^C[root@k8s-master-50 ~]# kubectl get pod -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP            NODE         NOMINATED NODE   READINESS GATES
tomcat9-run-6595fb5f85-fls4k   1/1     Running   0          85s   10.81.92.65   k8s-node51   <none>           <none>
[root@k8s-master-50 ~]# curl 10.81.92.65:8080
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        ......
    </body>

</html>
```

**扩容**

```sh
# 查看当前部署信息
kubectl get deployment
kubectl get deployment -o wide

# 将一个容器扩容至3个
kubectl scale --replicas=3 deployment/tomcat9-run

# 查看当前部署信息
kubectl get deployment
kubectl get deployment -o wide

# 查看pod信息
kubectl get pod
kubectl get pod -o wide
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl get deployment
NAME          READY   UP-TO-DATE   AVAILABLE   AGE
tomcat9-run   1/1     1            1           7m1s
[root@k8s-master-50 ~]# kubectl get deployment -o wide
NAME          READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS    IMAGES                      SELECTOR
tomcat9-run   1/1     1            1           7m1s   tomcat9-run   tomcat:9.0.20-jre8-alpine   run=tomcat9-run
[root@k8s-master-50 ~]# kubectl scale --replicas=3 deployment/tomcat9-run
deployment.apps/tomcat9-run scaled
[root@k8s-master-50 ~]# kubectl get deployment
NAME          READY   UP-TO-DATE   AVAILABLE   AGE
tomcat9-run   3/3     3            3           7m13s
[root@k8s-master-50 ~]# kubectl get deployment -o wide
NAME          READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS    IMAGES                      SELECTOR
tomcat9-run   3/3     3            3           7m13s   tomcat9-run   tomcat:9.0.20-jre8-alpine   run=tomcat9-run
[root@k8s-master-50 ~]# kubectl get  pod
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9-run-6595fb5f85-fls4k   1/1     Running   0          21m
tomcat9-run-6595fb5f85-gtxdp   1/1     Running   0          14m
tomcat9-run-6595fb5f85-m4drb   1/1     Running   0          14m
[root@k8s-master-50 ~]# kubectl get  pod -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
tomcat9-run-6595fb5f85-fls4k   1/1     Running   0          22m   10.81.92.65     k8s-node51   <none>           <none>
tomcat9-run-6595fb5f85-gtxdp   1/1     Running   0          15m   10.81.237.193   k8s-node52   <none>           <none>
tomcat9-run-6595fb5f85-m4drb   1/1     Running   0          15m   10.81.149.193   k8s-node53   <none>           <none>
[root@k8s-master-50 ~]# 
```

通过观察pod的一个输出可以发现，它们家口的网段是一致的，并且被分配了不同的节点上，10.81.92.65这个IP就是在node51这个节点上，其他的IP也对应了不同的节点。

而且使用此IP，加上8080端口，就可以直接访问

```sh
[root@k8s-master-50 ~]# curl 10.81.237.193:8080



<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        <div id="wrapper">
            <div id="navigation" class="curved container">
                <span id="nav-home"><a href="https://tomcat.apache.org/">Home</a></span>
                <span id="nav-hosts"><a href="/docs/">Documentation</a></span>
                <span id="nav-config"><a href="/docs/config/">Configuration</a></span>
                <span id="nav-examples"><a href="/examples/">Examples</a></span>
                <span id="nav-wiki"><a href="https://wiki.apache.org/tomcat/FrontPage">Wiki</a></span>
                <span id="nav-lists"><a href="https://tomcat.apache.org/lists.html">Mailing Lists</a></span>
                <span id="nav-help"><a href="https://tomcat.apache.org/findhelp.html">Find Help</a></span>
                <br class="separator" />
                ......
            </div>
       </div>
    </body>

</html>
```



**创建服务**

只有创建了服务才能被外网所访问

```sh
# 将之前部署的deployment（tomcat9-run）与service产生关系，外部便可以访问了。
kubectl expose deployment tomcat9-run --name=tomcat9-svc --port=8888 --target-port=8080 --protocol=TCP --type=NodePort
# 这个端口号是对内，对K8S集群内其他应该暴露的一个端口号
--port=8888
# deployment容器的端口号，
--target-port=8080

kubectl get service

# 简写
kubectl get svc
kubectl get svc -o wide

# 使用服务端口访问
curl 10.1.210.157:8888

# 外部使用浏览器访问
http://192.168.5.50:32395/
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl expose deployment tomcat9-run --name=tomcat9-svc --port=8888 --target-port=8080 --protocol=TCP --type=NodePort
service/tomcat9-svc exposed
[root@k8s-master-50 ~]# kubectl get service
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes    ClusterIP   10.1.0.1       <none>        443/TCP          29h
tomcat9-svc   NodePort    10.1.210.157   <none>        8888:32395/TCP   24m
[root@k8s-master-50 ~]# kubectl get svc
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes    ClusterIP   10.1.0.1       <none>        443/TCP          29h
tomcat9-svc   NodePort    10.1.210.157   <none>        8888:32395/TCP   16s
[root@k8s-master-50 ~]# kubectl get svc -o wide
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE   SELECTOR
kubernetes    ClusterIP   10.1.0.1       <none>        443/TCP          29h   <none>
tomcat9-svc   NodePort    10.1.210.157   <none>        8888:32395/TCP   48s   run=tomcat9-run
[root@k8s-master-50 ~]# curl 10.1.210.157:8888



<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        <div id="wrapper">
		......
		</div>
	</body>
</html>
```

浏览器打开测试：

![image-20231228163416398](.\images\image-20231228163416398.png)





## kubectl常用命令

语法规则：

```sh
kubectl [command] [TYPE] [NAME] [flags] [options]
```

- command: 指定一个或者多个资源执行的操作，例：`create`、`get `、`delete`。
- TYPE： 指定资源类型。 资源类型不区分大小写，可以指定单数、复数或缩写形式

```sh
kubectl get pod tomcat9-run-6595fb5f85-fls4k
kubectl get pods tomcat9-run-6595fb5f85-fls4k
kubectl get po tomcat9-run-6595fb5f85-fls4k
```

- NAME: 指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息`kubectl get pods`,对多个资源执行操作时，可以按类型和名称指定每个资源，或者一个或多个文件。
  - 要按类型和名称指定资源：
    - 要对所有类型相同的资源进行分组，可以执行： `TYPE1 name1 name2 name<#>`。
    - 分别指定多个资源类型：`TYPE1/name1 TYPE2/name2 TYPE3/name3 TYPE<#>/name<#>`。例如`kubectl get pod/example-pod1 replicationcontroller/example-rc1`
  - 用一个或多个文件指定资源：`-f file -f file2 -f file<#>`
    - 使用YAML而不是JSON，是因为YAML更容易使用，特别用于文件配制时。例：`kubectl get pod -f ./pod`
- flags: 指定可选参数，例如，可以使用-S或者-server参数指定Kubernetes API服务器的地址和端口。

注意：从命令行指定参数会覆盖默认值和任何相应的环境变量。

相关的帮助信息可以使用`kubectl help`查看

### **get命令**

`kubectl get` - 列出一个或者多个资源。

```sh
# 查看集群的状态信息
kubectl cluster-info

# 查看集群的状态
kubectl get cs

# 查看集群的节点信息
kubectl get nodes

# 查看集群命名空间
kubectl get ns

# 查看指定命名空间的服务
kubectl get svc -n kube-system

# 以纯文本列出所有Pod
kubectl get pods

# 以纯文本列出所有的Pod,并包含附加信息（如节点名）
kubectl get pods -o wide

# 以纯文本格式列出具有指定名称的副本控制器。提示：可以使用别名‘rc’缩短和替换‘replicationcontroller’资源类型。
kubectl get replicationcontroller <rc-name>
kubectl get rc <rc-name>

# 以纯文本格式列出所有副本控制器和服务
kubectl get rc,servers

# 以纯文本格式列出所有守护程序集，包括未初始化的守护程序集
# 在 Kubernetes 中，--include-uninitialized 标志在 Kubernetes 版本 1.19 之前是有效的，但在 Kubernetes 1.19 版本之后被废弃并删除。因此，如果你的 Kubernetes 版本为 1.19 或更新版本，则无法使用该标志。
kubectl get pods --include-uninitialized

# 列出在节点server01上运行的所有pod
kubectl get pods --field-selector=spec.nodeName=k8s-node51
```

输出：

```sh
[root@k8s-master-50 ~]# kubectl cluster-info
Kubernetes master is running at https://192.168.5.50:6443
KubeDNS is running at https://192.168.5.50:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get nodes
NAME            STATUS   ROLES    AGE   VERSION
k8s-master-50   Ready    master   46h   v1.17.5
k8s-node51      Ready    <none>   46h   v1.17.5
k8s-node52      Ready    <none>   46h   v1.17.5
k8s-node53      Ready    <none>   46h   v1.17.5
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   47h
kube-node-lease   Active   47h
kube-public       Active   47h
kube-system       Active   47h
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get svc -n kube-system
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.1.0.10    <none>        53/UDP,53/TCP,9153/TCP   47h
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9-run-6595fb5f85-fls4k   1/1     Running   1          17h
tomcat9-run-6595fb5f85-gtxdp   1/1     Running   1          17h
tomcat9-run-6595fb5f85-m4drb   1/1     Running   1          17h
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get pods -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
tomcat9-run-6595fb5f85-fls4k   1/1     Running   1          17h   10.81.92.66     k8s-node51   <none>           <none>
tomcat9-run-6595fb5f85-gtxdp   1/1     Running   1          17h   10.81.237.194   k8s-node52   <none>           <none>
tomcat9-run-6595fb5f85-m4drb   1/1     Running   1          17h   10.81.149.194   k8s-node53   <none>           <none>
[root@k8s-master-50 ~]#
[root@k8s-master-50 ~]# kubectl get replicationcontroller 
No resources found in default namespace.
[root@k8s-master-50 ~]# kubectl get rc
No resources found in default namespace.
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl get rc,services
NAME                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/kubernetes    ClusterIP   10.1.0.1       <none>        443/TCP          47h
service/tomcat9-svc   NodePort    10.1.210.157   <none>        8888:32395/TCP   17h
[root@k8s-master-50 ~]# 

[root@k8s-master-50 ~]# kubectl get pods --field-selector=spec.nodeName=k8s-node51
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9-run-6595fb5f85-fls4k   1/1     Running   1          18h

```



### **describe命令**

kubectl describe 命令显示一个或多个资源的详细状态，默认情况下包括未初始化的资源。

```sh
# 显示名称为<node-name>的节点的详细信息
kubectl describe nodes <node-name>

# 显示名为 <pod-name>的pod的详细信息
kubectl describe pods/<pod-name>

# 显示名为<rc-name>的副本控制器管理的所有pod的详细信息.
# 副本控制器创建的任何pod都以复制控制器的名称为前缀
kubectl describe pods <rc-name>

# 描述所有的Pod，不包括未初始化的pod
kubectl describe pods --include-uninitialized=false
# 在当前1.17.5版本中，此参数已经不生产效了
#Error: unknown flag: --include-uninitialized
#See 'kubectl describe --help' for usage.
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl describe nodes k8s-master-50
Name:               k8s-master-50
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=k8s-master-50
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/master=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 192.168.5.50/24
                    projectcalico.org/IPv4IPIPTunnelAddr: 10.81.230.0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 27 Dec 2023 11:02:11 +0800
Taints:             node-role.kubernetes.io/master:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  k8s-master-50
  AcquireTime:     <unset>
  RenewTime:       Fri, 29 Dec 2023 10:40:43 +0800
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Fri, 29 Dec 2023 09:54:24 +0800   Fri, 29 Dec 2023 09:54:24 +0800   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Fri, 29 Dec 2023 10:38:53 +0800   Wed, 27 Dec 2023 11:02:09 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Fri, 29 Dec 2023 10:38:53 +0800   Wed, 27 Dec 2023 11:02:09 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Fri, 29 Dec 2023 10:38:53 +0800   Wed, 27 Dec 2023 11:02:09 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Fri, 29 Dec 2023 10:38:53 +0800   Wed, 27 Dec 2023 11:17:04 +0800   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.5.50
  Hostname:    k8s-master-50
Capacity:
  cpu:                2
  ephemeral-storage:  59313872Ki
  hugepages-2Mi:      0
  memory:             16388392Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  54663664345
  hugepages-2Mi:      0
  memory:             16285992Ki
  pods:               110
System Info:
  Machine ID:                 c849151047d18342826600c28ebd79c1
  System UUID:                c8491510-47d1-8342-8266-00c28ebd79c1
  Boot ID:                    cf221679-f52f-4ab2-b804-91308f2f665f
  Kernel Version:             5.4.265-1.el7.elrepo.x86_64
  OS Image:                   CentOS Linux 7 (Core)
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://18.9.8
  Kubelet Version:            v1.17.5
  Kube-Proxy Version:         v1.17.5
PodCIDR:                      10.81.0.0/24
PodCIDRs:                     10.81.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  kube-system                 calico-kube-controllers-6b94766748-hfbjs    0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 calico-node-brwl7                           250m (12%)    0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 coredns-6955765f44-rq92j                    100m (5%)     0 (0%)      70Mi (0%)        170Mi (1%)     47h
  kube-system                 coredns-6955765f44-wmr68                    100m (5%)     0 (0%)      70Mi (0%)        170Mi (1%)     47h
  kube-system                 etcd-k8s-master-50                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 kube-apiserver-k8s-master-50                250m (12%)    0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 kube-controller-manager-k8s-master-50       200m (10%)    0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 kube-proxy-l6k6x                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h
  kube-system                 kube-scheduler-k8s-master-50                100m (5%)     0 (0%)      0 (0%)           0 (0%)         47h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                1 (50%)     0 (0%)
  memory             140Mi (0%)  340Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From                       Message
  ----    ------                   ----               ----                       -------
  Normal  Starting                 46m                kubelet, k8s-master-50     Starting kubelet.
  Normal  NodeHasSufficientMemory  46m (x8 over 46m)  kubelet, k8s-master-50     Node k8s-master-50 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    46m (x8 over 46m)  kubelet, k8s-master-50     Node k8s-master-50 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     46m (x7 over 46m)  kubelet, k8s-master-50     Node k8s-master-50 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  46m                kubelet, k8s-master-50     Updated Node Allocatable limit across pods
  Normal  Starting                 46m                kube-proxy, k8s-master-50  Starting kube-proxy.
[root@k8s-master-50 ~]# 
[root@k8s-master-50 ~]# kubectl describe pods/tomcat9-run-6595fb5f85-gtxdp
Name:         tomcat9-run-6595fb5f85-gtxdp
Namespace:    default
Priority:     0
Node:         k8s-node52/10.0.2.15
Start Time:   Thu, 28 Dec 2023 16:25:57 +0800
Labels:       pod-template-hash=6595fb5f85
              run=tomcat9-run
Annotations:  cni.projectcalico.org/podIP: 10.81.237.194/32
              cni.projectcalico.org/podIPs: 10.81.237.194/32
Status:       Running
IP:           10.81.237.194
IPs:
  IP:           10.81.237.194
Controlled By:  ReplicaSet/tomcat9-run-6595fb5f85
Containers:
  tomcat9-run:
    Container ID:   docker://7a6a24e758c62d5a5e9b8075256bb1d63f252c3b438e1484b62d160b7f4dfa7f
    Image:          tomcat:9.0.20-jre8-alpine
    Image ID:       docker-pullable://tomcat@sha256:17accf0afeeecce0310d363490cd60a788aa4630ab9c9c802231d6fbd4bb2375
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 29 Dec 2023 09:54:33 +0800
    Last State:     Terminated
      Reason:       Error
      Exit Code:    143
      Started:      Thu, 28 Dec 2023 16:25:59 +0800
      Finished:     Thu, 28 Dec 2023 17:36:13 +0800
    Ready:          True
    Restart Count:  1
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason          Age                From                 Message
  ----    ------          ----               ----                 -------
  Normal  SandboxChanged  48m (x2 over 49m)  kubelet, k8s-node52  Pod sandbox changed, it will be killed and re-created.
  Normal  Pulled          48m                kubelet, k8s-node52  Container image "tomcat:9.0.20-jre8-alpine" already present on machine
  Normal  Created         48m                kubelet, k8s-node52  Created container tomcat9-run
  Normal  Started         48m                kubelet, k8s-node52  Started container tomcat9-run
[root@k8s-master-50 ~]# 

[root@k8s-master-50 ~]# kubectl describe pods  tomcat9-run-6595fb5f85-fls4k
Name:         tomcat9-run-6595fb5f85-fls4k
Namespace:    default
Priority:     0
Node:         k8s-node51/10.0.2.15
Start Time:   Thu, 28 Dec 2023 16:18:50 +0800
Labels:       pod-template-hash=6595fb5f85
              run=tomcat9-run
Annotations:  cni.projectcalico.org/podIP: 10.81.92.66/32
              cni.projectcalico.org/podIPs: 10.81.92.66/32
Status:       Running
IP:           10.81.92.66
IPs:
  IP:           10.81.92.66
Controlled By:  ReplicaSet/tomcat9-run-6595fb5f85
Containers:
  tomcat9-run:
    Container ID:   docker://180f29f74c2ff39f40c7618b08bf735c684109beec52e60a0aad4d0188ec4279
    Image:          tomcat:9.0.20-jre8-alpine
    Image ID:       docker-pullable://tomcat@sha256:17accf0afeeecce0310d363490cd60a788aa4630ab9c9c802231d6fbd4bb2375
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 29 Dec 2023 09:53:52 +0800
    Last State:     Terminated
      Reason:       Error
      Exit Code:    143
      Started:      Thu, 28 Dec 2023 16:18:51 +0800
      Finished:     Thu, 28 Dec 2023 17:36:13 +0800
    Ready:          True
    Restart Count:  1
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>
[root@k8s-master-50 ~]# 
```

说明：

`kubectl get`命令通常用于检索同一资源类型的一个或者多个资源，它具有丰富的参数，允许使用-o或者--output参数自定义输出格式，也可以指定-w或者--watch开始观察特定对象的一时半会儿。

`kubectl describe`命令更侧重于描述指定资源的许多方面，它可以调用对API服务的多个API调用来为用户构建视图。例如: `kubectl describe node`命令不仅检索有关节点信息，还检索在其上运行的pod的摘要，为节点生成事件等。

### 进入容器命令

kubectl exec - 对pod中的容器执行命令。 与docker的exec命令非常类似

```sh
# 从pod <pod-name> 中获取运行‘date’的输出。默认情况下，输出来自第一个容器
kubectl exec <pod-name> date


# 在指定的pod(<tpod-name>)和指定的容器(<container-name>)进行输出
kubectl exec <pod-name> -c <container-name> date

# 使用一个交互的TTY并运行/bin/bash.默认情况下，输出来自第一个容器
kubectl exec -it <pod-name> /bin/bash
```



输出:

```sh
[root@k8s-master-50 ~]# kubectl exec tomcat9-run-6595fb5f85-xn8qs date
Sat Dec 30 11:10:22 UTC 2023
[root@k8s-master-50 ~]# kubectl exec tomcat9-run-6595fb5f85-nb2lq -c tomcat9-run date
Sat Dec 30 11:33:12 UTC 2023
[root@k8s-master-50 ~]# kubectl exec -it tomcat9-run-6595fb5f85-xn8qs bash
bash-4.4# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
4: eth0@if3248: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1440 qdisc noqueue state UP 
    link/ether 0a:49:be:8d:69:30 brd ff:ff:ff:ff:ff:ff
    inet 10.81.149.196/32 scope global eth0
       valid_lft forever preferred_lft forever
bash-4.4# exit
exit
```



### logs命令

kubectl logs - 打印Pod中容器的日志

```sh
# 从pod返回日志快照。
kubectl logs <pod-name>

# 从<pod-name>开始流程式传输日志，这类似于linux的'tail -f'命令
kubectl logs -f <pod-name>
```

输出

```sh
[root@k8s-master-50 ~]# kubectl logs tomcat9-run-6595fb5f85-nb2lq
30-Dec-2023 11:24:33.714 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version name:   Apache Tomcat/9.0.20
30-Dec-2023 11:24:33.721 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server built:          May 3 2019 22:26:00 UTC
30-Dec-2023 11:24:33.721 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version number: 9.0.20.0
......
30-Dec-2023 11:24:34.486 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["http-nio-8080"]
30-Dec-2023 11:24:34.491 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["ajp-nio-8009"]
30-Dec-2023 11:24:34.492 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in [482] milliseconds
[root@k8s-master-50 ~]# kubectl logs -f tomcat9-run-6595fb5f85-nb2lq
30-Dec-2023 11:24:33.714 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version name:   Apache Tomcat/9.0.20
30-Dec-2023 11:24:33.721 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server built:          May 3 2019 22:26:00 UTC
30-Dec-2023 11:24:33.721 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version number: 9.0.20.0
......
30-Dec-2023 11:24:34.484 INFO [main] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory [/usr/local/tomcat/webapps/manager] has finished in [50] ms
30-Dec-2023 11:24:34.486 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["http-nio-8080"]
30-Dec-2023 11:24:34.491 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["ajp-nio-8009"]
30-Dec-2023 11:24:34.492 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in [482] milliseconds
^C
```

### 格式化输出:

```sh
#将pod信息格式化输出到一个yaml文件中
kubectl get pod <pod-name> -o yaml
```

输出：

```yaml
[root@k8s-master-50 ~]# kubectl get pod tomcat9-run-6595fb5f85-nb2lq -o yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    cni.projectcalico.org/podIP: 10.81.92.68/32
    cni.projectcalico.org/podIPs: 10.81.92.68/32
  creationTimestamp: "2023-12-30T11:24:31Z"
  generateName: tomcat9-run-6595fb5f85-
  labels:
    pod-template-hash: 6595fb5f85
    run: tomcat9-run
  name: tomcat9-run-6595fb5f85-nb2lq
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: tomcat9-run-6595fb5f85
    uid: a681fe0d-f5d7-4974-9a6c-1539a55facd8
  resourceVersion: "220905"
  selfLink: /api/v1/namespaces/default/pods/tomcat9-run-6595fb5f85-nb2lq
  uid: c495c84b-4f75-4a84-a7cb-56db195d44a8
spec:
  containers:
  - image: tomcat:9.0.20-jre8-alpine
    imagePullPolicy: IfNotPresent
    name: tomcat9-run
    ports:
    - containerPort: 8080
      protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-bgthv
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: k8s-node51
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: default-token-bgthv
    secret:
      defaultMode: 420
      secretName: default-token-bgthv
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2023-12-30T11:24:31Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2023-12-30T11:24:33Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2023-12-30T11:24:33Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2023-12-30T11:24:31Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://6a4c1088bd02c51c8a54d37e221b9211ee0f84d340b414a9fdc471421abf1666
    image: tomcat:9.0.20-jre8-alpine
    imageID: docker-pullable://tomcat@sha256:17accf0afeeecce0310d363490cd60a788aa4630ab9c9c802231d6fbd4bb2375
    lastState: {}
    name: tomcat9-run
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2023-12-30T11:24:33Z"
  hostIP: 10.0.3.5
  phase: Running
  podIP: 10.81.92.68
  podIPs:
  - ip: 10.81.92.68
  qosClass: BestEffort
  startTime: "2023-12-30T11:24:31Z"
[root@k8s-master-50 ~]#
```



### **delete命令**

kubectl delete 从文件、stdin或者指定标签选择器、名称、资源选择器或资源中删除资源

```sh
# 使用pod.yaml文件中类型和名称删除pod
kubectl delete -f pod.yaml

# 删除标签名=<lab-name>的所有pod和服务
kubectl delete pods,services -l name=<label-name>

# 删除所有具有标签名称=<label-name>的pod和服务，包括未初始化的那些
kubectl delete pods,services -l name=<label-name> --include-uninitialized

# 删除所有pod 包括未初始化的pod
kubectl delete pods --all

# 强制删除一个pod，需要添加参数
--force --grace-period=0
```

输出:

```sh
[root@k8s-master-50 ~]# kubectl delete pod tomcat9-run-6595fb5f85-wdmxg
pod "tomcat9-run-6595fb5f85-wdmxg" deleted

[root@k8s-master-50 ~]# kubectl delete pod  tomcat9-run-6595fb5f85-nb2lq  --force --grace-period=0
warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
pod "tomcat9-run-6595fb5f85-nb2lq" force deleted
```



### 资源缩写

| 资源名                   | 缩写名 | API分组                   | 按命名空间 | 资源类型                |
| ------------------------ | ------ | ------------------------- | ---------- | ----------------------- |
| configmaps               | cm     |                           | true       | ConfigMap               |
| namespaces               | ns     |                           | false      | Namespace               |
| nodes                    | no     |                           | false      | Node                    |
| persistentvolumeclaims   | pvc    |                           | true       | PersistentVolumeClaim   |
| persistentvolumes        | pv     |                           | false      | PersistentVolume        |
| pods                     | po     |                           | true       | Pod                     |
| secrets                  |        |                           | true       | Secret                  |
| serviceaccounts          | sa     |                           | true       | ServiceAccount          |
| services                 | svc    |                           | true       | Service                 |
| daemonsets               | ds     | apps                      | true       | DaemonSet               |
| horizontalpodautoscalers | hpa    | autoscaling               | true       | HorizontalPodAutoscaler |
| cronjobs                 | cj     | batch                     | true       | CronJob                 |
| jobs                     |        | batch                     | true       | Job                     |
| ingresses                | ing    | extensions                | true       | ingress                 |
| poddisruptionbudgets     | pod    | policy                    | true       | PodDisruptionBudget     |
| clusterrolebindings      |        | rbac.authorization.k8s.io | false      | ClusterRoleBinding      |
| Clusterroles             |        | rbac.authorization.k8s.io | false      | clusterRole             |
| rolebindings             |        | rbac.authorization.k8s.io | true       | RoleBinding             |
| roles                    |        | rbac.authorization.k8s.io | true       | Role                    |
| storageclasses           | sc     | storage.k8s.io            | false      | StorageClass            |







## 使用yml文件配制k8s

### 使用yaml文件配制namespace

nullnullnamespace.yml

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: nullnull
```

远程运行

```sh
mkdir -p /data/k8s/namespace
cd /data/k8s/namespace
# 将文件上传到此目录

# 运行创建namespace的yml文件
kubectl apply -f nullnullnamespace.yml


# 可以运行删除Namespace
kubectl delete -f nullnullnamespace.yml
```

输出：

```sh
[root@k8s-master-50 namespace]# kubectl apply -f nullnullnamespace.yml
namespace/nullnull created
[root@k8s-master-50 namespace]# kubectl get namespace
NAME              STATUS   AGE
default           Active   3d13h
kube-node-lease   Active   3d13h
kube-public       Active   3d13h
kube-system       Active   3d13h
nullnull          Active   56s
[root@k8s-master-50 namespace]# kubectl delete -f nullnullnamespace.yml 
namespace "nullnull" deleted
[root@k8s-master-50 namespace]# kubectl get namespace
NAME              STATUS   AGE
default           Active   3d13h
kube-node-lease   Active   3d13h
kube-public       Active   3d13h
kube-system       Active   3d13h
[root@k8s-master-50 namespace]# 
```

### 使用yaml文件创建pod

nullnullpod.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tomcat9
  labels:
    app: tomcat9
 spec:
    containers:
      - name: tomcat9
        image: tomcat:9.0.20-jre8-alpine
        imagePullPolicy: ifNotPresent
    restartPolicy: Always
```

imagePullPolicy:

```sh
# 镜像下载的策略：
imagePullPolicy:
	Always: 总是拉取
	IfNotPresent: 如果本地有镜像，使用本地，如果本地没有镜像，下载镜像。
	Never: 只使用本地镜像，从不拉取
```

restartPolicy:

```sh
restartPolicy:
	Always: 只要退出就重启
	OnFailure: 失败退出时（exit code 不为0）才重启。
	Never: 永不重启
```

远程运行

```sh
mkdir -p /data/k8s/pod
cd /data/k8s/pod
# 将文件上传到此目录

# 运行pod
kubectl apply -f nullnullpod.yml

# 删除pod
kubectl delete -f nullnullpod.yml
```

输出:

```sh
[root@k8s-master-50 pod]# kubectl apply -f nullnullpod.yml
pod/tomcat9 created
[root@k8s-master-50 pod]# kubectl get pod
NAME                           READY   STATUS    RESTARTS   AGE
tomcat9                        1/1     Running   0          12s
tomcat9-run-6595fb5f85-7d2xw   1/1     Running   0          4h45m
tomcat9-run-6595fb5f85-w96pz   1/1     Running   0          4h44m
tomcat9-run-6595fb5f85-xn8qs   1/1     Running   0          5h23m
[root@k8s-master-50 pod]# kubectl get pod -o wide
NAME                           READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES
tomcat9                        1/1     Running   0          28s     10.81.149.197   k8s-node53   <none>           <none>
tomcat9-run-6595fb5f85-7d2xw   1/1     Running   0          4h45m   10.81.92.69     k8s-node51   <none>           <none>
tomcat9-run-6595fb5f85-w96pz   1/1     Running   0          4h44m   10.81.237.197   k8s-node52   <none>           <none>
tomcat9-run-6595fb5f85-xn8qs   1/1     Running   0          5h23m   10.81.149.196   k8s-node53   <none>           <none>
[root@k8s-master-50 pod]# curl 10.81.149.197:8080



<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        <div id="wrapper">
           	......
            <p class="copyright">Copyright &copy;1999-2023 Apache Software Foundation.  All Rights Reserved</p>
        </div>
    </body>

</html>
[root@k8s-master-50 pod]# kubectl delete -f nullnullpod.yml 
pod "tomcat9" deleted
[root@k8s-master-50 pod]# kubectl get pod -o wide
NAME                           READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES
tomcat9-run-6595fb5f85-7d2xw   1/1     Running   0          4h48m   10.81.92.69     k8s-node51   <none>           <none>
tomcat9-run-6595fb5f85-w96pz   1/1     Running   0          4h46m   10.81.237.197   k8s-node52   <none>           <none>
tomcat9-run-6595fb5f85-xn8qs   1/1     Running   0          5h25m   10.81.149.196   k8s-node53   <none>           <none>
```



### 使用yml文件Deployment

nullnulldeployment.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tomcat-deployment
  labels:
    apps: tomcat-deployment
spec:
  replicas: 3
  template:
    metadata:
      name: tomcat-deployment
      labels:
        app: tomcat
    spec:
      containers:
        - name: tomcat-deployment
          image: tomcat:9.0.20-jre8-alpine
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
  selector:
    matchLabels:
      app: tomcat
```

matchLabels

```sh
# 总结：
# 在Deployment中必须写matchLabels
# 在定义模板的时候必须定义labels，因为Deployment.spec.selector是必须的字段，而他又必须和template.labels对应。
```

运行deployment

```sh
mkdir -p /data/k8s/deployment
cd /data/k8s/deployment


kubectl apply -f nullnulldeployment.yml

# 查看deployment
kubectl get deployment -o wide

# 删除deployment
kubectl delete -f  nullnulldeployment.yml

# 再次查看deployment
kubectl get deployment -o wide
```

输出

```sh
[root@k8s-master-50 deployment]# kubectl apply -f nullnulldeployment.yml 
deployment.apps/tomcat-deployment created
[root@k8s-master-50 deployment]# kubectl get deployment -o wide
NAME                READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS          IMAGES                      SELECTOR
tomcat-deployment   3/3     3            3           58s   tomcat-deployment   tomcat:9.0.20-jre8-alpine   app=tomcat
[root@k8s-master-50 deployment]# kubectl delete -f  nullnulldeployment.yml
deployment.apps "tomcat-deployment" deleted
[root@k8s-master-50 deployment]# kubectl get deployment -o wide
No resources found in default namespace.
[root@k8s-master-50 deployment]# 
```

**控制器类型**

| 控制器名称  | 作用                                                         |
| ----------- | ------------------------------------------------------------ |
| Deployment  | 声明式更新控制器，用于发布无状态应用                         |
| ReplicaSet  | 副本集控制器，用于对Pod进行副本规模扩大和剪裁                |
| StatefulSet | 有状态的副本集，用于发布有状态的应用                         |
| DaemonSet   | 在k8s集群每个上node运行一个副本，用于发布监控或者日志收集类的应用。 |
| Job         | 运行一次性作业任务                                           |
| CronJob     | 运行周期性作业任务                                           |

Deployment控制器介绍：

具有上线部署、滚动升级、创建副本、回滚到以前某一版本（成功/稳定）等功能

Deployment包含ReplicaSet，除非需要自定义升级功能或者根本不需要升级Pod，否则还是建议使用Deployment而不是直接使用ReplicaSet



### 使用yml文件创建service

nullnullservice.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tomcat-deploy
  labels:
    app: tomcat-deploy
spec:
  replicas: 1
  template:
    metadata:
      name: tomcat-deploy
      labels:
        app: tomcat-pod
    spec:
      containers:
        - name: tomcat-deploy
          image: tomcat:9.0.20-jre8-alpine
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
      restartPolicy: Always
  selector:
    matchLabels:
      app: tomcat-pod

---
apiVersion: v1
kind: Service
metadata:
  name: tomcat-svc
spec:
  selector:
    app: tomcat-pod
  ports:
    - port: 8888
      targetPort: 8080
      nodePort: 30088
      protocol: TCP
  type: NodePort
```

servicer的selector

```sh
service.spec.selector.app 选择的内容仍是template.label.app的内容，不是deployment控制器的label内容。
```

service类型

```sh
ClusterIP: 默认，分配一个集群内部可以访问的虚拟IP
NodePort： 在每个Node上分配一个端口作业外部访问入口 
LoadBalancer: 工作在特定的Cloud Provider上，例如：Google Cloud,AWS, openStack
ExternalName: 表示把集群外部的服务引入到集群内部中来，即实现了集群内部pod和集群外部的服务进行通信。
```

service参数：

```sh
port: 访问service使用的端口
targetPort: pod中容器端口
nodePort: 通过Node实现外网用户访问k8s集群内service(30000-32767)
```

运行

```sh
mkdir -p /data/k8s/service
cd /data/k8s/service


kubectl apply -f nullnullservice.yml

# 查看service
kubectl get service -o wide

# 访问测试
curl *.*.*.*: xxxx

# 删除servioce
kubectl delete -f  nullnullservice.yml

# 再次查看service
kubectl get service -o wide
```

输出：

```sh
[root@k8s-master-50 service]# kubectl apply -f nullnullservice.yml
deployment.apps/tomcat-deploy created
service/tomcat-svc created
[root@k8s-master-50 service]# kubectl get service -o wide
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE     SELECTOR
kubernetes    ClusterIP   10.1.0.1       <none>        443/TCP          4d13h   <none>
tomcat-svc    NodePort    10.1.77.65     <none>        8888:30088/TCP   25s     app=tomcat-pod
[root@k8s-master-50 service]# curl http://192.168.5.50:30088



<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Apache Tomcat/9.0.20</title>
        <link href="favicon.ico" rel="icon" type="image/x-icon" />
        <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        <link href="tomcat.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        <div id="wrapper">
            ......
        </div>
    </body>

</html>
[root@k8s-master-50 service]# kubectl delete -f  nullnullservice.yml
deployment.apps "tomcat-deploy" deleted
service "tomcat-svc" deleted
[root@k8s-master-50 service]# kubectl get service -o wide
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
kubernetes   ClusterIP   10.1.0.1     <none>        443/TCP   4d13h   <none>

```



## 资源清单

### pod

资源清单有5 个顶组的字段组成： apiVersion、kind、metadata、spec、status。

```yaml
# 如果没有给定group名称，那么默认为core,可以使用kubectl apiversions 获取当前k8s版本上所有的apiVersion的版本信息（每个版本可能不同）
apiVersion: group/apiversion 
# 资源类型
kind: 

# 资源元数据信息
metadata:
  name:
  namespace:
  labes:
  # 主要目的是方便用户阅读查找
  annotations:

# 期望的状态(disired state)
spec:

# 当前的状态，本字段有kubernetes自身维护，用户不用去定义
status
```

使用kubectl命令可以查看apiVersion的各个版本信息

```sh
[root@k8s-master-50 ~]# kubectl api-versions
admissionregistration.k8s.io/v1
admissionregistration.k8s.io/v1beta1
apiextensions.k8s.io/v1
apiextensions.k8s.io/v1beta1
apiregistration.k8s.io/v1
apiregistration.k8s.io/v1beta1
apps/v1
authentication.k8s.io/v1
authentication.k8s.io/v1beta1
authorization.k8s.io/v1
authorization.k8s.io/v1beta1
autoscaling/v1
autoscaling/v2beta1
autoscaling/v2beta2
batch/v1
batch/v1beta1
certificates.k8s.io/v1beta1
coordination.k8s.io/v1
coordination.k8s.io/v1beta1
crd.projectcalico.org/v1
discovery.k8s.io/v1beta1
events.k8s.io/v1beta1
extensions/v1beta1
networking.k8s.io/v1
networking.k8s.io/v1beta1
node.k8s.io/v1beta1
policy/v1beta1
rbac.authorization.k8s.io/v1
rbac.authorization.k8s.io/v1beta1
scheduling.k8s.io/v1
scheduling.k8s.io/v1beta1
storage.k8s.io/v1
storage.k8s.io/v1beta1
v1
[root@k8s-master-50 ~]#
```

获取字段设置的帮助文档

```sh
[root@k8s-master-50 ~]# kubectl explain pod
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status       <Object>
     Most recently observed status of the pod. This data may not be up to date.
     Populated by the system. Read-only. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

[root@k8s-master-50 ~]# kubectl explain namespace
KIND:     Namespace
VERSION:  v1

DESCRIPTION:
     Namespace provides a scope for Names. Use of multiple namespaces is
     optional.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Spec defines the behavior of the Namespace. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status       <Object>
     Status describes the current status of a Namespace. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

[root@k8s-master-50 ~]# kubectl explain deployment
KIND:     Deployment
VERSION:  apps/v1

DESCRIPTION:
     Deployment enables declarative updates for Pods and ReplicaSets.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object metadata.

   spec <Object>
     Specification of the desired behavior of the Deployment.

   status       <Object>
     Most recently observed status of the Deployment.

[root@k8s-master-50 ~]# kubectl explain service
KIND:     Service
VERSION:  v1

DESCRIPTION:
     Service is a named abstraction of software service (for example, mysql)
     consisting of local port (for example 3306) that the proxy listens on, and
     the selector that determines which pods will answer requests sent through
     the proxy.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Spec defines the behavior of a service.
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status       <Object>
     Most recently observed status of the service. Populated by the system.
     Read-only. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

[root@k8s-master-50 ~]# 
```

字段配制的格式

```sh
<map[String]string><[]string><[]Object>

apiVersion <string> # 表示字符串类型
metadata <Object> # 表示需要嵌套多层字段
labels <map[String]string> #表示由k:v组成的映射
finalizers <[]string>      # 表示字符串列表
ownerReferences <[]Object> # 表示对象列表
hostPid <boolean>    # 布尔类型
priority <Integer>   # 整型
name <string>  -required- # 如果类型后面接-required- 表示为必填字段
```

![image-20240103123031953](.\images\image-20240103123031953.png)

### init C案例

准备镜像

```sh
docker pull busybox:1.32.0
docker pull nginx:1.17.10-alpine
```

initC的特点：

1. initC总是运行到成功为止。
2. 每个init C容器都必须在下一下init启动之前成功完成。
3. 如果initC容器运行失败，K8S会不断的重启该POD，直到initC容器成功为止
4. 如果pod对应的restartPolicy为never,它就不会重新启动。



initcpod.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
    - name: myapp-container
      image: busybox:1.32.0
      imagePullPolicy: IfNotPresent
      # 整个命令的意思是：打印“the app is running”，然后等待1小时。
      # 如果“echo the app is running”命令成功执行（返回值为0），那么sleep 3600命令才会被执行。
      command: ['sh','-c','echo the app is running && sleep 3600']
  initContainers:
    - name: init-myservice
      image: busybox:1.32.0
      imagePullPolicy: IfNotPresent
      # 等待某个服务变得可用，等待一个 DNS 记录被创建或更新，以便其他服务可以解析并使用它。
      command: ['sh','-c','until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
    - name: init-mydb
      image: busybox:1.32.0
      command: ['sh','-c','until nslookup mydb; do echo waiting for mydb; sleep 2; done;']
```

init-myservice.yml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
```

init-mydb.yml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9377
```



执行命令:

```sh
# 将文件上传k8s服务器，准备执行


# 启动initcpod的服务
kubectl apply -f initcpod.yml

# 查看pod的启动情况
kubectl get pods

# 查看详细的pod的启动情况
kubectl describe pod myapp-pod

# 查看myapp-pod中的第一个initcontainer日志
kubectl logs myapp-pod -c init-myservice


# 运行init-myservice服务
kubectl apply -f init-myservice.yml


# 查看init-myservice服务的运行情况
kubectl get svc

# 查看myapp-pod的运行情况，需要等一会，会发现pod的第一个init已经就绪
kubectl get pods

# 运行mydb服务
kubectl apply -f init-mydb.yml

# 查看mydb服务的运行情况
kubectl get svc

# 检查mydb的运行信息
kubectl get pods -w
```



输出：

```sh
[root@k8s-master-50 initc]# kubectl apply -f initcpod.yml
pod/myapp-pod created
[root@k8s-master-50 initc]# kubectl get pods
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          6s
[root@k8s-master-50 initc]# kubectl describe pod myapp-pod
Name:         myapp-pod
Namespace:    default
Priority:     0
Node:         k8s-node52/10.0.3.6
Start Time:   Wed, 03 Jan 2024 18:50:07 +0800
Labels:       app=myapp
Annotations:  cni.projectcalico.org/podIP: 10.81.237.200/32
              cni.projectcalico.org/podIPs: 10.81.237.200/32
              kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"app":"myapp"},"name":"myapp-pod","namespace":"default"},"spec":{"c...
Status:       Pending
IP:           10.81.237.200
IPs:
  IP:  10.81.237.200
Init Containers:
  init-myservice:
    Container ID:  docker://5293d13fac38790005149034f193d53924e23e99431d0bb21057c67e3b5ebd42
    Image:         busybox:1.32.0
    Image ID:      docker-pullable://busybox@sha256:bde48e1751173b709090c2539fdf12d6ba64e88ec7a4301591227ce925f3c678
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      until nslookup myservice; do echo waiting for myservice; sleep 2; done;
    State:          Running
      Started:      Wed, 03 Jan 2024 18:50:26 +0800
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
  init-mydb:
    Container ID:  
    Image:         busybox:1.32.0
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      until nslookup mydb; do echo waiting for mydb; sleep 2; done;
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Containers:
  myapp-container:
    Container ID:  
    Image:         busybox:1.32.0
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      echo the app is running && sleep 3600
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       False 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From                 Message
  ----    ------     ----  ----                 -------
  Normal  Scheduled  65s   default-scheduler    Successfully assigned default/myapp-pod to k8s-node52
  Normal  Pulling    63s   kubelet, k8s-node52  Pulling image "busybox:1.32.0"
  Normal  Pulled     46s   kubelet, k8s-node52  Successfully pulled image "busybox:1.32.0"
  Normal  Created    46s   kubelet, k8s-node52  Created container init-myservice
  Normal  Started    46s   kubelet, k8s-node52  Started container init-myservice
  
  # 可以发现，正在初始化init-myservice,但由于现在init-myservice还没有启动，肯定是启动不成功的
  
  [root@k8s-master-50 initc]# kubectl logs myapp-pod -c init-myservice
Server:         10.1.0.10
Address:        10.1.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer

waiting for myservice
Server:         10.1.0.10
Address:        10.1.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer

waiting for myservice
Server:         10.1.0.10
Address:        10.1.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer

waiting for myservice
Server:         10.1.0.10
Address:        10.1.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer

waiting for myservice
Server:         10.1.0.10
Address:        10.1.0.10:53

** server can't find myservice.default.svc.cluster.local: NXDOMAIN

*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer
*** Can't find myservice.default.svc.cluster.local: No answer
*** Can't find myservice.svc.cluster.local: No answer
*** Can't find myservice.cluster.local: No answer

# 可以发现，由于服务没有启动，在不断的打印日志，输出

[root@k8s-master-50 initc]# kubectl apply -f init-myservice.yml
service/init-myservice created
[root@k8s-master-50 initc]# kubectl get svc
NAME             TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
init-myservice   ClusterIP   10.1.11.253   <none>        80/TCP    37s
kubernetes       ClusterIP   10.1.0.1      <none>        443/TCP   7d7h
# 第一个初始化服务已经成功

[root@k8s-master-50 ~]# kubectl get pods 
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          6m58s

# 此时服务还未检测到，第一阶段还是失败状态。

[root@k8s-master-50 ~]# kubectl get pods -w
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          28m
myapp-pod   0/1     Init:1/2   0          28m
myapp-pod   0/1     Init:1/2   0          28m

# 启动了初始化的服务1，init已经成功了一半了

[root@k8s-master-50 initc]# kubectl apply -f init-mydb.yml
service/mydb created
[root@k8s-master-50 initc]# kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.1.0.1       <none>        443/TCP   7d8h
mydb         ClusterIP   10.1.251.136   <none>        80/TCP    6s
myservice    ClusterIP   10.1.191.192   <none>        80/TCP    5m2s
[root@k8s-master-50 initc]# kubectl get pods -w
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:1/2   0          32m
myapp-pod   0/1     PodInitializing   0          33m
myapp-pod   1/1     Running           0          33m
  
```

至此initC的案例已经成功的实现了。

### readinessProbe(准备就绪)

readinessprobepod.yml

```yam
apiVersion: v1
kind: Pod
metadata:
  name: readinessprobe-pod
  labels:
    app: readinessprobe-pod
spec:
  containers:
    - name: readinessprobe-pod
      image: nginx:1.17.10-alpine
      imagePullPolicy: IfNotPresent
      readinessProbe:
        httpGet:
          port: 80
          path: /index1.html
        initialDelaySeconds: 1
        periodSeconds: 3
  restartPolicy: Always
```

执行命令:

```sh
# 创建pod
kubectl apply -f readinessprobepod.yml

#检查pod状态，虽然pod状态显示为runnind，但是ready显示为0/1，因为就绪检查未通过
kubectl get pods

# 查看pod的详细信息，文件最后一行显示readiness probe failed...
kubectl describe pod readinessprobe-pod

# 容器内部文件 
kubectl exec -it readinessprobe-pod sh
cd /usr/share/nginx/html
echo "hello World!" >> index1.html
exit

# 查看pod的状态
kubectl get pods 

```

输出：

```sh
[root@k8s-master-50 initc]# kubectl apply -f readinessprobepod.yml
pod/readinessprobe-pod created
[root@k8s-master-50 initc]# kubectl get pods
NAME                 READY   STATUS              RESTARTS   AGE
myapp-pod            1/1     Running             1          2d3h
readinessprobe-pod   0/1     ContainerCreating   0          8s
[root@k8s-master-50 initc]# kubectl describe pod readinessprobe-pod
Name:         readinessprobe-pod
Namespace:    default
Priority:     0
Node:         k8s-node51/10.0.3.5
Start Time:   Fri, 05 Jan 2024 22:04:13 +0800
Labels:       app=readinessprobe-pod
Annotations:  cni.projectcalico.org/podIP: 10.81.92.72/32
              cni.projectcalico.org/podIPs: 10.81.92.72/32
              kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"app":"readinessprobe-pod"},"name":"readinessprobe-pod","namespace"...
Status:       Running
IP:           10.81.92.72
IPs:
  IP:  10.81.92.72
Containers:
  readinessprobe-pod:
    Container ID:   docker://fe03c9f9948cca1b50e20b4fafa8cc5a7cbe8a21432ebaa6b7435fbc46de1823
    Image:          nginx:1.17.10-alpine
    Image ID:       docker-pullable://nginx@sha256:763e7f0188e378fef0c761854552c70bbd817555dc4de029681a2e972e25e30e
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Fri, 05 Jan 2024 22:04:55 +0800
    Ready:          False
    Restart Count:  0
    Readiness:      http-get http://:80/index1.html delay=1s timeout=1s period=3s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age               From                 Message
  ----     ------     ----              ----                 -------
  Normal   Scheduled  <unknown>         default-scheduler    Successfully assigned default/readinessprobe-pod to k8s-node51
  Normal   Pulling    56s               kubelet, k8s-node51  Pulling image "nginx:1.17.10-alpine"
  Normal   Pulled     16s               kubelet, k8s-node51  Successfully pulled image "nginx:1.17.10-alpine"
  Normal   Created    15s               kubelet, k8s-node51  Created container readinessprobe-pod
  Normal   Started    15s               kubelet, k8s-node51  Started container readinessprobe-pod
  Warning  Unhealthy  1s (x5 over 13s)  kubelet, k8s-node51  Readiness probe failed: HTTP probe failed with statuscode: 404
[root@k8s-master-50 initc]# kubectl exec -it readinessprobe-pod sh
/ # cd /usr/share/nginx/html
/usr/share/nginx/html # echo "hello World!" >> index1.html
/usr/share/nginx/html # exit
[root@k8s-master-50 initc]# kubectl get pods 
NAME                 READY   STATUS    RESTARTS   AGE
myapp-pod            1/1     Running   1          2d3h
readinessprobe-pod   1/1     Running   0          2m26s
```

到此容器已经成功的开始运行。



### livenessProbe(存活检查) 案例一

livenessprobepod.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: livenessprobe-pod
  labels:
    app: livenessprobe-pod
spec:
  containers:
  
    - name: livenessprobe-pod
      image: busybox:1.32.0
      imagePullPolicy: IfNotPresent
      command: ["/bin/sh","-c","touch /tmp/livenesspod ; sleep 30 ; rm -rf /tmp/livenesspod; sleep 3600"]
      livenessProbe:
        exec:
          command: ["text","-e","/tmp/livenesspod"]
        initialDelaySeconds: 1
        periodSeconds: 3
  restartPolicy: Always
```

执行命令:

```sh
# 创建pod
kubectl apply -f livenessprobepod.yml

# 监控pod的状态变化，容器正常启动
kubectl get pod -w

# 等待30秒后，发现pod的RESTARTS值从0变为1，说明pod已经启动一次。

```

输出:

```sh
[root@k8s-master-50 initc]# kubectl apply -f livenessprobepod.yml
pod/livenessprobe-pod created
[root@k8s-master-50 initc]# kubectl get pod -w
NAME                READY   STATUS              RESTARTS   AGE
livenessprobe-pod   0/1     ContainerCreating   0          7s
livenessprobe-pod   1/1     Running             0          41s
livenessprobe-pod   1/1     Running             1          80s
```



### livenessProbe(存活检查) 案例二

livenessprobenginxpod.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: livenessprobenginx-pod
  labels:
    app: livenessprobenginx-pod
spec:
  containers:
    - name: livenessprobenginx-pod
      image: nginx:1.17.10-alpine
      imagePullPolicy: IfNotPresent
      ports:
        - containerPort: 80
          name: nginxhttpget
      livenessProbe:
        httpGet:
          port: 80
          path: /index.html
        initialDelaySeconds: 1
        periodSeconds: 3
        timeoutSeconds: 10
  restartPolicy: Always
```

操作

```sh
# 创建pod
kubectl apply -f livenessprobenginxpod.yml

# 查看pod状态
kubectl get pod

# 查看容器IP访问index.html，可正常访问
kubectl get pod -o wide
curl 10.81.X.X

#删除容器中的index.html
kubectl exec -it livenessprobenginx-pad -- rm -rf /usr/share/nginx/html/index.html

# 再次监控的状态，等一段时间后，poad的RESTARTS值从1变成2.说明pod已经重启一次。
kubectl get pod -w

#因为liveness监控index.html页面已经被删除，所以pod需要重新启动，重启后又重新 创建nginx镜像。nginx镜像中默认有index.html页面。

```

输出：

```sh
[root@k8s-master-50 initc]# kubectl apply -f livenessprobenginxpod.yml 
pod/livenessprobenginx-pod created
[root@k8s-master-50 initc]# kubectl get pod -w
NAME                     READY   STATUS              RESTARTS   AGE
livenessprobenginx-pod   0/1     ContainerCreating   0          7s
[root@k8s-master-50 initc]# kubectl describe pod livenessprobenginx-pod 
Name:         livenessprobenginx-pod
Namespace:    default
Priority:     0
Node:         k8s-node52/10.0.3.6
Start Time:   Fri, 05 Jan 2024 23:38:16 +0800
Labels:       app=livenessprobenginx-pod
Annotations:  cni.projectcalico.org/podIP: 10.81.237.202/32
              cni.projectcalico.org/podIPs: 10.81.237.202/32
              kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"app":"livenessprobenginx-pod"},"name":"livenessprobenginx-pod","na...
Status:       Running
IP:           10.81.237.202
IPs:
  IP:  10.81.237.202
Containers:
  livenessprobenginx-pod:
    Container ID:   docker://f98b23299eeb422798ca8b6aed9772f527ecb43f5c0624fe028f45d3e2378e00
    Image:          nginx:1.17.10-alpine
    Image ID:       docker-pullable://nginx@sha256:763e7f0188e378fef0c761854552c70bbd817555dc4de029681a2e972e25e30e
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 05 Jan 2024 23:39:43 +0800
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:80/index.html delay=1s timeout=10s period=3s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age        From                 Message
  ----    ------     ----       ----                 -------
  Normal  Scheduled  <unknown>  default-scheduler    Successfully assigned default/livenessprobenginx-pod to k8s-node52
  Normal  Pulling    99s        kubelet, k8s-node52  Pulling image "nginx:1.17.10-alpine"
  Normal  Pulled     14s        kubelet, k8s-node52  Successfully pulled image "nginx:1.17.10-alpine"
  Normal  Created    14s        kubelet, k8s-node52  Created container livenessprobenginx-pod
  Normal  Started    14s        kubelet, k8s-node52  Started container livenessprobenginx-pod
[root@k8s-master-50 initc]# kubectl get pod -o wide
NAME                     READY   STATUS    RESTARTS   AGE    IP              NODE         NOMINATED NODE   READINESS GATES
livenessprobenginx-pod   1/1     Running   0          114s   10.81.237.202   k8s-node52   <none>           <none>
[root@k8s-master-50 initc]# curl 10.81.237.202
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
[root@k8s-master-50 initc]# kubectl exec -it livenessprobenginx-pod -- rm -rf /usr/share/nginx/html/index.html
[root@k8s-master-50 initc]# kubectl get pod -w
NAME                     READY   STATUS    RESTARTS   AGE
livenessprobenginx-pod   1/1     Running   0          4m53s
livenessprobenginx-pod   1/1     Running   1          4m57s
[root@k8s-master-50 ~]# kubectl get pods -o wide
NAME                     READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
livenessprobenginx-pod   1/1     Running   2          10h   10.81.237.203   k8s-node52   <none>           <none>
[root@k8s-master-50 ~]#  curl 10.81.237.203
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
[root@k8s-master-50 ~]# kubectl describe pod livenessprobenginx-pod
Name:         livenessprobenginx-pod
Namespace:    default
Priority:     0
Node:         k8s-node52/10.0.3.6
Start Time:   Fri, 05 Jan 2024 23:38:16 +0800
Labels:       app=livenessprobenginx-pod
Annotations:  cni.projectcalico.org/podIP: 10.81.237.203/32
              cni.projectcalico.org/podIPs: 10.81.237.203/32
              kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"app":"livenessprobenginx-pod"},"name":"livenessprobenginx-pod","na...
Status:       Running
IP:           10.81.237.203
IPs:
  IP:  10.81.237.203
Containers:
  livenessprobenginx-pod:
    Container ID:   docker://cf8d25f2f2f13352daaa5b4f6f1b3b05cb9b539a06b10dcf81d156d997b26e87
    Image:          nginx:1.17.10-alpine
    Image ID:       docker-pullable://nginx@sha256:763e7f0188e378fef0c761854552c70bbd817555dc4de029681a2e972e25e30e
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Sat, 06 Jan 2024 09:32:25 +0800
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 05 Jan 2024 23:43:12 +0800
      Finished:     Sat, 06 Jan 2024 00:17:55 +0800
    Ready:          True
    Restart Count:  2
    Liveness:       http-get http://:80/index.html delay=1s timeout=10s period=3s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason          Age                From                 Message
  ----     ------          ----               ----                 -------
  Normal   Scheduled       <unknown>          default-scheduler    Successfully assigned default/livenessprobenginx-pod to k8s-node52
  Normal   Pulling         10h                kubelet, k8s-node52  Pulling image "nginx:1.17.10-alpine"
  Normal   Pulled          10h                kubelet, k8s-node52  Successfully pulled image "nginx:1.17.10-alpine"
  Normal   Created         10h (x2 over 10h)  kubelet, k8s-node52  Created container livenessprobenginx-pod
  Normal   Started         10h (x2 over 10h)  kubelet, k8s-node52  Started container livenessprobenginx-pod
  Warning  Unhealthy       10h (x3 over 10h)  kubelet, k8s-node52  Liveness probe failed: HTTP probe failed with statuscode: 404
  Normal   Pulled          10h                kubelet, k8s-node52  Container image "nginx:1.17.10-alpine" already present on machine
  Normal   Killing         10h                kubelet, k8s-node52  Container livenessprobenginx-pod failed liveness probe, will be restarted
  Warning  FailedMount     21m                kubelet, k8s-node52  MountVolume.SetUp failed for volume "default-token-bgthv" : failed to sync secret cache: timed out waiting for the condition
  Normal   SandboxChanged  21m                kubelet, k8s-node52  Pod sandbox changed, it will be killed and re-created.
  Normal   Pulled          21m                kubelet, k8s-node52  Container image "nginx:1.17.10-alpine" already present on machine
  Normal   Created         21m                kubelet, k8s-node52  Created container livenessprobenginx-pod
  Normal   Started         21m                kubelet, k8s-node52  Started container livenessprobenginx-pod
```

通过观察可以发现此pod在index.html文件被删除后，进行一次容器的重新创建，通过`describe`观察也可以证实是重新创建了容器，则不是重启了容器。



### livenessprobe(存活检查)案例三

livenessprobenginxpod2.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: livenessprobenginx-pod2
  labels:
    app: livenessprobenginx-pod2
spec:
  containers:
    - name: livenessprobenginx-pod2
      image: nginx:1.17.10-alpine
      imagePullPolicy: IfNotPresent
      livenessProbe:
        tcpSocket:
          # 监测8080端口，如果8080端口没有反馈，重启pod
          port: 8080
        initialDelaySeconds: 10
        periodSeconds: 3
        timeoutSeconds: 5
  restartPolicy: Always
```

操作命令:

```sh
# 创建pod
kubectl apply -f livenessprobenginxpod2.yml

# 查看pod的状态
kubectl get pod -w

# 存活检查监听8080端口，8080端口没有反馈信息后重启pod，RESTARTS值从0变为1
```

输出：

```sh
[root@k8s-master-50 initc]# kubectl apply -f livenessprobenginxpod2.yml 
pod/livenessprobenginx-pod2 created
[root@k8s-master-50 initc]# kubectl get pods -w
NAME                      READY   STATUS              RESTARTS   AGE
livenessprobenginx-pod2   0/1     ContainerCreating   0          17s
livenessprobenginx-pod2   1/1     Running             0          21s
livenessprobenginx-pod2   1/1     Running             1          38s
livenessprobenginx-pod2   1/1     Running             2          56s
```



### 沟子函数案例

lifeclepod.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: lifecle-pod1
  labels:
    app: lifecle-pod1
spec:
  containers:
    - name: lifecle-pod1
      image: busybox:1.32.0
      imagePullPolicy: IfNotPresent
      lifecycle:
        postStart:
          exec:
            #创建/data/kis/目录，在目录下创建index.html文件
            command: ['mkdir','-p','/data/k8s/index.html']
      command: ['sh','-c','sleep 5000']
  restartPolicy: Always
```

命令操作:

```sh
# 创建pod
kubectl apply -f lifeclepod.yml

# 查看pod的状态
kubectl get pod

# 进告诉容器，在 /data/k8s,创建index.html文件
kubectl exec -it lifecle-pod1 sh
mkdir /data/k8s
ls

```

输出:

```sh
[root@k8s-master-50 initc]# kubectl apply -f lifeclepod.yml
pod/lifecle-pod1 created
[root@k8s-master-50 initc]# kubectl get pod
NAME           READY   STATUS    RESTARTS   AGE
lifecle-pod1   1/1     Running   0          9s
[root@k8s-master-50 initc]# kubectl exec -it lifecle-pod1 sh
/ # mkdir -p  /data/k8s
/ # ls
bin   data  dev   etc   home  proc  root  sys   tmp   usr   var
/ # cd data
/data # ls
k8s
/data # cd k8s/
/data/k8s # ls
index.html
/data/k8s # 
```



### 总结

pod对象至创建开始至终止退出的时间范围称为生命周期，在这段时间内，pod会处于多种不同的状态，并执行一些操作，其中，创建主容器为必须的操作，其他可选的操作还包括运行初始化容器（init container）、容器启动后沟子(start hook)、容器的存活探测（liveness probe）、就绪性探测（readiness probe）以及容器终止前沟子（pre stop hook）等，这些操作是否执行取决于pod的定义。

#### pod的相位

使用`kubectl get pod`命令，STATUS被称之为相位（phase）.

无论是手动创建还是通过控制器创建pod，pod对象总是处于生命进程中以下几个相位之一：

- pending： apiserver创建了pod资源对象并存入etcd中，但它沿未被调度完成或者仍处于下载镜像的过程中。
- running: pod已经被调度至某个节点，并且所有容器已经被kubectl创建完成。
- succeeded: pod中的所有的容器都已经成功终止并且不被重启。
- failed: 所有容器都已经终止，但至少有一个容器至少终止失败，即容器返回了非0的退出状态或者已经被系统终止。
- unknown: apiserver无法获取到pod对象的状态信息，通常是由于其无法于所在工作节点kubelet通信所致。

pod的相位是在其生命周期中的宏观概念，而非对容器或者pod对象的综合汇总，而且相位的数量和含义被严格界定。



#### pod的创建过程

pod是k8s的基础单元，以下是一个pod资源对象的典型创建过程：

1. 通用通过kubectl或者其他api客户端提交pod spec给api server。
2.  api server尝试着将pod对象的相关信息存入etcd中，待定入操作执行完成，api server即会返回确认信息至客户端。
3. api server开始反应etcd中的状态变化。
4. 所有k8s的组件均使用watch机制来跟踪检查api server上的相关变动。
5. kube-scheduler通过其watch觉察到api server创建了新的pod对象但尚未绑定至任何工作节点。
6. kube-scheduler为pod对象挑选一个工作节点并将结果信息更新至api server
7. 调度结果信息由api server更新至etcd，而且api server也开始反应此pod对象的调度结果。
8. pod被调度到目标工作节点上的kubelet尝试在当前节点上调用docker启动容器，并将容器的结果状态回送至api server
9. api server将Pod状态存入etcd中
10. 在etcd确认写入操作成功完成后，api server将确认信息发送至相关的kubelet。



#### pod生命周期中的重要行为

除了创建应用容器之外，用户还可以为pod对象定义其生命周期中的多种行为，如初始化容器、存活性探测及就就绪性探测等。

1. 初始化容器

初始化容器即应用程序的主容器启动之前要运行的容器，常用于为主容器执行一些预置操作，它们具有两种典型特征：

（1） 初始化容器必须运行完成至结束，若某初始化容器运行失败，那k8s需要重启它，直到成功为止。

（2） 初始化容器都必须按定义的顺序串运行。

有不少的场景都需要在应用程序启动之前进行部分初始化操作，例如: 等待其他相关联的组件服务可用、基于环境变量或配制模板为应用程序生成配制文件、从配制中心获取配置等。初始化容器的典型应用需求具体包含如下几种：

1）运用特定的工具程序，出于安全等反面原因，这些程序不适于包含在主容器镜像中。

2）提供主容器镜像中不具备的工具程序或者自定义代码

3） 为容器镜像的构建和部署人员提供了分离、独立工作的途径、使得它们不必协同起来制作单个镜像文件。

4）初始化容器和主容器处于不同的文件系统视图中，因此可以分别安全地使用敏感数据，例如secrets资源。

5）初始化容器要先于应用容器串行启动并运行完成，因此可以用于延后应用容器的启动直到其依赖的条件得到满足。

pod资源的spec.initContainers字段以列表形式定义可用的初始容器，其嵌套可用字段类似于spec.containers。



#### pod生命周期的钩子函数

容器生命周期钩子使它能够感知其自身生命周期管理中的事件，并在相应的时刻到来时运行由用户指定的处理程序代码，k8s为容器提供了两种生命周期钩子：

- postStart ： 于容器创建完成之后立即运行的钩子处理器（handler）,不过k8s无法确保它一定会于容器中的entrypoint之前运行。
- preStop: 于容器终止操作之前立即运行的钩子处理器，它以同步的方式调用，因此在其完成之前会阻赛删除容器的调用。

钩子的处理器的实现方法由Exec和HTTP两种，Exec在钩子事件触发时直接在当前容器中运行由用户定义的命令，HTTP则在当前容器中向某个URL发起HTTP请求。postStart和preStop处理器定义在spec.lifecycle嵌套字段中。



#### 容器探测

容器探测时pod对象生命周期中的一项重要的日常任务，它是kubelet对容器周期性执行的健康状态诊断，诊断操作由容器的处理器定义，k8s支持三种容器控探针用于pod探测：

- ExecAction: 在容器中执行一个命令，并根据其返回的状态码进行诊断操作称为Exec探测，状态码为0表示成功，否则即为不健康状态
- TCPSocketAction: 通过与容器的某TCP端口尝试建立连接进行诊断，端口能够成功打开即为正常，否则为不健康状态。
- HTTPGetAcion: 通过向容器IP地址的某指定端口的指定path发起HTTP Get请求进行诊断，响应码大于等于200且小于400时即为成功。

任何一种探测方式都可能存在三种结果：

1） success（成功）容器诊断通过

2） failure(失败)   容器未通过诊断

3）unknown(未知)  诊断失败，因此不会采取任何行动。

kubelet右要活动容器上执行两种类型的检测：

(livenessProbe)存活性检查：用于判断容器是否处于运行状态，一量此检测未通过，kubelet将杀死容器并根据restartPolicy决定是否将其重启，未定义存活性检测的容器的默认状态为success

(readinessProbe)就绪性检查：用于判断容器是否准备就绪并可对外提供服务；未通过检测的容器意味着尚未准备就绪，端点控制会将其IP从所有匹配到此pod对象的service对象的端点列表移除；检测通过之后，会再次将其IP添加至端点列表中。



#### 容器的重启策略

容器程序发生崩溃或者容器申请超出限制的资源等原因都可能会导致 pod对象的终止，此时是否应用重建该pod对象则取决于重启策略（restartPolicy）属性：

- Alwasy: 但凡对象终止就将其重启，此为默认设定。
- OnFailure: 仅在pod对象出现错误时方才将其重启。
- Never: 从不重启。

restartPolicy适用于pod对象中的所有容器。而它仅用于控制在同一节点上重新启动pod对象的相关容器。首次需要重启的容器，将在其需要时立即进行重启，随后两次需要重启的操作将由kubelet延迟一段时间后进行，且反复的重启操作的延迟时长以此为10S、20S、40S、80S、160S、300S，300s是最大时延时长。事实上，一旦绑定到一个节点上，pod对象将永远不会重新绑定到另外一个节点，它要么被重启，要么终止，直到节点发生故障或者删除。



#### pod的终止过程

当用户提交删除请求之后，系统就会进行强制删除操作的宽限期倒计时，并将TERM信息发送给pod对象的每个容器的主进程。宽限期倒计时结束后，这些进程将收到强制终止的KILL信号，pod对象随即也将由api server删除，如果在等待进程终止的过程中，kubelet或者容器管理器发生了重启，那么终止 操作会重新获取一个满额的删除宽限期并重新执行删除操作。

具体流程

1. 用户发送删除pod对象的命令。
2. api服务器中的Pod对象会随着时间的推移而更新，在宽限期内（默认30s），pod被视为dead。
3. 将pod的标识terminating状态。
4. 与第三种同时进行，kubelet在监控pod对象转为terminating状态的同时启动pod关闭过程。
5. 与第三步同时运行，端点控制监控到pod对象的关闭行为时将其从所有匹配的到此端口的service资源的端点列表中移除。
6. 如果当前pod对象定义了proStop钩子处理器，则在其标记为terminating后即会以同步的方式启动执行，若宽限期结束后，preStop仍未执行结束，则第二步会被重新 执行并额外执行并额外获取一个时长为2S的小宽限期。
7. pod对象中的容器进程收到TERM信号。
8. 宽限期结束后，若存在任何一个仍在运行的进程，那么pod对象即会收到SIGKILL信号。
9. kubelet请求api server将此pod资源的宽限期设置为0从而完成删除操作，它变得对用户不再可见。

默认情况下，所有删除操作宽限期都是30S，不过kubectl delete可以使用`--grace-period=`选项自定义其时长，若使用0值，则表示直接强制删除指定资源，不过此时需要同时用命令`--forece`选项。





## k8s-pod控制器

简介：

 Controller Manager由kube-controller-manager和cloud-controller-manager组成，是Kubenetes的大脑，它通过apiserver监控整个集群的状态，并确保集群处于预期的工作状态。

kubectl-controller-manager由一系列的控制器组成

```sh
1 Replication Controller(复制控制器)
2 Node Controller（维护node状态）
3 CronJob Controller(处理Cron表达式的Job)
4 DaemonSet Controller（守护进程集）
5 Deployment Controller(无状态服务部署)
6 Endpoint Controller(依据 service spec 创建 endpoint，依据 podip 更新 endpoint。)
7 Garbage Controller(通过ownerReferences处理级联删除，它的内部会有一个Graph Builder，构建一个父子关系图)
8 Namespace Controller(保证 namespace 删除前该 namespace 下的所有资源都先被删除。)
9 Job Controller（处理 job。）
10 Pod AutoScaler(处理 Pod 的自动缩容/扩容)
11 ReplicaSet(复制集)
12 Service Controller(为 LoadBalancer type 的 service 创建 LB VIP。)
13 ServiceAccount Controller( 确保 serviceaccount 在当前 namespace 存在。)
14 StatefulSet Controller（有状态集）
15 Volume Controller( 依据 PV spec 创建 volume。)
16 Resource quota Controller(在用户使用资源之后，更新状态。)
```

cloud-controller-manager在 Kubernetes启用Cloud Provider的时候才需要， 用来配合云服务,提供商的控制， 也包括一系列的控制器

```sh
1 Node Controller
2 Route Controller
3 Service Controller
```

从v1.6开始，cloud provider已经经历了几次重大重构，以便在不修改Kubernetes核心代码的同时构建,自定义的云服务商支持





### 常见的pod控制器及含义 

1. ReplicaSet: 适合无状态的服务部署

​		用户创建指定数量的pod副本数量，确保pod副本数量符合预期状态，并且支持滚动式自动扩容和缩容功能。

​		ReplicaSet主要三个组件组成：

​			 (1) 用户期望的pod副本数量

​			 (2) 标签选择器，判断哪个pod归自己管理。

​			 (3) 当现存的pod数量不足，会根据pod资源模板进行新建

​		帮助用户管理无状态的pod资源，精确反应用户定义的目标数量，但是ReplicaSet不是直接使用控制器，而是使用Deployment。

2. Deployment： 适合无状态的服务部署

​			工作在ReplicaSet之上，用下载管理无状态的应用，目前来说最好的控制器。支持滚动更新和回滚功能，还提供声明式配置。

3. StatefullSet: 适合有状态的服务部署。需要学习完存储卷后进行系统学习。

4. DaemonSet: 一次部署，所有的节点都会部署，例如一些典型的应用场景：

   - 运行集群存储daemon,例如在每个node上运行glusterd、ceph。
   - 在每个node上运行日志收集daemon,例如fluentd、logstash。
   - 在每个Node上运行监控daemon,例如Prometheus Node Exporter

   用于确保集群中的每一个节点只运行特定的Pod副本，通常用于实现系统组后台任务。例如ELK服务。

   特点：服务是无状态的、服务必须是守护进程。

5. job： 一次性的执行任务。只要完成就立即退出，不需要重启或者重建。

6. Cronjob： 周期性的执行任务。周期性任务控制，不需要持续后台运行。



使用的镜像：

```sh
docker pull nginx:1.17.10-alpine
docker pull nginx:1.18.0-alpine
docker pull nginx:1.19.2-alpine
```



### Replication Controller控制器

Replication Controller简称RC，是kubernetes系统中的核心概念之一，简单来说，它其实定义了一个期望的场景，即场景某种pod的副本的数量在任意时刻都符合某个预期值，所以RC定义包含以下：

- pod期待的副本数量
- 用于筛选目标pod的Label Selector
- 当pod的副本数量小于期望值时，用于创建新的pod的pod模板（template）



**ReplicaSet**

ReplicationController用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的Pod来替代；而如果异常多出来的容器也会自动回收。

在新版本的Kubernetes中建议使用ReplicaSet来取代ReplicationController。ReplicaSet跟ReplicationController没有本质的不同，只是名字不一样，并且ReplicaSet支持集合式的selector。

虽然ReplicaSet可以独立使用，但一般还是建议使用 Deployment 来自动管理ReplicaSet，这样就无需担心跟其他机制的不兼容问题（比如ReplicaSet不支持rolling-update但Deployment支持）。

```yaml
apiVersion: apps/v1 #api版本定义
kind: ReplicaSet #定义资源类型为ReplicaSet
metadata: #元数据定义
  name: myapp
  namespace: default
spec: #ReplicaSet的规格定义
  replicas: 2 #定义副本数量为2个
  selector: #标签选择器，定义匹配pod的标签
    matchLabels:
      app: myapp
      release: canary
  template: #pod的模板定义
    metadata: #pod的元数据定义
      name: myapp-pod #自定义pod的名称
      labels: #定义pod的标签，需要和上面定义的标签一致，也可以多出其他标签
        app: myapp
        release: canary
        environment: qa
    spec: #pod的规格定义
      containers: #容器定义
      - name: myapp-container #容器名称
        image: nginx:1.17.10-alpine #容器镜像
        ports: #暴露端口
        - name: http
          containerPort: 80
```

可以通过kubectl命令行方式获取更加详细信息

```
kubectl explain rs
kubectl explain rs.spec
kubectl explain rs.spec.template.spec
```



controller/replicasetdemo.yml

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicasetdemo
  labels:
    app: replicasetdemo
spec:
  replicas: 3
  template:
    metadata:
      name: replicasetdemo
      labels:
        app: replicasetdemo
    spec:
      containers:
        - name: replicasetdemo
          image: nginx:1.17.10-alpine
          imagePUllPolicy: IfNotPresent
          ports:
            - containerPort: 80
      restartPolicy: Always
  selector:
    matchLabels:
      app: replicasetdemo
```

运行命令:

```sh
# 运行ReplicaSet
kubectl apply -f replicasetdemo.yml

# 查看rs控制器
kubectl get rs

# 查看pod信息
kubectl get pod

# 查看pod的详细信息
kubectl describe pod replicasetdemo-cxqxm

# 测试controller控制器下的pod删除、重新被controller控制器拉起
kubectl  delete pod --all
kubectl get pod


# 以命令行的方式修改pod的副本数量为8个
kubectl scale replicaset replicasetdemo --replicas=8
kubectl get rs
kubectl get pods


# 修改pod的副本数量，通过资源清单方式, 例如修改 replicas,修改为5，进行缩容,修改文件后，直接生效。
kubectl edit replicasets.apps replicasetdemo
kubectl get rs


# 显示pod的标签
kubectl get pod --show-labels
# 修改pod的标签
kubectl label pod replicasetdemo-48bmb app=nullnull --overwrite=True
# 再次显示pod的标签： 发现多了一个pod，原来的rs中又重新拉起一个pod，说明RS是通过label去管理pod
kubectl get pod --show-labels

# 删除rs
kubectl delete rs replicasetdemo
```

输出:

```sh
[root@k8s-master-50 controller]# kubectl apply -f replicasetdemo.yml 
replicaset.apps/replicasetdemo created
[root@k8s-master-50 initc]# kubectl get rs
NAME             DESIRED   CURRENT   READY   AGE
replicasetdemo   3         3         3       33m
[root@k8s-master-50 initc]# kubectl get pod
NAME                   READY   STATUS    RESTARTS   AGE
replicasetdemo-cxqxm   1/1     Running   0          35m
replicasetdemo-rlvh9   1/1     Running   0          35m
replicasetdemo-zpbqd   1/1     Running   0          35m
[root@k8s-master-50 initc]# kubectl describe pod replicasetdemo-cxqxm
Name:         replicasetdemo-cxqxm
Namespace:    default
Priority:     0
Node:         k8s-node52/10.0.3.6
Start Time:   Mon, 08 Jan 2024 09:12:19 +0800
Labels:       app=replicasetdemo
Annotations:  cni.projectcalico.org/podIP: 10.81.237.204/32
              cni.projectcalico.org/podIPs: 10.81.237.204/32
Status:       Running
IP:           10.81.237.204
IPs:
  IP:           10.81.237.204
Controlled By:  ReplicaSet/replicasetdemo
Containers:
  replicasetdemo:
    Container ID:   docker://583250ef6e4b43d52d2d3c0acf6aac42c3ba21a56ef9cc73d1d377d55e0c6bf0
    Image:          nginx:1.17.10-alpine
    Image ID:       docker-pullable://nginx@sha256:763e7f0188e378fef0c761854552c70bbd817555dc4de029681a2e972e25e30e
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Mon, 08 Jan 2024 09:12:19 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bgthv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-bgthv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bgthv
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From                 Message
  ----    ------     ----  ----                 -------
  Normal  Scheduled  35m   default-scheduler    Successfully assigned default/replicasetdemo-cxqxm to k8s-node52
  Normal  Pulled     35m   kubelet, k8s-node52  Container image "nginx:1.17.10-alpine" already present on machine
  Normal  Created    35m   kubelet, k8s-node52  Created container replicasetdemo
  Normal  Started    35m   kubelet, k8s-node52  Started container replicasetdemo
[root@k8s-master-50 initc]# kubectl  delete pod --all
pod "replicasetdemo-cxqxm" deleted
pod "replicasetdemo-rlvh9" deleted
pod "replicasetdemo-zpbqd" deleted
[root@k8s-master-50 initc]# 
[root@k8s-master-50 initc]# kubectl get pod
NAME                   READY   STATUS    RESTARTS   AGE
replicasetdemo-h59k4   1/1     Running   0          14s
replicasetdemo-sv2v4   1/1     Running   0          14s
replicasetdemo-zr8v4   1/1     Running   0          14s
# 在所有的pod都删除后，由于deployment，还存在，又重新拉起了pod。副本数据为3个
[root@k8s-master-50 initc]# kubectl scale replicaset replicasetdemo --replicas=8
replicaset.apps/replicasetdemo scaled
[root@k8s-master-50 initc]# kubectl get rs
NAME             DESIRED   CURRENT   READY   AGE
replicasetdemo   8         8         8       41m
[root@k8s-master-50 initc]# kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
replicasetdemo-48bmb   1/1     Running   0          14s
replicasetdemo-5mj69   1/1     Running   0          14s
replicasetdemo-clcmk   1/1     Running   0          14s
replicasetdemo-h59k4   1/1     Running   0          4m21s
replicasetdemo-hlfrp   1/1     Running   0          14s
replicasetdemo-sv2v4   1/1     Running   0          4m21s
replicasetdemo-vc5kp   1/1     Running   0          14s
replicasetdemo-zr8v4   1/1     Running   0          4m21s
# 使用命令一键扩容后，即可看到新的容器已经被创建了出来。

[root@k8s-master-50 initc]# kubectl edit replicasets.apps replicasetdemo
Edit cancelled, no changes made.
[root@k8s-master-50 initc]# kubectl get rs
NAME             DESIRED   CURRENT   READY   AGE
replicasetdemo   5         5         5       49m
[root@k8s-master-50 initc]# kubectl get pod
NAME                   READY   STATUS    RESTARTS   AGE
replicasetdemo-48bmb   1/1     Running   0          8m1s
replicasetdemo-h59k4   1/1     Running   0          12m
replicasetdemo-hlfrp   1/1     Running   0          8m1s
replicasetdemo-sv2v4   1/1     Running   0          12m
replicasetdemo-zr8v4   1/1     Running   0          12m
[root@k8s-master-50 initc]# kubectl get pod --show-labels
NAME                   READY   STATUS    RESTARTS   AGE   LABELS
replicasetdemo-48bmb   1/1     Running   0          38m   app=replicasetdemo
replicasetdemo-h59k4   1/1     Running   0          42m   app=replicasetdemo
replicasetdemo-hlfrp   1/1     Running   0          38m   app=replicasetdemo
replicasetdemo-sv2v4   1/1     Running   0          42m   app=replicasetdemo
replicasetdemo-zr8v4   1/1     Running   0          42m   app=replicasetdemo
[root@k8s-master-50 initc]# kubectl label pod replicasetdemo-48bmb app=nullnull --overwrite=True
pod/replicasetdemo-48bmb labeled
[root@k8s-master-50 initc]# kubectl get pod --show-labels
NAME                   READY   STATUS    RESTARTS   AGE   LABELS
replicasetdemo-48bmb   1/1     Running   0          41m   app=nullnull
replicasetdemo-4vs97   1/1     Running   0          5s    app=replicasetdemo
replicasetdemo-h59k4   1/1     Running   0          45m   app=replicasetdemo
replicasetdemo-hlfrp   1/1     Running   0          41m   app=replicasetdemo
replicasetdemo-sv2v4   1/1     Running   0          45m   app=replicasetdemo
replicasetdemo-zr8v4   1/1     Running   0          45m   app=replicasetdemo
# 观察可以发现，当修改了labels后，又新建了一个pod

[root@k8s-master-50 initc]# kubectl delete rs replicasetdemo
replicaset.apps "replicasetdemo" deleted
[root@k8s-master-50 initc]# kubectl get pod --show-labels
NAME                   READY   STATUS        RESTARTS   AGE    LABELS
replicasetdemo-48bmb   1/1     Running       0          42m    app=nullnull
replicasetdemo-4vs97   1/1     Terminating   0          119s   app=replicasetdemo
replicasetdemo-h59k4   1/1     Terminating   0          47m    app=replicasetdemo
replicasetdemo-hlfrp   1/1     Terminating   0          42m    app=replicasetdemo
replicasetdemo-sv2v4   1/1     Terminating   0          47m    app=replicasetdemo
replicasetdemo-zr8v4   1/1     Terminating   0          47m    app=replicasetdemo
[root@k8s-master-50 initc]# kubectl get pod --show-labels
NAME                   READY   STATUS    RESTARTS   AGE   LABELS
replicasetdemo-48bmb   1/1     Running   0          43m   app=nullnull
[root@k8s-master-50 initc]# 
```

**总结**

kubectl命令行工具适用于RC绝大部分命令同样使用于ReplicaSet,此外，我们当前很少单独使用Replicaset，它主要被Demployment这个更高层的资源对象所使用，从而形成一整套pod创建、删除、更新的编排机制，我们在使用Demployment时无需关心它是如何维护和创建ReplicaSet的，这一切都是自自动发生的。

最后，总结下RC（ReplicaSet）的一些特性和作用：

- 在绝大多数情况下，我们通过定义一个RC实现的pod的创建及副本数量的自动控制。
- 在RC里包括完整的Pod定义模板。
- RC通过Label Selector机制实现对pod的副本的自动控制。
- 通过改变RC里的pod副本数量，可以实现pod的扩容和缩容。
- 通过改变RC里pod的模板中的镜像版本，可以实现滚动升级。





### Deployment

Deployment在kubernetes在1.2版本中引入的新概念，用于更好的解决Pod的编排问题，为此Deployment在内部使用了ReplicaSet来实现目的，我们可以把Deployment理解为ReplicaSet的一次升级，两者相似度超过90%

Deployment的使用场景：

- 创建一个Deployment对象来生成对应的ReplicaSet并完成Pod副本的创建。
- 检查Deployment的状态来看部署动作是否完成（Pod副本数量是否达到了预期的值）
- 更新Deployment以创建新的Pod（比如镜像升级）
- 如果当前Deployment不稳定，可以回滚到一个早先的Deployment版本。
- 暂停Deployment以便一次性修改多个PodTemplateSpec的配制项，之后在恢复Deployment,进行发布。
- 扩展Deployment以应对高负载。
- 查看Demployemtn的状态，以此作为发布是否成功的标志。
- 清理不在需要的旧版本ReplicaSet。

Deployment的模板

使用命令行获取更加详细的说明：

```sh
kubectl explain deploy
kubectl explain deploy.spec
kubectl explain deploy.spec.template.spec
```



除了API生命与kind类型的区别，Deployment的定义与ReplicaSet的定义很类似。

controller/demploymentdemo.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploymentdemo1
  labels:
    app: deploymentdemo1
spec:
  replicas: 10
  template:
    metadata:
      name: deploymentdemo1
      labels:
        app: deploymentdemo1
    spec:
      containers:
        - name: demploymentdemo1
          image: nginx:1.17.10-alpine
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80
      restartPolicy: Always
  selector:
    matchLabels:
      app: deploymentdemo1
```

操作命令

```sh
kubectl apply -f deploymentdemo.yml 

# 查看deployment
# 查看rs:demployment名称+hashcode码组成
kubectl get rs

# 查看pod
kubectl get pod
```



输出:

```sh
[root@k8s-master-50 controller]# kubectl apply -f deploymentdemo.yml 
deployment.apps/deploymentdemo1 created
[root@k8s-master-50 controller]# kubectl get rs
NAME                        DESIRED   CURRENT   READY   AGE
deploymentdemo1-787d9c585   10        10        0       6s
[root@k8s-master-50 controller]# kubectl get pod
NAME                              READY   STATUS              RESTARTS   AGE
deploymentdemo1-787d9c585-25gqk   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-csm6n   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-cw6p2   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-dfc4q   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-gbzff   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-hf2g5   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-m5lf5   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-pjvjj   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-s2dw8   0/1     ContainerCreating   0          11s
deploymentdemo1-787d9c585-zfxff   0/1     ContainerCreating   0          11s
[root@k8s-master-50 controller]# kubectl get pod
NAME                               READY   STATUS    RESTARTS   AGE
deploymentdemo1-56bfc59f9b-2qn8p   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-6wxp6   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-8vxpf   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-dxc7b   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-fq9w6   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-gsq8l   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-msg9x   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-n9ns8   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-xtww2   1/1     Running   0          22s
deploymentdemo1-56bfc59f9b-zc52j   1/1     Running   0          22s
```



**镜像更新升级**

命令行方式:

```sh
# 升级nginx镜像版本为1.18.0
kubectl set image deployment deploymentdemo1 deploymentdemo1=nginx:1.18.0-alpine


# 查看pod的升级情况
kubectl get pods -w

# 进入某一个pod，查看nginx的版本,检查是否已经成功的升级到了1.18.0
kubectl exec -it kubectl exec -it deploymentdemo1-df6bc5d4c-hgnnp sh
nginx -v
exit
```

操作：

```sh
[root@k8s-master-50 controller]# kubectl set image deployment deploymentdemo1 deploymentdemo1=nginx:1.18.0-alpine
deployment.apps/deploymentdemo1 image updated
[root@k8s-master-50 controller]# kubectl get pods -w
NAME                               READY   STATUS        RESTARTS   AGE
deploymentdemo1-7ccccfbf85-7lm84   0/1     Terminating   0          25s
deploymentdemo1-7ccccfbf85-cg2m2   0/1     Terminating   0          25s
deploymentdemo1-7ccccfbf85-g75bd   0/1     Terminating   0          25s
deploymentdemo1-7ccccfbf85-mg2hq   0/1     Terminating   0          25s
deploymentdemo1-7ccccfbf85-mmm7j   0/1     Terminating   0          25s
deploymentdemo1-7ccccfbf85-xw4vq   0/1     Terminating   0          25s
deploymentdemo1-df6bc5d4c-6tsdk    1/1     Running       0          5s
deploymentdemo1-df6bc5d4c-9l4vv    1/1     Running       0          7s
deploymentdemo1-df6bc5d4c-bnpfm    1/1     Running       0          7s
deploymentdemo1-df6bc5d4c-hgnnp    1/1     Running       0          4s
deploymentdemo1-df6bc5d4c-jv2hl    1/1     Running       0          5s
deploymentdemo1-df6bc5d4c-kthc4    1/1     Running       0          7s
deploymentdemo1-df6bc5d4c-pcksg    1/1     Running       0          7s
deploymentdemo1-df6bc5d4c-rg9ch    1/1     Running       0          5s
deploymentdemo1-df6bc5d4c-z64q2    1/1     Running       0          5s
deploymentdemo1-df6bc5d4c-zg2h6    1/1     Running       0          7s
deploymentdemo1-7ccccfbf85-cg2m2   0/1     Terminating   0          28s
deploymentdemo1-7ccccfbf85-cg2m2   0/1     Terminating   0          28s
deploymentdemo1-7ccccfbf85-7lm84   0/1     Terminating   0          28s
deploymentdemo1-7ccccfbf85-7lm84   0/1     Terminating   0          28s
deploymentdemo1-7ccccfbf85-mg2hq   0/1     Terminating   0          28s
deploymentdemo1-7ccccfbf85-mg2hq   0/1     Terminating   0          28s
deploymentdemo1-7ccccfbf85-xw4vq   0/1     Terminating   0          31s
deploymentdemo1-7ccccfbf85-xw4vq   0/1     Terminating   0          31s
deploymentdemo1-7ccccfbf85-g75bd   0/1     Terminating   0          31s
deploymentdemo1-7ccccfbf85-g75bd   0/1     Terminating   0          31s
deploymentdemo1-7ccccfbf85-mmm7j   0/1     Terminating   0          31s
deploymentdemo1-7ccccfbf85-mmm7j   0/1     Terminating   0          31s
^C[root@k8s-master-50 controller]# kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
deploymentdemo1-df6bc5d4c-6tsdk   1/1     Running   0          38s
deploymentdemo1-df6bc5d4c-9l4vv   1/1     Running   0          40s
deploymentdemo1-df6bc5d4c-bnpfm   1/1     Running   0          40s
deploymentdemo1-df6bc5d4c-hgnnp   1/1     Running   0          37s
deploymentdemo1-df6bc5d4c-jv2hl   1/1     Running   0          38s
deploymentdemo1-df6bc5d4c-kthc4   1/1     Running   0          40s
deploymentdemo1-df6bc5d4c-pcksg   1/1     Running   0          40s
deploymentdemo1-df6bc5d4c-rg9ch   1/1     Running   0          38s
deploymentdemo1-df6bc5d4c-z64q2   1/1     Running   0          38s
deploymentdemo1-df6bc5d4c-zg2h6   1/1     Running   0          40s
[root@k8s-master-50 controller]# kubectl exec -it deploymentdemo1-df6bc5d4c-hgnnp sh
/ # nginx -v
nginx version: nginx/1.18.0
/ # exit
[root@k8s-master-50 controller]# 
```

**yaml文件方式**

```sh
# 升级镜像的版本至1.19.2-alpine,将修改为“- image: nginx:1.19.2-alpine”
kubectl edit deployments.apps deploymentdemo1

# 查看pod升级的情况
kubectl get pods -w

# 进入一个容器查看容器升级后的版本信息
kubectl exec -it deploymentdemo1-584f6b54dd-j2t9l sh
nginx -v
exit
```

输出 ：

```sh
[root@k8s-master-50 controller]# kubectl edit deployments.apps deploymentdemo1
deployment.apps/deploymentdemo1 edited
[root@k8s-master-50 controller]# kubectl get pods -w
NAME                               READY   STATUS        RESTARTS   AGE
deploymentdemo1-584f6b54dd-22h66   1/1     Running       0          6s
deploymentdemo1-584f6b54dd-47d7n   1/1     Running       0          5s
deploymentdemo1-584f6b54dd-4n6xg   1/1     Running       0          5s
deploymentdemo1-584f6b54dd-65cvz   1/1     Running       0          6s
deploymentdemo1-584f6b54dd-7wrbk   1/1     Running       0          4s
deploymentdemo1-584f6b54dd-j2t9l   1/1     Running       0          6s
deploymentdemo1-584f6b54dd-r8ncx   1/1     Running       0          5s
deploymentdemo1-584f6b54dd-vhhsh   1/1     Running       0          6s
deploymentdemo1-584f6b54dd-xqtfm   1/1     Running       0          4s
deploymentdemo1-584f6b54dd-xs7rh   1/1     Running       0          6s
deploymentdemo1-7ccccfbf85-46wl2   0/1     Terminating   0          79s
deploymentdemo1-7ccccfbf85-v5hkk   0/1     Terminating   0          79s
deploymentdemo1-7ccccfbf85-46wl2   0/1     Terminating   0          81s
deploymentdemo1-7ccccfbf85-46wl2   0/1     Terminating   0          81s
deploymentdemo1-7ccccfbf85-v5hkk   0/1     Terminating   0          86s
deploymentdemo1-7ccccfbf85-v5hkk   0/1     Terminating   0          86s
[root@k8s-master-50 controller]# kubectl exec -it deploymentdemo1-584f6b54dd-j2t9l sh
/ # nginx -v
nginx version: nginx/1.19.2
/ # exit
[root@k8s-master-50 controller]# 
```



**Deployment扩容**

```sh
kubectl scale deployment deploymentdemo1 --replicas=16

# 查看当前的pod的信息
kubectl get pod
```

输出：

```sh
[root@k8s-master-50 controller]# kubectl scale deployment deploymentdemo1 --replicas=16
deployment.apps/deploymentdemo1 scaled
[root@k8s-master-50 controller]# kubectl get pod
NAME                               READY   STATUS    RESTARTS   AGE
deploymentdemo1-584f6b54dd-22h66   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-47d7n   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-4n6xg   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-5tv47   1/1     Running   0          3s
deploymentdemo1-584f6b54dd-65cvz   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-7wrbk   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-845dp   1/1     Running   0          3s
deploymentdemo1-584f6b54dd-9rx8s   1/1     Running   0          3s
deploymentdemo1-584f6b54dd-gwfpj   1/1     Running   0          3s
deploymentdemo1-584f6b54dd-j2t9l   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-lql4k   1/1     Running   0          3s
deploymentdemo1-584f6b54dd-qwl4k   1/1     Running   0          3s
deploymentdemo1-584f6b54dd-r8ncx   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-vhhsh   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-xqtfm   1/1     Running   0          93m
deploymentdemo1-584f6b54dd-xs7rh   1/1     Running   0          93m
[root@k8s-master-50 controller]# kubectl scale deployment deploymentdemo1 --replicas=4
deployment.apps/deploymentdemo1 scaled
[root@k8s-master-50 controller]# kubectl get pod -w
NAME                               READY   STATUS        RESTARTS   AGE
deploymentdemo1-584f6b54dd-22h66   1/1     Running       0          93m
deploymentdemo1-584f6b54dd-47d7n   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-5tv47   0/1     Terminating   0          39s
deploymentdemo1-584f6b54dd-65cvz   1/1     Running       0          93m
deploymentdemo1-584f6b54dd-7wrbk   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-9rx8s   0/1     Terminating   0          39s
deploymentdemo1-584f6b54dd-j2t9l   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-lql4k   0/1     Terminating   0          39s
deploymentdemo1-584f6b54dd-qwl4k   0/1     Terminating   0          39s
deploymentdemo1-584f6b54dd-r8ncx   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-vhhsh   1/1     Running       0          93m
deploymentdemo1-584f6b54dd-xqtfm   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-xs7rh   1/1     Running       0          93m
deploymentdemo1-584f6b54dd-qwl4k   0/1     Terminating   0          39s
deploymentdemo1-584f6b54dd-qwl4k   0/1     Terminating   0          39s
deploymentdemo1-584f6b54dd-7wrbk   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-7wrbk   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-lql4k   0/1     Terminating   0          41s
deploymentdemo1-584f6b54dd-lql4k   0/1     Terminating   0          41s
deploymentdemo1-584f6b54dd-r8ncx   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-r8ncx   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-j2t9l   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-j2t9l   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-5tv47   0/1     Terminating   0          42s
deploymentdemo1-584f6b54dd-5tv47   0/1     Terminating   0          42s
deploymentdemo1-584f6b54dd-9rx8s   0/1     Terminating   0          46s
deploymentdemo1-584f6b54dd-9rx8s   0/1     Terminating   0          46s
deploymentdemo1-584f6b54dd-xqtfm   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-xqtfm   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-47d7n   0/1     Terminating   0          93m
deploymentdemo1-584f6b54dd-47d7n   0/1     Terminating   0          93m
# 可以肉眼可见的速度，在将相应的pod进行停止操作。
[root@k8s-master-50 controller]# kubectl get pod 
NAME                               READY   STATUS    RESTARTS   AGE
deploymentdemo1-584f6b54dd-22h66   1/1     Running   0          95m
deploymentdemo1-584f6b54dd-65cvz   1/1     Running   0          95m
deploymentdemo1-584f6b54dd-vhhsh   1/1     Running   0          95m
deploymentdemo1-584f6b54dd-xs7rh   1/1     Running   0          95m
```





### 滚动更新

滚动更新一般存在4种部署方式：蓝绿部署、滚动部署、灰度发布、金丝雀发布。

1. 蓝绿部署, 是在不停老版本、部署新版本然后进行测试，确认OK，将流量切换到新版本，然后老版本同时也升级到新版本。蓝绿部署无需停机，并且风险极小。
1. 滚动发布，一般是取出一个或者多个服务器停止服务，执行更新，并重新将其投入使用，周而复始，直到集群中所有版本都更新成最新版本。这种部署方式相对于蓝绿部署，更加节约资源---它不需要运行两个集群、两倍的实例数。我们可以部分部署，例如每次只取出集群的20%进行升级。
1. 灰度发布：是指的在黑白之间，能够平滑过渡的一种发布方式。AB Test 就是一种灰度发布，让一部分用户继续用A，一部分用户开始用B，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁到B上面来，灰度发布可以保持整体系统的稳定，在初始灰度的时候，就可以发现、调整问题，以保证其影响度，而我们平常所说的金丝雀也就是灰度发布的一种方式。



#### 滚动更新-金丝雀发布

Deployment控制器支持自定义控制更新过程中的滚动节奏，如“暂停（pause）”或“继续（resume）”更新操作。比如等待第一批新的Pod资源创建完成后立即暂停更新过程，此时仅存在一部分版本的应用，主体部分还是旧的版本。然后再筛选一小部分的用户请求路由到新版本的Pod应用，继续观察能否稳定地按期望的方式运行。确定没有问题之后再继续完成余下的Pod资源滚动更新 ，否则立即回滚更新操作。这就是金丝雀发布（Canary Release）

```sh
# 更新Deployment的nginx:1.18.0-alpine版本，并配置暂停deployment
kubectl set image deployment deploymentdemo1 deploymentdemo1=nginx:1.18.0-alpine && kubectl rollout pause deployment deploymentdemo1

# 观察更新的状态
kubectl rollout status deployments deploymentdemo1

# 监控更新的过程，可以看到已经新增了一个资源，但是并未按照预期的状态去删除一个旧的资源，就是因为使用了pause暂停命令。
kubectl get pods -l app=deploymentdemo1 -w

# 确保更新的pod没问题，继续更新
kubectl rollout resume deploy deploymentdemo1

# 查看更新的情况
kubectl get pods -l app=deploymentdemo1 -w

# 进入某一个pod内部，查看nginx的更新版本信息
kubectl exec -it deploymentdemo1-df6bc5d4c-8mpfv sh
nginx -v
exit

```

输出：

```sh
[root@k8s-master-50 data]# kubectl set image deployment deploymentdemo1 deploymentdemo1=nginx:1.18.0-alpine && kubectl rollout pause deployment deploymentdemo1
deployment.apps/deploymentdemo1 image updated
deployment.apps/deploymentdemo1 paused
[root@k8s-master-50 data]# kubectl rollout status deployments deploymentdemo1
Waiting for deployment "deploymentdemo1" rollout to finish: 2 out of 4 new replicas have been updated...
[root@k8s-master-50 data]# kubectl get pods -l app=deploymentdemo1 -w
NAME                              READY   STATUS    RESTARTS   AGE
deploymentdemo1-df6bc5d4c-88xsz   1/1     Running   0          3m4s
deploymentdemo1-df6bc5d4c-8mpfv   1/1     Running   0          11s
deploymentdemo1-df6bc5d4c-8zxjf   1/1     Running   0          3m3s
deploymentdemo1-df6bc5d4c-ghjhc   1/1     Running   0          11s
# 通过观察时间可以发现，已经有两个pod完成了更新，还有两个未更新，这一兰
[root@k8s-master-50 data]# kubectl exec -it deploymentdemo1-df6bc5d4c-8mpfv sh
/ # nginx -v
nginx version: nginx/1.18.0
/ # exit
[root@k8s-master-50 data]# 
[root@k8s-master-50 data]# kubectl rollout resume deploy deploymentdemo1
deployment.apps/deploymentdemo1 resumed
[root@k8s-master-50 data]# kubectl rollout status deployments deploymentdemo1
deployment "deploymentdemo1" successfully rolled out
```



#### 滚动更新-版本回退

默认情况下，kubernetes会在系统中保存前两次的Deployment的rollout历史记录，以便可以随时回退（可以修改`revision history limit`来更改保存的revision数）

注意：只要Deployment的rollout被触发就会创建一个revision，也就是说当且仅当Deployment的Pod Template（如.spec.template）被更改，例如更新template中的label和容器镜像时，就会创建出一个新的revision

其他的更新，比如扩容Deployment不会被创建revision---因此我们可以很方便的手动或者自动扩容。这意味首当回退到历史revision时，只有Deployment中的pod template部分才会回退。

rollout常见命令

| 子命令  | 功能说明                   |
| ------- | -------------------------- |
| history | 查看rollout操作历史        |
| pause   | 将提供的资源设置为暂停状态 |
| restart | 重启某资源                 |
| resume  | 将某资源从暂停状态恢复正常 |
| status  | 查看rollout操作状态        |
| undo    | 回滚前一rollout            |





## 结束



